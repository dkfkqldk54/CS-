<h1>:green_book: <쉽게 배우는 알고리즘> 정리</h1>

<a href="#2">:pencil2: Chapter2. 알고리즘 설계와 분석의 기초</a>
- O 표기법
- Ω 표기법
- Θ 표기법
- o 표기법
- ω 표기법

<a href="#3">:pencil2: Chapter3. 점화식과 알고리즘 복잡도 분석</a>
- 반복 대치
- 추정 후 증명
- 마스터 정리

<a href="#4">:pencil2: Chapter4. 정렬</a>
- 기본적인 정렬 알고리즘: 선택 정렬
- 기본적인 정렬 알고리즘: 버블 정렬
- 기본적인 정렬 알고리즘: 삽입 정렬
- 고급 정렬 알고리즘: 병합 정렬
- 고급 정렬 알고리즘: 퀵 정렬
- 고급 정렬 알고리즘: 힙 정렬
- 특수 정렬 알고리즘: 기수 정렬
- 특수 정렬 알고리즘: 계수 정렬
  
<a href="#5">:pencil2: Chapter5. 선택 알고리즘</a>
- 평균 선형 시간 선택 알고리즘
- 최악의 경우에도 선형 시간을 보장하는 선택 알고리즘

<a href="#6">:pencil2: Chapter6. 검색 트리</a>
- 레코드, 키의 정의 및 검색 트리
- 이진 검색 트리
- 이진 검색 트리: 삽입
- 이진 검색 트리: 삭제
- 레드 블랙 트리
- 레드 블랙 트리: 삽입
- 레드 블랙 트리: 삭제
- B트리
- B트리: 삽입
- B트리: 삭제
- KD 트리
- KDB 트리
- R트리
- 그리드 파일
  
<a href="#7">:pencil2: Chapter7. 해시 테이블</a>
- 해시 테이블
- 해시 함수
- 충돌 해결: 체이닝
- 충돌 해결: 개방 주소 방법
- 해시 테이블에서 검색 시간 분석

<h2><a id="2">:pencil2: Chapter2. 알고리즘 설계와 분석의 기초</a></h2>

**:pushpin: O 표기법**

O(g(n)) = {f(n) | ∃c > 0, n0 > 0 s.t. ∀n >= n0, f(n) <= cg(n)}<br>
O(g(n)) = {f(n) | 모든 n > n0에 대하여 f(n) <= cg(n)인 양의 상수 c와 n0가 존재한다}<br>
O(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 f(n) <= cg(n)인 양의 상수 c가 존재한다}<br>
<br>
O(g(n))은 충분히 큰 n에 대하여 g(n)에 상수만 곱하면 g(n)이 누를 수 있는 모든 함수의 집합임.<br>
n^2에 3보다 크거나 같은 상수를 곱하면 3n^2을 누를 수 있으므로 3n^2은 O(n^2)에 속함.<br>
<br>
5n^2 = O(n^2)임을 보여라.<br>
c를 6, n0을 1로 잡으면 모든 n >= n0(=1)에 대하여 5n^2 <= 6n^2임. 즉 정의를 만족하는 c와 n0이 존재함.<br>

**:pushpin: Ω 표기법**
  
  Ω(g(n)) = {f(n) | ∃c > 0, n0 > 0 s.t. ∀n >= n0, cg(n) <= f(n)}<br>
  Ω(g(n)) = {f(n) | 모든 n > n0에 대하여 cg(n) <= f(n)인 양의 상수 c와 n0가 존재한다}<br>
  Ω(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 cg(n) <= f(n)인 양의 상수 c가 존재한다}<br>
  <br>
  Ω(g(n))은 충분히 큰 n에 대하여 g(n)에 상수만 곱하면 g(n)이 압도당할 수 있는 모든 함수의 집함임.<br>
  <br>
  5n^2 = Ω(n^2)임을 보여라.<br>
  c를 4로 잡고, n0 = 1로 잡으면 모든 n >= n0(=1)에 대하여 4n^2 <= 5n^2임. 즉 정의를 만족하는 c와 n0이 존재함.<br>
  
**:pushpin: Θ 표기법**
 
  Θ(g(n)) = O(g(n)) ∩ Ω(g(n))<br>
  Θ(g(n)) = {f(n) | ∃c1, c2 > 0, n0 > 0 s.t. ∀n >= n0, c1g(n) <= f(n) <= c2g(n)}<br>
  Θ(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 c1g(n) <= f(n) <= c2g(n)인 양의 상수 c1, c2가 존재한다.}<br>
  <br>
  5n^2 = Θ(n^2)임을 보여라.<br>
  5n^2 = O(n^2), 5n^2 = Ω(n^2)임을 보였기 때문에 5n^2 = Θ(n^2)이다.<br>

**:pushpin: o 표기법**
  
  함수의 증가율이 점근적 의미에서 어느 한계보다 더 작다는 것을 표현하고자 할 때 사용됨.<br>
  예를 들어 함수 5n = o(n^2)이다. 5n의 증가율은 n^2의 증가율보다 작기 때문이다.<br>
  그렇지만 함수 (1/2) * n^2은 o(n^2)에 속하지 않음. (1/2) * n^2의 증가 속도가 n^2의 증가 속도와 점근적으로 동일하기 때문임.<br>
  <br>
  o(g(n)) = {f(n) | lim f(n) / g(n) = 0 }<br>
  o(g(n)) = {f(n) | ∃n0 > 0 s.t. ∀c>0 and n>=n0, f(n) < cg(n)}<br>
  o(g(n)) = {f(n) | n이 충분히 크면 모든 c>0에 대하여 f(n) < cg(n)이다.}<br>
  o(g(n))은 충분히 큰 n에 대하여 g(n)에 아무리 작은 상수를 곱해도 g(n)이 압도하는 모든 함수의 집합임.<br>
  <br>
  5n^2 = o(n^3)임을 보여라.<br>
  lim (n^2 -5) / n^3 = 0임.<br>
  
**:pushpin: ω 표기법**
  
  함수의 증가율이 점근적 의미에서 어느 한계보다 더 크다는 것을 표현하고자 할 때 사용함.<br>
  예를 들어 5n^3 = ω(n^2)임. 5n^3의 증가 속도는 n^2의 증가 속도보다 큼.<br>
  함수 2n^2은 ω(n^2)에 속하지 않음. 2n^2의 증가 속도는 n^2의 증가 속도와 점근적으로 동일하기 때문임.<br>
  ω표기는 함수의 기울기상의 여유 있는 하한을 나타냄.<br>
  <br>
  ω(g(n)) = {f(n) | lim f(n) / g(n) = ∞}<br>
  ω(g(n)) = {f(n) | ∃n0 > 0 s.t. ∀c>0 and n>=n0, cg(n) < f(n)}<br>
  ω(g(n)) = {f(n) | n이 충분히 크면 모든 c > 0에 대하여 cg(n) < f(n)이다.}<br>
  <br>
  n^3 / 4 = ω(n^2)임을 보여라.<br>
  lim ( (n^3 /4) / n^2 ) = ∞이므로 n^3 / 4 = ω(n^2)이다.<br>

<h2><a id="3">:pencil2: Chapter3. 점화식과 알고리즘 복잡도 분석</a></h2>


점화식은 어떤 함수를 자신과 똑같은 함수를 이용해 나타내는 것임.<br>
n!의 점화식은 f(n) = n * f(n-1), 피보나치 수열의 점화식은 f(n) = f(n-1) + f(n-2)임.<br>

**:pushpin: 반복 대치**

<pre>
factorial(n)
{
  1. if (n = 1) return 1;
  2. return n * factorial(n-1);
}
</pre>

이 알고리즘으로 n!을 구하는데 걸리는 시간을 T(n)이라고 하면, T(n) = T(n-1) + c임.<br>
c는 자기호출을 제외한 나머지의 수행 시간으로 1을 수행하는 시간과 2의 곱셈을 한번 수행하는 시간임.<br>
크기가 1이면 T(1) <= c임.<br>

<pre>
T(n) = T(n-1) + c
     = T(n-2) + 2c
     ...
     = T(1) + (n-1)c
     <= cn
</pre>

T(n) <= cn이므로 T(n) = O(n)이다.<br>

**병합 정렬**<br>

입력의 크기가 n인 배열을 정렬하는 병합 정렬은 배열을 이등분한 다음, 각각을 재귀적으로 병합 정렬해 이 둘을 병합함으로써 정렬을 끝냄.<br>
입력의 크기가 n인 배열에 대한 병합 정렬에서 대소 비교의 총 횟수를 T(n)이라고 하자.<br>
n = 2^k이라고 가정해도 일반성을 잃지 않으므로 n = 2^k으로 가정하겠음.<br>
T(n) <= 2T(n/2) + n임.<br>
n은 merge(A, p, q, r)에 필요한 최대 비교 횟수 n-1과 if (p<r) then에 있는 비교 한 번을 합한 것임.<br>

<pre>
T(n) <= 2T(n/2) + n
     <= 2^2T(n/2^2) + 2n
     ...
     <= 2^kT(n/2^k) + kn
     = nT(1) + nlogn
     = n + nlogn
      = O(nlogn)
</pre>

T(n) <= 2T(n/2) + n을 T(n) <= 2T(n/2) + O(n)으로 표시하기도 함.<br>
여기서 O(n)은 집합으로서 O(n)을 의미하지 않고, O(n)에 속하는 함수 하나를 대신하는 관행적 표현임.<br>

**n = 2^k**<br>

점근적 복잡도의 계산을 용이하게 하기 위해 n = 2^k이라는 가정을 종종 사용함.<br>
어떠한 n이라도 n과 2n 사이에 2의 멱수가 하나 있음.<br>
즉, n <= 2^k <= 2n인 2^k가 하나 존재함.<br>
임의의 상수 r에 대해 T(n) = O(n^r)이라면 T(2n) = O(2^r * n^r) = O(n^r)이므로 T(n) = T(2n)임.<br>
T(n) <= T(2^k) <= T(2n)이고 T(n) = T(2n) 이므로 T(n) = T(2^k) = T(2n)임.<br>
즉, n의 오른쪽에서 처음으로 마나는 2의 멱수에 대한 함수도 항상 n에 대한 함수와 같은 점근적 복잡도를 가지므로 n = 2^k로 가정해도 점근적 분석의 결과는 같음.<br>

**:pushpin: 추정 후 증명**

추정 후 증명은 식의 모양을 보고 점근적 복잡도를 추정한 다음 그것이 옳음을 귀납적으로 증명하는 방법임.<br>

<pre>
T(n) <= 2T(n/2) + n의 점근적 복잡도는 T(n) = O(nlogn)이다. 즉 충분히 큰 n에 대하여 T(n) <= cnlogn인 양의 상수 c가 존재한다.

경계조건: T(2) <= c2log2를 만족하는 c가 존재한다.

귀납적 가정과 전개: n/2에 대해 T(n/2) <= c(n/2)log(n/2)을 만족한다고 가정하면,
T(n) <= 2T(n/2) + n
     <= 2c(n/2)log(n/2) + n
     = cnlogn - cnlog2 +n
     <= cnlogn
     
이를 만족하는 상수 c가 존재한다. 따라서 T(n) = O(nlogn)이다.
</pre>

여기서 log1 = 0이므로 T(1) <= 1log1은 불가능하기 때문에 T(2)를 경계조건으로 잡은 것임.<br>
경계조건으로 T(2)가 아닌 훨씬 큰 상수 a에 대해 T(a) <= caloga임을 보여도 상관없음.<br>
일반적으로  T(n) = f(n)임을 보이기 위해 상수 a를 경계치로 잡으면 T(a)도 상수가 되므로 f(n)이 양수인 한 T(a) <= cf(a)를 만족시키는 c가 항상 존재함.<br>
그러므로 경계조건을 만족시키는 경계치는 항상 잡을 수 있기 때문에 경계조건을 확인하지 않아도 됨.<br>

<pre>
T(n) <= 2T(n/2 + 10) + n의 점근적 복잡도는 T(n) = O(nlogn)이다. 즉, 충분히 큰 n에 대하여 T(n) <= cnlogn인 양의 상수 c가 존재한다.

증명

T(n) <= 2T(n/2 + 10) + n
     <= 2c(n/2 + 10)log(n/2 + 10) + n
     = cnlog(n/2 + 10) + 20clog(n/2 + 10) + n
     <= cnlog(3n / 4) + 20clog(3n / 4) + n
     = cnlogn + cn(log3 - log4) + 20clog(3n / 4) + n
     <= cnlogn
     
이를 만족하는 상수 c가 존재한다. 따라서 T(n) = O(nlogn)이다.
이 때, n에 대한 제약이 있다.
n/2 + 10 < n이어야 하므로 n > 20이어야 논리적인 전개가 가능하다.
식을 간단하게 하기 위해서 n/2 + 10 <= 3n /4임을 가정했는데 이는 n >= 40이어야 가능하다.
</pre>

cn(log3 - log4) + 20clog(3n / 4) + n이 음수가 되어야 하는데, c(log3 - log4) + 1을 충분히 작은 음수로 만들 수 있으면 20clog(3n/4)이 n에 대한 차수가 낮아 압도할 수 있음.<br>

**주의**<br>
<pre>
T(n) <= 2T(n/2) + n의 점근적 복잡도가 T(n) = O(n)이라고 가정하고, 충분히 큰 n에 대하여 T(n) <= cn인 양의 상수 c가 존재한다는 것을 증명하려 할 때
T(n) <= 2T(n/2) + n
     <= 2c(n/2) + n
     = cn + n
     = c'n
     = O(n)
</pre>
c' = c + 1이라는 추가적은 상수를 만들어서 c'n <= cn이라고 주장하면 안 됨.<br>
처음에 쓴 c와 나중에 쓰는 c는 같은 상수여야 함.<br>

<pre>
T(n) <= 2T(n/2) + n
     <= 2c(n/2) + n
     = cn + n
     = O(n)
</pre>
n(c+1) = O(n)인건 맞지만 T(n) <= cn + n <= cn임을 증명하지 못 했음.<br>
따라서 귀납적 증명이 완결되지 않았으므로 틀렸음.<br>

<pre>
T(n) = 2T(n/2) + 1의 점근적 복잡도는 O(n)이다.

실패하는 증명

충분히 큰 n에 대해 T(n) <= cn인 양의 상수 c가 존재한다는 것을 증명하려 한다.
T(n) = 2T(n/2)  + 1
     <= 2c(n/2) + 1
     <= cn + 1
여기서 cn + 1 < cn임을 입증할 수 없음.

성공하는 증명

T(n) <= cn임을 증명하는 대신 T(n) <= cn - 2임을 증명할 수 있어도 T(n) = O(n)임.<br>
T(n) = 2T(n/2) + 1
     <= 2(cn/2 - 2) + 1
     = cn -3
     <= cn
</pre>

추정 후 증명법이 유용하게 사용되려면 우선 추정을 의미있게 해야 함.<br>
너무 여유롭게 추정해 증명하는 것은 별로 의미가 없음.<br>

**추가: 경계조건**<br>

재귀적으로 정의된 함수나 수열은 자신의 값을 계산하기 위해 자신과 같은 함수나 수열을 더 작은 입력값으로 호출하는 경우가 많음.<br>
이러한 호출은 일반적으로 입력값이 충분히 작아지거나 특정 조건을 만족할 때까지 반복됨.<br>
이 때, 함수나 수열이 호출되는 과정에서 더 이상 호출하지 않고 종료될 때, 이를 '경계조건' 혹은 '종료조건'이라고 부름.<br>
예를 들어, 피보나치 수열은 아래와 같은 점화식으로 정의됨.<br>
<pre>
F(0) = 0
F(1) = 1
F(n) = F(n-1) + F(n-2) (n >= 2)
</pre>
이 때, F(0)와 F(1)은 경계조건으로 사용됨.<br>
F(0)와 F(1)을 미리 정의함으로써, F(n)을 계산하기 위한 종료 조건으로 사용할 수 있음.<br>

**:pushpin: 마스터 정리**

<pre>
T(n) = aT(n/b) + f(n)
</pre>

마스터 정리는 특정한 모양을 가진 재귀식에서 바로 결과를 알 수 있는 유용한 정리임.<br>
입력의 크기가 n인 문제를 풀기 위해 입력의 크기가 n/b인 문제를 a개 풀고, 나머지 f(n)의 오버헤드가 필요한 알고리즘들이 해당됨.<br>
a >= 1, b >= 1에 대해 T(n) = aT(n/b) + f(n)인 점화식에서, h(n) = n^(logb(a))라고 할 때 T(n)의 점근적 복잡도는 다음과 같음.<br>
<pre>
어떤 양의 상수 ε에 대하여 f(n) / h(n) = O(1/n^ε)이면, T(n) = θ(h(n))이다.
어떤 양의 상수 ε에 대하여 f(n) / h(n) = Ω(n^ε)이고, 어떤 상수 c(<1)와 충분히 큰 모든 n에 대해 af(n/b) <= cf(n)이면 T(n) = θ(f(n))이다. 
f(n) / h(n) = θ(1)이면 T(n) = θ(h(n)logn)이다.
</pre>

마스터 정리의 근사 버전은 아래와 같음.
<pre>
lim f(n) / h(n) = 0이면 T(n) = θ(h(n))이다.
lim f(n) / h(n) = ∞이고, 충분히 큰 모든 n에 대해 af(n/b) < f(n)이면 T(n) = θ(f(n))이다.
f(n) / h(n) = θ(1)이면 T(n) = θ(h(n)logn)이다.
</pre>

마스터 정리의 근사 버전은 원형과 정확히 같지는 않음.<br>
lim f(n) / h(n) = ∞은 h(n)이 f(n)을 압도한다는 뜻이고, f(n) / h(n) = O(1/n^ε)은 h(n)이 f(n)을 적어도 다항식의 비율로 압도한다는 뜻임.<br>
lim f(n) / h(n) = lim 1 / logn의 경우에는 lim f(n) / h(n) = 0이지만 logn의 비율로 압도할 뿐 다항식의 비율로 압도하지는 않음.<br>
즉. f(n) / h(n) = O(1/n^ε)은 성립하지 않음.<br>
lim f(n) / h(n) = ∞와 f(n) / h(n) = Ω(n^ε) 사이에도 다항식 비율에 관한 차이가 있음.<br>
충분히 큰 모든 n에 대해 af(n/b) < f(n)과 어떤 상수 c(<1)와 충분히 큰 모든 n에 대해 af(n/b) <= cf(n)도 고정 비율 c이하의 보장이라는 미묘한 차이가 있지만 반례를 찾기가 극히 힘들기 때문에 사실상 이 둘을 구분 없이 사용해도 무방함.<br>

<pre>
T(n) = 2T(n/3) + c (c는 상수)

a = 2, b =  3, f(n) = c, h(n) = n^log3(2)
lim f(n) / h(n) = 0이므로 T(n) = θ(n^log3(2))이다.
</pre>

<pre>
T(n) = 2T(n/4) + n

a = 2, b = 4, f(n) = n, h(n) = n^log4(2)
lim f(n) / h(n) = ∞이고 af(n/b) = 2(n/4) = n/2 < n임.
또한 n/2 <= (1/2)f(n)이므로 c = 1/2에 대해 af(n/b) <= cf(n)을 만족함.
따라서 T(n) = θ(nlogn)임.
</pre>

<pre>
T(n) = 2T(n/2) + n

a = 2, b = 2, f(n) = n, h(n) = n
f(n) / h(n) = 1이므로 T(n) = θ(nlogn)임.
</pre>

마스터 정리를 쓸 수 없는 모양인 점화식의 변수를 치환함으로써 마스터 정리를 쓸 수 있는 모양으로 변형하는 방법도 있음.<br>

<pre>
T(n) = 2T(n^(1/2)) + logn

m = log2(n)으로 놓으면

T(2^m) = 2T(2^(m/2)) + m

이는 아래와 같이 다시 표현할 수 있음.

S(m) = 2S(m/2) + m

a = 2, b = 2, f(m) = m, h(m) = m이므로
S(m) = θ(mlogm) = θ(logn loglogn)임.
</pre>

마스터 정리에서 f(n)은 크기가 n인 문제(최상위 레벨)에서 발생하는 자기 호출 이외의 오버헤드로 크기가 다른 문제들 간의 관계를 반영하는 비용임.<br>
h(n)은 반복적인 자기호출 끝에 마지막으로 크기 1인 문제를 만나는 횟수임.<br>
자기호출 때문에 부담이 더 커지면 수행 시간은 h(n)이 결정하고, 관계를 반영하는 오버헤드가 더 커지면 f(n)이 결정함.<br>
단 f(n)은 상위 레벨의 오버헤드만 의미하므로 하위 레벨에서 발생하는 오버헤드는 반영하고 있지 않음.<br>
af(n/b) <= cf(n)은 자기호출로 만나는 하위레벨의 문제들에서 발생하는 자기호출 이외의 오버헤드들의 총합이 레벨이 내려가면서 적어도 감소해야 한다는 것을 의미함.<br>

<h2><a id="4">:pencil2: Chapter4. 정렬</a></h2>

**:pushpin: 기본적인 정렬 알고리즘: 선택 정렬**

우선 배열 A[1, ..., n]에서 가장 큰 원소를 찾아 이 원소와 배열의 끝자리에 있는 A[n]과 자리를 바꿈.<br>
그러면 방금 맨 뒷자리로 옮긴 원소, 즉 가장 큰 원소는 자기 자리를 찾았으므로 더 이상 신경 쓰지 않아도 됨.<br>
이 원소는 정렬이 끝났다고 볼 수 있으므로 이제 이 원소를 제외한 나머지 원소들로 같은 작업을 반복함.<br>

<pre>
간략한 기술

selectionSort(A[], n)
{
  for last <- n downto 2 {
    A[1, ..., last]중 가장 큰 수 A[k]를 찾는다;
    A[k] <-> A[last];
  }
}
</pre>

<pre>
기호적 기술
selectionSort(A[], n)
{
  for last <- n downto 2 {
    k <- theLargest(A, last);
    A[k] <-> A[last];
  }
}

theLargest(A[], last)
{
  largeset <- 1;
  for i <- 2 to last
    if (A[i] > A[largest]) then largest <- i;
  return largest;
}
</pre>

이 알고리즘에서 입력은 배열 A[1, ..., n]임.<br>
변수 last는 정렬할 배열의 맨 마지막 인덱스, 즉 배열의 크기를 나타냄.<br>
처음에는 배열의 크기가 n으로 시작하므로, A[1, ..., n]을 정렬 대상으로 삼음.<br>
가장 큰 수를 찾아 제자리에 놓을 때마다 last는 1씩 줄어들음.<br>
선택 정렬의 수행 시간은 모든 경우에 Θ(n^2)임.<br>
배열의 크기가 n일 때 n-1번 비교, ... , 배열의 크기가 2일 때 1번 비교함.<br>
따라서 1 + ... + n-1 = n(n-1) / 2임.<br>
수를 비교하는 횟수가 전체 시간을 좌우하므로 이것을 기준으로 수행 시간을 계산함.<br>

**:pushpin: 기본적인 정렬 알고리즘: 버블 정렬**

<pre>
간략한 기술

bubbleSort(A[], n)
{
  for last <- n downto 2
    for i <- 1 to last-1
      if (A[i] > A[i+1]) then A[i] <-> A[i+1];
}
</pre>

버블 정렬의 총 순환 횟수는 (n-1) + (n-2) + ... + 2 + 1 = n(n-1) / 2임.<br>
따라서 수행 시간은 Θ(n^2)임.<br>
버블 정렬 알고리즘은 중간에 배열이 이미 정렬이 되어 있는 상태라도 계속 끝까지 무의미한 순환을 계속함.

<pre>
수정된 bubble sort

bubbleSort(A[], n)
{
  for last <- n downto 2
      sorted <- TRUE;
    for i <- 1 to last-1 {
      if (A[i] > A[i+1]) {
        A[i] <-> A[i+1];
        sorted <- FALSE;
      }
    }
    if (sorted = TRUE) then return;
}
</pre>

**:pushpin: 기본적인 정렬 알고리즘: 삽입 정렬**

삽입 정렬은 이미 정렬되어 있는 i개짜리 배열에 하나의 원소를 더 더하여 정렬된 i+1개짜리 배열을 만드는 과정을 반복함.<br>
한 개짜리 배열에서 시작하여 그 크기를 하나씩 늘리는 정렬임.<br>

<pre>
insertionSort(A[], n)
{
  for i <- 2 to n
    A[1, ..., i]의 적합한 자리에 A[i]를 삽입한다.
}
</pre>

for 루프는 문제의 크기를 하나씩 키워나가는 역할을 함.<br>
A[i]에 관심을 두는 시점에는 A[1, ..., i-1]은 항상 정렬이 되어 있음.<br>
A[i]가 A[i-1]보다 크면 앞에 있는 모든 원소보다 크므로 A[i]는 그냥 제자리에 두면 됨.<br>
그렇지 않으면 A[i-1]부터 시작해서 왼쪽으로 차례로 훑으면서 A[i]가 들어갈 자리를 찾음.<br>
A[i]가 들어가는 자리부터 시작해서 이후의 원소들은 한 칸씩 오른쪽으로 밀려남.<br>

<pre>
insertionSort(A[], n)
{
  for i <- 2 to n {
    loc <- i - 1;
    newItem <- A[i]
    
    while (loc >= 1 and newItem < A[loc]) {
      A[loc + 1] = A[loc];
      loc--;
    }
    A[loc + 1] <- newItem;
  }
}
</pre>

for 루프는 n-1번 순환함.<br>
매 for 루프에서 while은 최대 i-1번 순환함.<br>
가장 운이 좋으면 while 문은 돌아가지 않음.<br>
최악의 경우 수행 시간은 (n-1) + (n-2) + ... + 2 + 1 = n(n-1)/2임.<br>
따라서 Θ(n^2)임.<br>
보통은 대략 A[1, ..., i-1]에서 평균적으로 절반 정도를 훑고 끝낼 것임.<br>
그러므로 전체 비교 횟수는 최악의 경우에 비해 절반 정도 될 것임.<br>
그래도 시간복잡도는 Θ(n^2)임.<br>
<br>
삽입 정렬은 거의 정렬되어 있는 상태로 입력되는 경우에는 가장 매력적인 알고리즘임.<br>
배열이 완전히 정렬된 채로 입력되면 while 루프는 한 번도 수행되지 않고, for 루프는 한번 순환할 때마다 상수 시간이 소요됨.<br>
for 루프는 n-1번 순환되므로 Θ(n)에 가까운 시간이 듬.<br>
배열이 거의 정렬되어 있을 때도 삽입이 매우 수월해져 Θ(n)에 가까운 시간이 듬.<br>
버블 정렬은 배열이 이미 정렬되어 있는 경우에 무의미한 순환을 줄이기 위해 방법이 있긴 했지만 오버헤드가 생김.<br>
삽입 정렬은 별도 장치가 없어도 효율적으로 끝남.<br>
따라서 상황에 따라 가끔 삽입 정렬을 섞어서 씀.<br>
<br>
선택 정렬과 버블 정렬이 n개짜리 배열에서 시작하여 아직 정렬되지 않은 배열의 크기를 하나씩 줄이는 데 반하여, 삽입 정렬은 1개짜리 배열에서 시작하여 이미 정렬된 배열의 크기를 하나씩 늘리는 정렬임.<br>
삽입 정렬에는 수학적 귀납법의 원리가 들어가 있음.<br>
배열의 크기가 1일 때는 성립함.(이미 정렬되어 있으므로)<br>
배열의 크기가 k일 때 성립하면(정렬되어 있으면), 적절한 삽입으로 크기가 k+1일 때도 성립함(정렬됨).<br>
이것으로 삽입 정렬은 올바르게 정렬을 한다는 것이 귀납적으로 증명됨.<br>

**:pushpin: 고급 정렬 알고리즘: 병합 정렬**

병합 정렬은 먼저 입력을 반으로 나눔.<br>
이렇게 나눈 전반부와 후반부를 각각 독립적으로 정렬함.<br>
마지막으로 정렬된 두부분을 합쳐서, 즉 병합하여 정렬된 배열을 얻음.<br>
여기서 전반부, 후반부를 각각 정렬할 때도 역시 반으로 나눈 다음 정렬해서 병합함.<br>
즉, 원래의 정렬 문제와 성격이 똑같고 단지 크기만 반으로 줄였을 뿐임.<br>
병합 정렬은 자신에 비해 크기가 반인 문제를 두 개 푼 다음, 이들을 병합하는 일을 재귀적으로 반복함.<br>

<pre>
mergeSort(A[], p, r)
{
  if (p < r) then {
    q <- (p+r) / 2;
    mergeSort(A, p, q);
    mergeSort(A, q+1, r);
    merge(A, p, q, r);
  }
}

merge(A[], p, q, r)
{
  정렬되어 있는 두 배열 A[p, ..., q]와 A[q+1, ..., r]을 합쳐 정렬된 하나의 배열 A[p,...,r]을 만듬.
}
</pre>

<pre>
merge(A[], p, q, r)
{
  i <- p; j <- q + 1; t <- 1;
  while (i <= q and j <= r) {
    if (A[i] <= A[j])
    then tmp[t++] = A[i++];
    else tmp[t++] = A[j++];
  }
  
  while (i <= q)       //왼쪽 배열이 남은 경우
    tmp[t++] = A[i++];
  while (j <= r)       //오른쪽 배열이 남은 경우
    tmp[t++] = A[j++];
  i <- p; t <- 1;
  while (i <= r)
    A[i++] <- tmp[t++];
}
</pre>

T(n) <= a (if n = 1)<br>
T(n) <= 2T(n/2) + cn (if n > 1)<br>
부등호를 쓰는 이유는 좌변과 우변이 정확히 일치하지 않을 수 있기 때문임.<br>
따라서 근사적으로 나타내는 표현의 하나라고 생각하면 됨.<br>
상수 a는 크기가 1인 문제를 푸는 시간을 나타냄.<br>
상수 c는 병합에 드는 시간을 충분히 잡아주기 위해서 n에 곱한 것임.<br>
비교의 횟수만으로 수행 시간을 분석한다면 c=1로 충분함.<br>
어쨋든 병합은 선형 시간이 소요됨.<br>
n=2^k라고 가정하고 전개하면 다음과 같음.<br>

<pre>
T(n) <= 2T(n/2) + cn
     <= 2^2T(n/2^2) + 2cn
     ...
     <= 2^kT(n/2^k) + kcn
     = an + cn * logn
     = Θ(nlogn)
</pre>

병합 정렬의 수행 시간은 최악의 경우 Θ(nlogn)임.

**:pushpin: 고급 정렬 알고리즘: 퀵 정렬**

퀵 정렬은 평균적으로 가장 좋은 성능을 가져 현장에서 가장 많이 쓰는 정렬 알고리즘임.<br>
우선 정렬할 배열에서 기준원소를 하나 고름.<br>
아무 원소나 임의로 골라도 되나 여기서는 맨 뒤의 원소를 기준원소로 삼음.<br>
이 기준원소를 중심으로 더 작거나 같은 수는 왼쪽으로, 큰 수는 오른쪽으로 재배치함.<br>
기준원소는 이렇게 분할된 양쪽 부분 배열 사이에 자리하게 됨.<br>
이렇게 분할된 왼쪽 부분 배열을 따로 정렬함.<br>
마찬가지로 오른쪽 부분 배열도 따로 정렬함.<br>
기준원소는 손대지 말고 제자리에 그대로 둠.<br>
왼쪽과 오른쪽 부분 배열을 정렬할 때 퀵 정렬을 재귀적으로 사용함.<br>

<pre>
quickSort(A[], p, r)
{
  if (p < r) then {
    q <- partition(A, p, r);
    quickSort(A, p, q-1);
    quickSort(A, q+1, r);
  }
}

partition(A[], p, r)
{
  배열 A[p, ..., r]의 원소들을 A[r]을 기준으로 양쪽으로 재배치하고 A[r]이 자리한 위치를 리턴함.
}
</pre>

병합 정렬은 먼저 재귀적으로 작은 문제를 해결한 다음 후 처리를 하는데 반해서, 퀵 정렬은 선행 작업을 한 다음 재귀적으로 작은 문제를 해결하면서 바로 끝냄.<br>

<pre>
partition(A[], p, r)
{
  x <- A[r];
  i <- p -1;
  for j <- p to r-1
    if (A[j] <= x) then A[++i] <-> A[j];
  A[i+1] <-> A[r]
  return i+1;
}
</pre>

퀵 정렬의 수행 시간을 분석해보자.<br>
우선 분할은 배열을 왼쪽부터 끝까지 한 번 훑어나가는 작업이므로 Θ(n)의 시간이 듬.<br>
퀵 정렬의 수행에서 가장 이상적인 경우는 분할이 항상 반반씩 균등하게 될 때임.<br>
이 때는 T(n) = 2T(n/2) + Θ(n)이므로 병합 정렬과 같은 모양임.<br>
따라서 Θ(nlogn)이 됨.<br>
최악의 경우는 계속해서 한쪽은 하나도 없고, 다른 쪽에 다 몰리도록 분할이 되는 경우임.<br>
T(n) = T(n-1) + Θ(n)이므로 Θ(n^2)이 됨.<br>
한 쪽이 완전히 비거나 이에 근접한 상태가 반복되면 이런 비효율적인 시간이 나옴.<br>
퀵 정렬의 수행 시간은 분할이 얼마나 균형잡히게 잘 되느냐에 달려 있음.<br>
평균 수행 시간은 분할했을 때 모든 가능한 경우를 평균내면 됨.<br>
기준 원소가 1등이면 1구역과 2구역의 크기가 0:n-1, 2등이면 1:n-2, ..., i등이면 i-1:n-i임.<br>
T(n) = T(i-1) + T(n-i) + Θ(n)임.<br>
기준원소는 동일한 확률로 1등부터 n등 중의 하나가 되므로 이들을 평균하면 다음과 같음.<br>
T(n) = (1/n) * Σ(i=1~n)[T(i-1) + T(n-i)] + Θ(n) = (2/n) * Σ(k=0~n-1)T(k) + Θ(n)임.<br>
이를 계산하면 T(n) = Θ(nlogn)임.<br>

<pre>
위 내용을 증명하겠음.

우선 T(2) <= c2log2를 만족하도록 충분히 큰 c를 잡을 수 있음.
2 <= k <= n인 모든 k에 대해 T(k) <= cklogk가 성립한다 가정하고 T(n) <= cnlogn이 됨을 증명하면 됨.

T(n) = (1/n) * Σ(i=1~n)[T(i-1) + T(n-i)] + Θ(n)
     = (2/n) * Σ(k=0~n-1)T(k) + Θ(n)
     = (2/n) * Σ(k=2~n-1)T(K) + Θ(n)  // k=0, k=1일 때는 Θ(n)에 흡수됨.
     <= (2/n) * Σ(k=2~n-1)cklogk + Θ(n)
     <= (2c/n) * [(1/2) * n^2 * logn - (1/8) * n^2] + Θ(n) // 이 부분 아래에 증명하겠음.
     = cnlogn - cn/4 + Θ(n)
     <= cnlogn
     
Σ(k=1~n-1)klogk = Σ(k=1~(n/2-1))klogk + Σ(k=(n/2)~n-1)klogk
왼쪽 항의 logk는 log(n/2)을 상한으로 잡을 수 있고, 오른쪽 항의 logk는 logn을 상한으로 잡을 수 있음.
Σ(k=1~n-1)klogk <= log(n/2)Σ(k=1~(n/2-1))k + lognΣ(k=(n/2)~n-1)k 
                 = lognΣ(k=1~(n/2-1))k + lognΣ(k=(n/2)~n-1)k - Σ(k=1~(n/2-1))k 
                 = lognΣ(k=1~(n-1))k - Σ(k=1~(n/2-1))k
                 <= logn * n * (n-1) / 2 - (1/2) * (n/2) * (n/2 - 1)
                 <= (1/2) * n^2 * logn - (1/8) * n^2

Σ(k=2~n-1)klogk <= Σ(k=1~n-1)klogk이므로 Σ(k=1~n-1)klogk <= 1/2) * n^2 * logn - (1/8) * n^2임.
</pre>

<pre>
퀵 정렬이 제대로 정렬한다는 것을 귀납적으로 증명

n = 1
원소가 하나이므로 이미 정렬됨

n < k
quicksort가 제대로 정렬한다고 가정

n = k
partition에 의해서 세 부분으로 나뉨
왼쪽은 k보다 작고, 오른쪽도 k보다 작으므로 귀납적 가정에 의해서 제대로 정렬됨.
따라서 n=k일 때도 정렬됨.
</pre>

**:pushpin: 고급 정렬 알고리즘: 힙 정렬**

힙은 이진 트리로서 맨 아래 층을 제외하고는 완전히 채워져 있고, 맨 아래층은 왼쪽부터 꽉 채워져 있음.<br>
힙의 모든 노드는 하나씩의 값을 갖고 있는데, 다음 힙 성질을 만족함.<br>
각 노드의 값은 자기 자식의 값보다 작다(최소힙, 힙에 값이 같은 원소가 두 개 이상 있는 경우에는 작다 대신 작거나 같다)<br>
리프 노드는 자식이 없으므로 논리상 이 성질은 자동 만족됨.<br>
모든 노드가 이 성질을 만족하면, 이진 트리의 루트 노드에는 최솟값이 자리하게 됨.<br>
반대로 최대힙의 루트 노드에는 최댓값이 자리하게 됨.<br>
힙 정렬은 먼저 주어진 배열을 힙으로 만듬.<br>
그런 다음 힙에서 가장 작은 값을 차례로 하나씩 제거하면서 힙의 크기를 줄여나감.<br>
나중에 힙에 아무 원소도 남지 않으면 힙 정렬이 끝남.<br>
정렬은 힙에서 원소들이 제거된 순서대로 함.<br>

**힙 만들기**<br>

일반적으로 A[k]의 자식은 A[2k]와 A[2k+1]이 됨.<br>
A[k]의 부모는 A[k/2]이 됨.<br>
이렇게 부모자식 관계를 배열의 인덱스를 사용해 간단히 계산할 수 있으므로 링크나 포인터가 필요없음.<br>
n개의 원소를 가진 배열 A[1, ..., n]이 주어졌다고 하자.<br>
heapify(A, k, n)은 A[k]에 매달린 두 서브 트리가 힙성질을 만족하는 상태에서 A[k]를 루트로 하는 서브 트리 전체가 힙성질을 만족하도록 수선하는 함수임.<br>
루트의 두 자식 중 작은 값을 x, 큰 값을 y라고 하자.<br>
x를 루트와 비교한다.<br>
루트의 값이 x보다 크지 않으면 힙성질이 만족되어 수선은 끝남.<br>
루트의 값이 x보다 크면 x와 루트의 값을 맞바꿈.<br>
이제 x는 새로운 루트가 되었고, 루트의 값은 한 단계 내려옴.<br>
이렇게 루트가 한 칸 아래로 내려온 노드를 r이라 하자.<br>
여기서 다시 r을 새로운 루트로 삼아 이 작업을 재귀적으로 반복함.<br>
이런 식으로 내려갈 수 있는 곳까지 내려감.<br>
중간에 루트가 자식 중 작은 값보다 크지 않은 경우를 만나면 중단함.<br>

<pre>
buildHeap(A[], n)
{
  for i <- n/2 downto 1
    heapify(A, i, n);
}

heapify(A[], k, n)
{
  left <- 2k; right <- 2k+1;
  
  if (right <= n) then {    // k가 두 자식을 가지는 경우
    if (A[left] < A[right]) 
      then smaller <- left;
    else smaller <- right;            
  }
  
  else if (left <= n) then smaller <- left;
  else return;
  
  if (A[smaller] < A[k]) then {
    A[k] <-> A[smaller];
    heapify(A, smaller, n);
  }
}
</pre>

리프 노드는 그 자체로 힙성질을 만족하므로 buildHeap()은 리프가 아닌 노드 중 맨 뒤에서부터 루트로 삼아 heapify()를 수행함.<br>
n/2은 리프가 아닌 노드 중 맨 마지막 노드의 인덱스임.<br>

buildHeap()에 소요되는 시간은 Θ(n)임.<br>
heapify()는 해당 서브 트리의 높이가 시간을 좌우함.<br>
어떤 서브 트리도 높이가 log2(n)을 넘지 않으므로 heapify를 한번 수행하는데 O(logn)이 소요됨.<br>
그런데 buildHeap()에서 heapify()를 호출하는 횟수는 ⌊n/2⌋이므로 전체적으로 O(nlogn)이 됨.<br>
그러나 이는 과하게 잡은 상한임.<br>
모든 heapify()의 시간을 O(logn)으로 잡은 것이 과함.<br>
맨 처음 호출되는 heapify()의 입력 트리는 높이가 고작 1이고, 이런 것들이 꽤 여러 개 있음.<br>
그 다음 레벨로 올라가면 높이는 증가하지만, 높이가 높은 부분 트리의 수는 줄어들음.<br>
따라서 이를 합산하면 O(nlogn)이 아닌 Θ(n)이 됨.<br>
이에 대한 증명은 아래에 했음.<br>

<pre>
buildHeap()의 수행 시간 계산

heapify()는 힙의 높이에 비례하는 시간이 소요됨.
즉, 힙의 높이가 h라면 O(h) 시간이 소요됨.
원소의 수가 총 n개인 힙의 높이는 ⌊log2(n)⌋임.
높이가 h인 트리의 노드 수는 기껏해야 ⌈n/2^(h+1)⌉임.
따라서 buildHeap()의 수행 시간은 다음과 같음.

Σ(h=0~⌊log2(n)⌋)⌈n/2^(h+1)⌉O(h) = O(nΣ(h=0~⌊log2(n)⌋)h/2^h)=O(n)임.

O(logn)이 될 것이라는 직관과는 달리 O(n)이 됨.
Σ(h=0~⌊log2(n)⌋)h/2^h이 2보다 크지 않기 때문임.

Σ(h=0~∞)x^h = 1/(1-x)
이를 양변에 미분하면 다음과 같음.
Σ(h=0~∞)h*x^(h-1) = 1/(1-x)^2
양변에 x를 곱하면 다음과 같음
Σ(h=0~∞)h*x^h = x/(1-x)^2
h/2^h는 h*x^h에서 x가 1/2인 경우와 같음.
Σ(h=0~⌊log2(n)⌋)h/2^h <= Σ(h=0~∞))h/2^h
                      = (1/2) / (1/2)^2
                      = 2
따라서 2가 Σ(h=0~⌊log2(n)⌋)h/2^h의 상한임.
</pre>

힙이 완성되었으면 정렬 작업을 함.<br>
루트 노트에 있는 원소를 제거하여 다른 곳에 저장함.<br>
루트 노드가 없어졌으므로 트리의 크기가 하나 줄음.<br>
맨 끝에 있는 원소를 루트 노드로 옮겨 새로운 루트로 삼음.<br>
루트 노드로 옮긴 원소가 있던 자리에 방금 제거한 루트 노드 원소를 저장함.<br>
이것으로 대부분의 경우 루트 노드와 자식 간에 힙성질이 깨짐.<br>
heapify()를 이용해 힙성질을 만족하도록 수선함.<br>

<pre>
heapSort(A, n)
{
  buildHeap(A, n);
  for i <- n downto 2 {
    A[1] <-> A[i];
    heapify(A, 1, i-1);
  }
}
</pre>

buildHeap()은 Θ(n)의 시간이 듬.<br>
for 루프는 n-1번 순환하고 각 순환에서 시간을 좌우하는 heapify()는 충분히 잡아서 O(logn)의 시간이면 됨.<br>
그러므로 힙 정렬의 총 수행 시간은 O(nlogn)임.<br>

**비교 정렬 시간의 하한**<br>

원소끼리 비교하는 것으로만 정렬을 하는 것을 비교 정렬이라고 함.<br>
비교 정렬은 최악의 경우 수행 시간이 절대 Ω(nlogn)을 밑돌 수 없음.<br>
이것을 결정 트리 모델을 사용해서 증명할 수 있음.<br>
<br>
편의상 정렬하고자 하는 모든 원소가 다르다고 가정함.<br>
이것으로 일반성을 잃지는 않음.<br>
임의의 비교 정렬은 결정 트리에서 탐색으로 볼 수 있음.<br>
삽입 정렬을 결정 트리 모델로 그려봄.<br>

<pre>
if (a1 < a2)
  if (a2 < a3) return a1 < a2 < a3
  else
    if (a1 < a3) return a1 < a3 < a2
    else return a3 < a1 < a2
else
  if (a1 < a3) return a2 < a1 < a3
  else
    if (a2 < a3)  return a2 < a3 < a1
    else return a3 < a2 < a1
</pre>

리프노드를 만날 때까지 비교를 계속 함.<br>
결정 트리의 루트에서 시작해 리프에 이르면 정렬은 끝남.<br>
정렬 알고리즘은 입력 수열의 모든 가능한 경우에 대해 다 제대로 정렬을 해줘야 하므로 결정 트리의 리프 노드는 n!개가 되어야 함.<br>
n!에 관해서는 스털링의 근사식을 얻을 수 있음.<br>

<pre>
n! = √(2πn)(n/e)^n * (1 + Θ(1/n))

logn! = log( √(2πn)(n/e)^n * (1 + Θ(1/n)) )
      = log√(2πn)(n/e)^n + nlogn - nloge
      = Θ(nlogn)
</pre>

정렬의 수행에서 최악의 경우는 결정 트리에서 가장 깊은 리프 노드까지 내려가는 것임.<br>
n!개의 리프 노드를 가진 트리의 높이는 적어도 ⌈log2(n!)⌉임.<br>
따라서 최악의 깊이는 적어도 ⌈log2(n!)⌉은 되어야 하므로 최악의 경우 수행 시간은 이 전개식의 결론을 이용하면 적어도 Θ(nlogn)임.<br>
즉 Ω(nlogn)임.<br>

**:pushpin: 특수 정렬 알고리즘: 기수 정렬**

지금까지 배운 정렬 알고리즘들은 모두 원소 두 개를 비교함으로써 정렬을 하는 비교 정렬이었음.<br>
즉, 원소의 상대적인 대소 관계만 판단할 뿐이지 원소의 분포나 자릿수 등은 고려하지 않았음.<br>
입력 원소들이 특수한 성질을 만족하는 경우에는 Θ(nlogn)이란 한계를 극복할 수 있음<br>
<br>
기수 정렬은 입력이 모두 k 자릿수 이하의 자연수인 특수한 경우에 사용할 수 있는 방법으로 Θ(n) 시간이 소요되는 정렬 알고리즘임.<br>
우선 가장 낮은 자릿수만 가지고 모든 수를 재배열함.<br>
그런 다음 가장 낮은 자릿수는 잊어버림.<br>
그리고 앞과 같은 방법으로 더 이상 자릿수가 남지 않을 때까지 계속함.<br>
이렇게 하면 마지막에는 정렬된 배열을 갖게 됨.<br>

<pre>
radixSort(A[], n, k)
{
  for <- i to k
    i번째 자릿수에 대해 A[1, ..., n]을 안정성을 유지하면서 정렬함.
}
</pre>

"안정성을 유지하면서 정렬한다"는 것은 값이 같은 원소끼리는 정렬 후에 원래의 순서가 바뀌지 않는 성질을 뜻함.<br>
2150과 2154는 4번째 자릿수가 2로 똑같음.<br>
2150이 2154보다 앞에 있으면 4번째 자리에 대해 정렬했을 때 이 순서는 유지되어야 함.<br>
안정성을 유지하기 위해 다른 정렬 알고리즘을 쓰면 이미 Θ(n)을 초과해버리므로 다른 방법을 사용해야 함.<br>
예를 들어, 0부터 9까지 표시된 10개의 공간을 준비해놓고 각각의 수를 가진 입력은 해당 공간에 차례대로 넣어주는 등 이 부분을 O(n)에 끝내야 함.<br>
알고리즘은 이런 일을 k번 반복하는데 k가 상수이므로 전체 시간은 여전히 O(n)임.<br>

**:pushpin: 특수 정렬 알고리즘: 계수 정렬**

계수 정렬은 정렬하고자 하는 원소들의 값이 O(n)을 넘지 않는 경우에 사용할 수 있음.<br>
예를 들어, 배열 A[1, ..., n]의 원소들이 k를 넘지 않는 자연수인 경우를 들 수 있음.<br>
계수 정렬은 먼저 배열의 원소를 훑어보고 1부터 k까지의 자연수가 각각 몇 번 나타나는지를 셈.<br>
이 정보가 있으면 A[1, ..., n]의 각 원소가 몇 번째에 놓이면 되는지를 계산해낼 수 있음.<br>

<pre>
countingSort(A[], B[], n)
{
  for i <- 1 to k
    C[i] <- 0
  for j <- 1 to n
    C{A[j]]++;
  for i <- 2 to k
    C[i] <- C[i] + C[i-1]
  for j <- n downto 1 {
    B[C[A[j]]] <- A[j];
    C[A[j]]--;
  }
}
</pre>

알고리즘에서 A[1, ..., n]을 정렬한 결과가 배열 B[1, ..., n]에 저장됨.<br>
계수 정렬의 수행 시간은 Θ(n)임.<br>
첫 번째 for 루프는 Θ(k), 두 번째 for 루프는 Θ(n), 세 번째 for 루프는 Θ(k), 마지막 for 루프는 Θ(n)의 시간이 소요됨.<br>
k가 O(n)을 초과하면 시간은 Θ(k)가 됨.<br>
k가 O(nlogn)을 초과하면 계수 정렬은 병합 정렬, 퀵 정렬, 힙 정렬보다 매력이 없어짐.<br>
그래서 일반적으로 계수 정렬은 k가 O(n)을 초과하지 않는 경우에 선형 시간에 정렬하기 위해 사용함.<br>
정렬할 원소가 꼭 양수일 필요도 없음.<br>
원소들이 모두 -k와 k 사이의 정수이고 k가 O(n)일 경우에도 여전히 계수 정렬을 사용하여 선형 시간에 정렬할 수 있음.<br>

<h2><a id="5">:pencil2: Chapter5. 선택 알고리즘</a></h2>
  
**:pushpin: 평균 선형 시간 선택 알고리즘**

n개의 원소가 규칙 없이 저장된 배열에서 i번째 작은 원소를 찾으려 함.<br>
먼저 퀵 정렬에서 사용한 분할 알고리즘을 상기해봄.<br>
분할 알고리즘은 기준원소보다 작거나 같은 원소는 기준원소의 왼쪽 그룹으로, 기준원소보다 큰 원소는 기준원소의 오른쪽 그룹으로 재배치함.<br>
분할 알고리즘이 리턴하는 값으로 기준 원소가 전체에서 몇 번째 작은 원소인지 알 수 있음.<br>
이것으로 기준원소가 전체에서 k번째 작은 원소란 사실을 알았다고 하자.<br>
이제 i와 k의 값을 비교함.<br>
i가 k보다 작으면, i번째 작은 수는 왼쪽 그룹에 있는 원소 중 하나임.<br>
i가 k와 같으면, 기준원소가 바로 i번째 작은 수임.<br>
i가 k보다 크면, i번째 작은 수는 오른쪽 그룹에 있는 원소 중 하나임.<br>

<pre>
select(A, p, r, i)
{
  if (p=r) then return A[p];
  q <- partition(A, p, r);
  k <- q - p + 1;
  if (i < k) then return select(A, p, q-1, i);
  else if (i = k) then return A[q];
  else return select(A, q+1, r, i-k);
}
</pre>

기준 원소가 전체 집합에서 k번째 작은 원소이면 두 그룹은 각각 k-1개와 n-k개 나뉘고 알고리즘의 수행 시간은 다음과 같음.<br>
T(n) <= max[T(k-1), T(n-k)] + Θ(n)<br>
입력 배열은 가능한 모든 경우가 고루 일어난다고 가정하면, 전체 배열에서 기준원소의 순위 k는 1부터 n까지 동일한 확률을 갖음.<br>
이들의 평균을 위의 관계식에 반영하면 다음과 같음.<br>

<pre>
T(n) 
<= max[T(k-1), T(n-k)] + Θ(n)
<= (1/n)*∑(k=1~n)max[T(k-1), T(n-k)] + Θ(n)<br>
<= (2/n)*∑(⌊n/2⌋~(n-1))T(k) + Θ(n)<br>
⌊n/2⌋<=k<n인 모든 k에 대해 T(k) <= ck라 가정하면<br>
<= (2/n)*∑(⌊n/2⌋~(n-1))ck + Θ(n)<br>
=  (2/n) * (∑(1~(n-1))ck - ∑(⌊n/2⌋~(n-1))ck + Θ(n)<br>
=  (n/2) * [c * (n-1) * n / 2 - c * (⌊n/2⌋ -1) * ⌊n/2⌋ / 2] + Θ(n)
<= 2c / n * ((n-1) * n / 2 - (n / 2 -2) * (n / 2 - 1)) + Θ(n)
=  c(n-1) - (c/n) * (n^2/4 - 3*n/2 + 2) + Θ(n)
=  cn + (-cn /  4  + c / 2 - 2c / n + Θ(n))
<= cn
상수 c를 충분히 크게 잡으면 -cn/4이 Θ(n)을 압도해서 -cn /  4  + c / 2 - 2c / n + Θ(n)이 음수가 되도록 할 수 있음.
</pre>

그러므로 T(n)=O(n)임. T(n)=Ω(n)임은 명백하므로 T(n)=Θ(n)임.<br>
이 알고리즘의 경우 평균적인 경우 Θ(n)의 시간이 소요되지만, 최악의 경우에는 Θ(n^2)의 시간이 소요됨.<br>
최악의 예는 분할 결과 0:n-1로 계속 분할이 되고 찾고자 하는 원소가 운 나쁘게도 큰 그룹에 속하는 일이 반복되는 경우임.<br>
이 때 수행 시간의 점화식은 다음과 같음.<br>
T(n) = T(n-1) + Θ(n)<br>
이것을 전개하면 T(n)=Θ(n^2)이 됨.<br>
항상 이렇게 되지 않아도 이에 준할 정도로 자주 분할의 균형이 깨지면 역시 T(n)=Θ(n^2)이 됨.

**:pushpin: 최악의 경우에도 선형 시간을 보장하는 선택 알고리즘**
  
계속 1:9로 분할이 되고 이 중 나쁜 경우로 큰 그룹(9에 해당하는 부분)에서 탐색을 하게 된다고 하자.<br>
이 경우에는 다음 점화식으로 표현할 수 있음.<br>
T(n) = T(9n/10) + Θ(n)<br>
입력의 크기가 n인 문제를 풀기 위해 Θ(n)의 오버헤드를 사용한 다음 입력의 크기가 9n/10인 문제를 재귀적으로 호출한다.<br>
이것은 계산하면 T(n) = Θ(n)이 됨.<br>
1:99로 분할되어도 여전히 점근적 시간은 T(n) = Θ(n)이 됨.<br>
분할의 균형이 아주 나빠보여도 일정한 상수비만 넘지 않으면 점근적 복잡도는 항상 Θ(n)이 됨.<br>
이 절의 알고리즘은 분할의 균형을 어느 정도까지 보장함으로써 최악의 경우 Θ(n)을 보장함.<br>
그렇지만 분할의 균형만 적당히 맞춘다고 Θ(n)이 무조건 보장되는 것은 아님.<br>
균형을 맞추는 오버헤드가 너무 커져버리면 목표를 이룰 수 없음.<br>

<pre>
linearSelect(A, p, r, i)
{
  1. 원소의 총 수가 5개 이하인 i번째 원소를 찾고 알고리즘을 끝낸다.
  2. 전체 원소를 5개씩의 원소를 가진 ⌈n/5⌉개의 그룹으로 나눈다.(원소의 총수가 5의 배수가 아니면 이 중 한 그룹은 5개 미만이 된다.)
  3. 각 그룹에서 중앙값(원소가 5개이면 3번째 원소)를 찾는다. 이렇게 찾은 중앙값들을 m1, m2, ..., ,m⌈n/5⌉이라 하자.
  4. m1, m2, ..., m⌈n/5⌉들의 중앙값 M을 재귀적으로 구한다. 원소의 총수가 홀수이면 중앙값이 하나이므로 문제가 없고, 원소의 총수가 짝수이면 두 중앙값 중 임의로 선택한다.
  5. M을 기준원소로 삼아 전체 원소를 분할한다.
  6. 분할된 두 그룹 중 적합한 쪽을 선택해 단계 1~6을 재귀적으로 반복한다.
}
</pre>

x를 M보다 작은 원소들, o를 M보다 큰 원소들이라 하고 a를 M보다 크거나 작을 수 있는 원소들이라 가정하자.<br>
가장 바람직한 것은 a가 M의 대소 관계에 따라 왼쪽 그룹과 오른쪽 그룹으로 흩어지는 것이다.<br>
최악의 경우에는 모두 한쪽으로 몰릴 수도 있음.<br>
그럼 이런 최악의 경우에 분할의 균형은 어느 정도까지 나빠질 수 있을까?<br>
o그룹에는 적어도 3n/10-3개의 원소가 포함됨.<br>
M까지 포함하면 3n/10-2개임.<br>
이들을 제외한 나머지 원소는 많아야 n-(3n/10-2) = 7n/10 + 2개임.<br>
최악의 경우에는 이렇게 분할되고, 찾고자 하는 원소가 이 7n/10+2개짜리 그룹에 속함.<br>
이것으로 분할 비율은 최악의 경우에도 7n/10+2 : 3n/10-3이 되어 대략 7:3보다는 나빠지지 않을 수 있게 됨.<br>
select에서 이렇게 분할의 비율이 어느 정도 보장된다면 바로 선형 시간 알고리즘이 됨.<br>
그렇지만 이렇게 분할의 균형을 어느 정도 보장하기 위해서 무시 못할 오버헤드가 듬.<br>
이것은 좋은 기준 원소를 정하는 오버헤드임.<br>
이것으로 얻는 이득이 오버헤드를 극복한다면 좋은 결과를 이끌어낼 수 있음.<br>
<br>
단계 1은 원소의 총 수가 고작 5개 이하인 경우로 상수 시간이 소요될 뿐더러 반복적인 자기호출의 맨 마지막에 단 한 번만 수행되므로 전체 수행 시간에 영향을 주지 않음.
단계 2는 n개의 원소를 5개짜리 그룹으로 나누는 것이므로 각 원소를 한 번씩만 지나가면서 소속 그룹을 정해주면 되어 Θ(n)임.
단계 3은 각 그룹에서 중앙값을 찾는 데 상수 시간이 들고, 이런 작업을 ⌈n/5⌉번 하므로 Θ(n)임.
단계 5는 partition이므로 Θ(n)의 시간이 소요됨.
단계 2, 3, 5를 모두 합쳐서 Θ(n)이 소요됨.
단계 4와 단계 6은 자기호출을 하는 부분임.
즉, 동일한 linearSelect() 알고리즘을 호출하는데 단계 4는 입력의 크기가 ⌈n/5⌉, 단계 6은 입력의 크기가 최대 7n/10 + 2임.
<pre>
따라서 앞 알고리즘의 수행 시간 점화식은 다음과 같음.
T(n) <= T(⌈n/5⌉) + T(7n/10 + 2) + Θ(n)
여기서 기준 원소를 잘 선택하는 오버헤드 부분은  T(⌈n/5⌉) + Θ(n)에 해당함.
이 식을 전개하면 다음과 같음.
T(n) <= T(⌈n/5⌉) + T(7n/10 + 2) + Θ(n)
     <= T(n/5 + 1) T(7n/10 + 2) + Θ(n)
n0 <= k < n인 모든 k에 대해서 T(k) <= ck라고 가정하면 다음과 같음(n0는 경계치)
     <= c(n/5+1) + c(7n/10+2) + Θ(n)
     = c(9n/10 + 3) + Θ(n)
     = cn - cn/10 + 3c + Θ(n)
     <= cn
(-cn/10이 3c + Θ(n)을 압도할 수 있도록 하는 상수 c가 존재하기 때문에 성립함.)
따라서 T(n) <= cn이므로 T(n)=O(n)이다.
T(n) = Ω(n)임은 명백하므로 T(n) = Θ(n)임.
</pre>

n보다 작은 k에 대해서 T(k) <= ck라고 가정해서 위와 같은 전개가 가능했음.<br>
그런데 이것을 만족하려면 n/5+1과 7n/10+2가 각각 n보다 작아야함.<br>
이를 만족하려면 n이 7이상이면 됨.(즉, 경계치 n0가 7이상이 됨.)<br>
알고리즘의 점근적 시간 분석은 충분히 큰 n에 대한 것이므로 n이 7이상이라는 조건은 아주 가벼운 것임.<br>
n/5 <= k < n인 모든 k에 대해 T(k) <= ck라고 가정해도 됨.<br>

<h2><a id="6">:pencil2: Chapter6. 검색 트리</a></h2>

**:pushpin: 레코드, 키의 정의 및 검색 트리**

레코드는 개체에 대한 모든 정보를 포함하고 있음.<br>
사람의 레코드라면 주민번호, 이름, 주소 등이 담김.<br>
이 각각의 정보를 나타내는 부분을 필드라고 함.<br>
검색 트리에 레코드를 다 저장할 수도 있으나 보통은 해당 레코드를 대표할 수 있는 필드만으로 검색 트리를 만듬.<br>
사람 레코드의 경우 주민번호만 있으면 그 레코드를 대표할 수 있으므로 주민번호를 대표 필드로 삼아 트리를 만들 수 있음.<br>
이렇게 다른 레코드와 중복되지 않으면서 레코드를 대표할 수 있는 필드를 검색키 또는 키라고 함.<br>
키는 필드 하나로 구성할 수도 있고, 복수 개의 필드로 구성할 수도 있음.<br>
검색 색인을 만들기 위해서는 레코드 대신 키와 해당 레코드가 저장된 위치 정보만 있으면 됨.<br>
<br>
검색 트리는 한 노드에서 최대 몇 개의 자식 노드로 분기할 수 있느냐에 따라 이진 검색 트리와 다진 검색 트리로 나눔.<br>
이진 검색 트리는 최대 두 개의 자식 노드를 가질 수 있고, 다진 검색 트리는 세 개 이상의 자식 노드로 분기할 수 있음.<br>
일반적으로 k진 다진 검색 트리라 하면 자식을 최대 k개까지 가질 수 있는 검색트리를 뜻함.<br>
검색 트리는 저장되는 장소에 따라 내부 검색 트리와 외부 검색 트리로 나뉨.<br>
내부 검색 트리는 검색 트리가 메인 메모리 내에 존재하고, 외부 검색 트리는 검색 트리가 외부(주로 디스크)에 존재함.<br>
메인 메모리에서 모든 키를 수용할 수 있으면 검색 트리 전체를 메인 메모리로 한 번만 탑재한 후 내부 검색 트리로 사용할 수 있음.<br>
반면, 메인 메모리에서 모든 키를 수용할 수 없을 정도로 크면 디스크 공간에 저장된 상태로 검색을 해야 함.<br>
그러므로 외부 검색 트리의 경우에는 디스크 접근 시간이 검색의 효율을 좌우함.<br>
검색 트리는 검색키가 포함하는 필드의 수에 따라 일차원 검색 트리와 다차원 검색 트리로 나뉨.<br>
키를 구성하는 필드가 하나이면 일차원 검색 트리, 두 개 이상이면 다차원 검색 트리임.<br>
이진 검색 트리, 다진 검색 트리, B트리, AVL트리, 레드 블랙 트리 등은 모두 일차원 검색 트리임.<br>
KD트리, KDB트리, R트리 등은 다차원 검색 트리임.<br>

**:pushpin: 이진 검색 트리**

이진 검색 트리 특성
<pre>
1. 이진 검색 트리의 각 노드는 키 값을 하나식 갖음. 각 노드의 키 값은 모두 달라야함.
2. 최상위 레벨에 루트 노드가 있고, 각 노드는 최대 두 개의 자식 노드를 갖음.
3. 임의의 노드의 키 값은 자신의 왼쪽에 있는 모든 노드의 키 값보다 크고, 오른쪽에 있는 모든 노드의 키 값보다 작음.
</pre>

이진 검색 트리 검색
<pre>
treeSearch(t, x):
{
  if (t = NIL or key[t] = x) then return t;
  if (x < key[t])
    then return treeSearch(left[t], x);
    else return treeSearch(right[t], x);
}
</pre>

**:pushpin: 이진 검색 트리: 삽입**

원소 x를 이진 검색 트리에 삽입하려면 우선 이진 검색 트리에 x를 키 값으로 가진 노드가 없어야 함.<br>
원소 x를 삽입할 자리를 찾기 위해서는 우선 실패하는 검색을 한 번 수행해야 함.<br>
즉, 루트 노드에서 x에 대한 검색을 수행해 임의의 리프 노드에 이르러 더 이상 내려갈 곳이 없음이 확인되면 x를 그 리프 노드의 자식으로 매달음.<br>
이진 검색 트리의 모양은 원소들이 삽입되는 순서에 따라 결정됨.<br>
<br>
이진 검색 트리에서 삽입
<pre>
treeInsert(t, x):
{
  if (t = NIL) then {
    key[r] <- x; left[r] <- NIL; right[r] <- NIL;
    return r
  }
  
  if (x < key[t])
    then {left[t] <- treeInsert(left[t], x); return t;}
    else {right[t] <- treeInsert(right[t], x); return t;}
}
</pre>
n개의 원소로 이진 검색 트리를 만들 때, 이진 검색 트리가 가장 이상적으로 균형이 잡히면 최악의 경우라 하더라도 검색 시간은 Θ(logn)임.<br>
가장 나쁘게 기울면 평균 검색 시간이 Θ(n)이 됨.<br>
가능한 모든 삽입 순서에 따른 이진 검색 트리를 모두 고려하면 평균 검색 시간은 Θ(logn)임.<br>
삽입은 실패하는 검색 후 상수 시간의 후처리를 하므로 점근적 수행 시간은 검색과 동일함.<br>

Note
<pre>
n개의 키로 만들 수 있는 상대적인 순서는 모두 n!개임.
가능한 모든 순서를 고려할 때 임의의 키를 검색하는 데 필요한 평균 시간은 O(logn)임.
이진 트리에서 각 노드의 깊이를 더한 것을 내부 경로 길이(Internal Path Length,IPL)임.
7개의 노드가 있을 때 IPL이 26이면 평균 2.6번의 비교가 필요한 것임.
</pre>

평균 검색 시간 증명
<pre>
키의 총 수가 n개인 모든 이진 검색 트리의 평균 IPL은 O(nlogn)임.

D(n)을 키가 n개인 모든 이진 검색 트리의 평균 IPL이라고 하자.
D(0) = 0, D(1) = 1, D(2) = 3, D(n)의 점화식은 다음과 같음.

D(n) = (1/n) * ∑(i=0~n-1)(D(i) + D(n-i-1)) + n = (2/n) * ∑(i=0~n-1)(D(i)) + n

n = 2일 때 D(2) <= c2log2가 만족하도록 c를 잡을 수 있음.
모든 2 <= k < n에 대해 D(k) <= cklogk가 성립한다고 가정하자.(귀납적 가정)

D(n) = (2/n)∑(i=0~n-1)D(i) + n = (2/n)∑(i=2~n-1)D(i) + (2/n)D(1) + n
    <= (2/n)∑(i=0~n-1)cilogi + n + 2/n
    <= (2/n)∫(1~n)cxlogxdx + n + 2/n
    = (2c/n)[(1/2) * (n**2) * logn - (1/4) * (n ** 2) + 1/4] + n + (2/n)
    = cnlogn - cn/2 + c/2n + n + 2/n
    = cnlogn + (2-c)n/2 + (c+4) / 2n
    <= cnlogn

c = 3이면 2<=n이면 모든 n에 대해 D(n) <= cnlogn이므로 D(n) = O(nlogn)임.

이 정리는 임의 노드의 평균 깊이는 O(logn)이 됨을 의미함. 따라서 임의 노드를 검색할 때 성공적인 검색의 평균 시간은 O(logn)임.
</pre>

**:pushpin: 이진 검색 트리: 삭제**

노드 r을 삭제하고자 할 때는 다음 세 가지 경우에 따라 각각 다르게 처리를 해주어야 함.<br>
case 1: r이 리프 노드인 경우<br>
case 2: r의 자식 노드가 하나인 경우<br>
case 3: r의 자식 노드가 두 개인 경우<br>
<br>
이 중에서 r의 자식이 둘인 경우를 살펴보자.<br>
r의 부모가 r을 가리키던 포인터는 하나임.<br>
r자리에 옮겨놓아도 이진 검색 트리의 성질을 꺠지 않는 원소를 찾아야함.<br>
왼쪽 서브 트리에서 가장 큰 원소(크기 순으로 r의 직전 원소)와 오른쪽 서브 트리에서 가장 작은 원소(크기 순으로 r의 직후 원소)임.<br>
둘 중 하나를 택해 키를 r의 자리로 옮김.(여기서는 r의 직후 원소를 선택함.)<br>
그런 다음 직후 원소가 들어 있던 노드를 삭제함.<br>
다행히 직후 원소는 절대 왼쪽 자식을 가질 수 없음.<br>
그러므로 이 직후 원소의 삭제는 case1이나 case2에 속하게 되어 비교적 간단한 삭제 작업이 됨.<br>

<pre>
treeDelete(t, r, p):
{
  if (r=t) then root <- deleteNode(t);
  else if (r = left[p])
    then left[p] <- deleteNode(r);
    else right[p] <- deleteNode(r);
}

deleteNode(r)
{
  if (left[r] = right[r] = NIL) then return NIL;
  else if (left[r] = NIL and right[r] != NIL) then return right[r];
  else if (left[r] != NIL and right[r] = NIL) then return left[r];
  else {
    s <- right[r];
    while (left[s] != NIL)
      {parent <- s; s <- left[s];}
    key[r] <- key[s];
    if (s = right[r]) then right[r] <- right[s]
                      else left[parent] <- right[s];
    return r;
  }
}
</pre>

case1과 case2는 상수 시간이 들음.<br>
case3은 노드 r의 직후 원소를 찾는데 최악의 경우 트리의 높이에 비례하는 시간이 들음.<br>
직후 원소를 찾은 다음에 삭제하는 것은 case1 또는 case2에 해당되므로 상수 시간이 들음.<br>
따라서 삭제 작업을 위한 최악의 시간은 트리의 높이에 따라 O(logn)과 O(n) 사이에서 결정됨.<br>

**:pushpin: 레드 블랙 트리**

이진 검색 트리는 균형이 깨지면 Θ(n)에 근접한 시간이 소요될 수 있음.<br>
그래서 필요한 것이 균형잡힌 이진 검색 트리임.<br>
균형잡힌 이진 검색 트리는 최악의 경우에도 이진 트리의 균형이 잘 맞도록 유지함.<br>
레드 블랙 트리는 이진 검색 트리의 모든 노드에 레드 또는 블랙의 색상을 칠함.<br>
단 다음의 성질을 만족해야 함.<br>
<pre>
1. 루트는 블랙이다.
2. 모든 리프(NIL)은 블랙이다.
3. 노드가 레드이면 그 노드의 자식은 반드시 블랙이다.
4. 루트 노드에서 임의의 리프 노드에 이르는 경로에서 만나는 블랙 노드의 수는 모두 같다.
</pre>
노드 하나를 할당하여 이를 리프로 정하고 모든 NIL 리프에 대한 포인터가 이 노드를 가리키도록 처리함.<br>
이렇게 하면 공간을 절약할 수 있을 뿐더러 경계조건을 다루기도 편리해짐.<br>
레드 블랙 트리에서 검색은 트리의 내용을 건드리지 않으므로 이진 검색 트리에서 검색과 동일함.<br>
삽입과 삭제도 기본적으로는 이진 검색 트리와 동일하지만 삽입이나 삭제 후 레드 블랙 특성에 위반하는 경우가 생길 수 있음.<br>
이 때는 적절한 작업을 해서 레드 블랙 특성을 만족하도록 바로잡아 주어야 함.<br>

**:pushpin: 레드 블랙 트리: 삽입**

레드 블랙 트리에서 노드를 삽입할 때는 먼저 이진 검색 트리의 삽입 알고리즘에 따라 삽입을 한 다음 새 노드의 색상을 레드로 색칠함. 이 노드를 x라고 하자.<br>
새 노드는 항상 맨 아래쪽에 매달리므로 삽입 직후에 x의 아래쪽은 블랙 노드인 리프 2개만 있어 레드 블랙 특성에서 문제가 생기지 않음.<br>
x의 위쪽과 관련해서 문제가 생기는지만 확인하면 됨.<br>
x의 부모 노드 p가 블랙이면 그것으로 삽입은 완료됨.<br>
p가 레드인 경우만 해결하면 됨.<br>
p가 레드이면 레드 노드가 2개 연속으로 있으므로 레드 블랙 특성 3을 위반함.<br>
그런데 삽입 전에는 레드 블랙 트리였으므로 특성 3에 따라 p의 부모 노드는 반드시 블랙임. 이를 p^2이라 하자.<br>
마찬가지로 특성 3에 따라 x의 형제 노드도 반드시 블랙임.<br>
x 주변에서 레드나 블랙 두 가지 다 가능한 것은 p의 형제 노드 s뿐임.<br>
s의 색상에 따라 다음 두 가지 경우로 나뉨.<br>

<pre>
case 1: s가 레드
case 2: s가 블랙
case 2-1: x가 p의 오른쪽 자식
case 2-2: x가 p의 왼쪽 자식
</pre>

case1<br>
p와 s의 색상을 레드에서 블랙으로 바꾸고 p^2의 색상을 블랙에서 레드로 바꿈.<br>
p^2가 루트이면 p^2의 색상을 다시 블랙으로 바꾸고 끝냄.<br>
p^2가 루트가 아니면 p^2가 부모 색상을 확인해야 함.<br>
p^2의 부모 색상이 블랙이면 레드 블랙 특성이 모두 만족함.<br>
p^2의 부모 색상이 레드이면 레드 블랙 특성 3이 위반되어 처음과 똑같은 문제가 발생함.<br>
이것은 원래 x에 대해서 발생했던 문제와 똑같은 문제가 p^2에 대해서 발생했음을 뜻함.<br>
p^2을 문제 발생 노드로 하여 재귀적으로 다시 시작함.<br>
<br>
case2-1<br>
p를 중심으로 왼쪽으로 회전함.<br>
case2-2로 이동함.<br>
<br>
case2-2<br>
p^2을 중심으로 오른쪽으로 회전하고 p와 p^2의 색상을 맞바꿈.<br>
<br>
case2를 만나면 case2-2의 수선을 마지막으로 상황이 종료됨.<br>
case1을 만나면 상황이 끝날 수도 있고 똑같은 상황이 다른 노드에서 시작될 수도 있음.<br>
이런 상황이 재귀적으로 반복되어 루트까지 올라갈 수도 있음.<br>

**:pushpin: 레드 블랙 트리: 삭제**

레드 블랙 트리에서 노드를 삭제할 때는 기본적으로 이진 검색 트리에서 삭제 방법에 따라 노드를 삭제한 후 색상을 맞춰줌.<br>
이진 검색 트리에서 임의의 노드 d를 삭제할 때 d의 자식이 둘이면 d의 오른쪽 서브 트리에서 최소 원소(노드 d의 직후 원소)를 가진 노드 m의 키를 노드 d로 옮긴 다음 노드 m을 삭제함.<br>
노드 d의 색상을 건드리지 않은 채 키만 바뀌는 것은 레드 블랙 특성에는 영향을 미치지 않음.<br>
문제가 되는 것은 최소 원소 m을 삭제한 후 m 주변의 레드 블랙 특성의 위반 여부임.<br>
최소 원소 노드 m은 왼쪽 자식을 갖지 않음.<br>
따라서 최소 원소 노드 m은 최대 한 개의 자식만을 가질 수 있으므로 두 개의 자식을 가진 노드의 삭제 작업은 자식이 없거나 한 개만을 가진 노드의 삭제 작업으로 귀결됨.<br>
따라서 레드 블랙 트리에서 삭제 작업을 자식이 없거나 한 개만을 가진 노드의 삭제에 국한해도 무방함.<br>
<br>
삭제하려고 하는 노드 m의 자식을 x라고 하자. 자식이 없으면 x는 NIL 노드가 되는데 이를 구별할 필요는 없음.<br>
m은 자기 부모 노드의 왼쪽 자식일 수도 있고, 오른쪽 자식일 수도 있음.<br>
두 경우는 완전히 대칭적이므로 m이 자기 부모의 왼쪽 자식임을 가정하겠음.<br>
m이 레드이면 삭제 후 아무런 조치가 필요없음.<br>
따라서 m이 블랙인 경우만 고려하면 됨.<br>
m이 블랙이더라도 x가 레드이면 삭제 후 x의 색상을 블랙으로 바꾸어버리면 레드 블랙 특성을 만족함.<br>
m과 x의 색상이 모두 블랙일 때가 까다로움.<br>
m과 x가 블랙인 상태에서 m이 삭제되면 x는 m의 부모 p의 자식이 되고 루트에서 x를 통과하는 경로의 블랙 노드 개수가 한 개 모자라서 레드 블랙 특성 4가 깨짐.<br>
p의 색상에 따라 case1(레드)와 case2(블랙)으로 나뉨.<br>
p가 레드이면 s(x의 형제노드)는 반드시 블랙이고, l(x의 왼자식)과 r(x의 오른자식)은 모든 색상의 조합이 가능함.<br>

case1
<pre>
p가 레드이므로 s는 반드시 블랙. l의 색상, r의 색상에 따라
case 1-1: <블랙, 블랙>
case 1-2: <레드, 레드> 또는 <블랙, 레드> = <*, 레드>
case 1-3: <레드, 블랙>
</pre>

case2
<pre>
p가 블랙이므로, s의 색상, l의 색상, r의 색상에 따라
case 2-1: <블랙, 블랙, 블랙>
case 2-2: <블랙, 레드, 레드> 또는 <블랙, 블랙, 레드> = <블랙, *, 레드>
case 2-3: <블랙, 레드, 블랙>
case 2-4: <레드, 블랙, 블랙>
</pre>

이 때, case 1-2와 2-2는 p의 색상만 다름. p의 색상이 처리 방법에 영향을 미치지 않으므로 통합함.<br>
case 1-3과 2-3도 마찬가지 이유로 통합함.<br>

case 통합
<pre>
case 1-1: <블랙, 블랙, 블랙>
case *-2: <*, *, 레드> 
case *-3: <*, 레드, 블랙>
case 2-4: <레드, 블랙, 블랙>
</pre>

case 1-1<br>
단순히 p와 s의 색상을 맞바꿈. x에 이르는 경로상에서 블랙이 하나 추가되었으므로 x에 이르는 경로에서 블랙 노드가 하나 모자라던 것이 해소됨. 루트에서 s를 지나는 경로상의 블랙 노드의 수에는 변화가 없음. 특성 4가 만족됨.<br>
case *-2<br>
p를 중심으로 왼쪽으로 회전시키고, p와 s의 색상을 맞바꾼 다음, r의 색상을 레드에서 블랙으로 바꿈.<br>
x에 이르는 경로상에서 블랙이 하나 추가되었으므로 x를 지나는 경로에서 블랙 노드 하나가 모자라던 것이 해소됨.<br>
딸려있던 서브 트리들은 루트에서 지나가는 경로상에 있는 블랙 노드의 수에 변화가 없음.<br>
case *-3<br>
s를 중심으로 오른쪽으로 회전시키고 l과 s의 색상을 맞바꿈. case *-2로 이동함.<br>
case 2-1<br>
단순히 s의 색상을 블랙에서 레드로 바꿈.<br>
이제 2를 지나가는 경로에서도 블랙 노드가 하나 모자라게 되어 p를 지나가는 경로 전체에서 블랙 노드가 하나 모자르게 됨.<br>
이것은 원래 x에 대해서 발생했던 문제와 똑같은 문제가 p에 대해서 발생했음을 뜻함.<br>
p를 문제 발생 노드로 하여 재귀적으로 다시 시작함.<br>
case 2-4<br>
p를 중심으로 왼쪽으로 회전시키고 p와 s의 색상을 맞바꿈.<br>
l과 r을 경유하는 경로와 관련해서는 문제가 없음.<br>
다만 문제가 발생한 x의 부모 노드의 색상이 블랙에서 레드로 바뀜.<br>
case 1에 해당하게 되어 색상의 조합을 따져 case 1-1, 1-2, 1-3 중 하나로 이동함.<br>

**레드 블랙 트리의 작업 성능 분석**<br>

<pre>
키의 총 수가 n인 레드 블랙 트리의 가능한 최대 깊이는 O(logn)이다.

키의 총수가 n이라는 것은 레드 블랙 트리의 내부 노드의 수가 n임을 뜻함.
레드나 블랙의 색상을 고려하지 않을 때 가장 이상적으로 꽉 채워진 트리의 깊이는 ⌊logn⌋ + 1임.
그러므로 레드 블랙 트리가 아무리 잘 만들어져도 루트에서 임의의 리프에 이르는 경로에 존재하는 블랙 노드의 개수는 ⌊logn⌋ + 1을 넘을 수 없음.
레드 블랙 특성 3에 따라 레드 노드는 두 개가 연속해서 존재할 수 없으므로 루트에서 임의의 리프에 이르는 경로에서 블랙 노드의 개수보다 많을 수 없음.
그러므로 루트에서 임의의 리프에 이르는 경로의 길이는 2(⌊logn⌋ + 1)을 넘을 수 없음.
이는 O(logn)임. 따라서 키의 총 수가 n인 레드 블랙 트리의 가능한 최대 깊이는 O(logn)임.
</pre>

이 정리를 따라 레드 블랙 트리에서 검색에 소요되는 시간은 O(logn)이 됨.<br>
레드 블랙 트리에서 삽입과 삭제 모두 역시 O(logn)임.<br>
삽입의 경우 case2는 상수 시간에 끝남. case1은 case1을 재귀적으로 호출할 수 있으므로 최악의 겨웅 루트 노드까지 올라갈 수도 있음. 이 경우 O(logn)의 시간이 들음.<br>
삭제의 경우 case1-1, *-2, *-3은 상수 시간에 끝나고 case2-4는 case1-1, 1-2, 1-3 중 하나로 이동하므로 역시 상수 시간에 끝남.<br>
case2-1은 부모 노드에서 같은 상황이 다시 반복될 수도 있고, 루트 노드까지 올라갈 수도 있음.<br> 
이것이 최악의 경우인데 역시 O(logn)의 시간이 들음.<br>

**:pushpin: B-트리**

디스크에 접근하는 것은 메인 메모리로 접근하는 것에 비해 엄청난 시간이 들음.<br>
디스크 한 블록을 읽어오는 것은 기계어 명령을 처리하는 것에 비해 10만배의 시간이 소요됨.<br>
디스크에 데이터를 읽고 쓰기 위해서는 블록 단위로 접근을 함.<br>
단 한 바이트를 읽거나 쓰고 싶어도 한 블록을 통째로 읽어오거나 써야함.<br>
이 블록을 소프트웨어 레벨에서는 보통 페이지라고 함.<br>
검색 트리가 방대하면 검색 트리를 메모리에 모두 올려놓고 사용할 수가 없음.<br>
결국 검색 트리가 디스크에 있는 상태로 작업해야 하는데 이 경우에는 CPU 작업의 효율성보다 디스크 접근 횟수가 효율을 좌우함.<br>
검색 트리의 분기 수를 늘리면 검색 트리의 기대 깊이를 낮출 수 있음.<br>
검색 트리가 디스크에 있는 상태로 사용되면 이를 외부 검색 트리라함.<br>
분기의 수가 2개를 넘으면 다진 검색 트리라 함.<br>
B트리는 디스크 환경에서 사용하기에 적합한 외부 다진 검색 트리임.<br>
B트리의 한 노드에는 최대 k개까지 키가 크기 순으로 저장되어 있음.<br>
키가 k개 있으면 이 노드는 k+1개의 자식을 가짐.<br>
각각에 대응되는 서브 트리를 T0, T1, ..., Tk라고 하면 서브 트리 Ti의 모든 키들은 keyi-1보다 크고 keyi보다 작음.<br>
<pre>
B트리는 균형잡힌 다진 검색 트리로 다음 성질을 만족함.

루트를 제외한 모든 노드는 ⌊k/2⌋~k개의 키를 갖는다.
모든 리프 노드는 같은 깊이를 가짐.
</pre>
B트리는 분기의 수를 가능하면 늘리되 균형을 맞추기 위해 각 노드가 채울 수 있는 최대 허용량의 반 이상의 키는 채워야하는 검색 트리임.<br>
B트리에서 k의 값은 키의 크기와 분기를 위한 포인터에 필요한 공간의 양을 반영하여 디스크의 한 블록이 수용할 수 있는 한도 내에서 최댓값을 잡음.<br>
키와 분기를 위한 포인터 뿐만 아니라 임의의 키가 검색키와 일치했을 때 해당 키를 가진 레코드를 가져올 수 있는 페이지 번호가 같이 있어야 함.<br>
부모 노드로의 포인터도 하나 필요함.<br>
여기서 포인터는 모두 페이지에 대한 포인터이므로 페이지 번호를 사용함.<br>
예를 들어 디스크의 한 블록이 8,192 바이트이고, 키의 크기가 16바이트, 페이지 번호가 4바이트를 차지한다면, 최대 341개의 키 값을 가질 수 있음.<br>
이 경우 각 노드는 170~341개의 키를 갖음.<br>
이렇게 디스크 블록의 크기와 노드의 크기를 일치시키는 것은 디스크에서 정보를 읽어올 때 블록(페이지) 단위로 읽어오기 때문에 최대한 효율을 높이기 위해서임.<br>
<keyi, pi>에서 pi들은 레코드의 구체적인 위치가 아니고 해당 레코드가 저장된 페이지의 번호임.<br>
pi를 이용해서 페이지 pi를 통째로 메인 메모리에 가져온 후 그 안에서 해당 레코드를 찾아서 처리함.<br>
메인 메모리에서 해당 레코드를 찾기 위해 간단한 프로그램이 필요하지만 이것은 디스크에서 가져오는 시간에 비하면 무시할 수 있을 정도로 작은 시간임.<br>
레코드의 위치를 나타내기 위해 페이지 번호 대신 페이지 내에서 위치까지 나타낼 수도 있겠지만 이를 위한 공간이 더 필요하기 때문에 감수할 만한 가치가 없음.<br>

**:pushpin: B-트리: 삽입**

B트리에서 키 x에 대한 검색은 기본적으로 이진 검색 트리에서 검색과 같음.<br>
이진 검색 트리에는 각 노드에 키가 하나밖에 없지만, B트리에서는 최대 k개까지 키를 가질 수 있음.<br>
이진 검색 트리에서는 검색키가 노드의 키와 일치하는 것이 있는지 확인하는 반면, B트리에서는 노드의 여러 키 중 검색키와 일치하는 것이 있는지 확인함.<br>
이진 검색 트리에서는 유일한 키와 비교하여 왼쪽 또는 오른쪽 분기를 정하는 반면, B트리에서는 keyi-1 < x < keyi인 두 키를 찾아 분기를 해야 할 자식을 찾음.<br>
자식으로 분기를 하고 나면 깊이만 하나 내려간 똑같은 검색 문제가 됨.<br>
이것은 이진 검색 트리에서처럼 재귀 호출로 처리할 수 있음.<br>
<br>
B 트리에서 키 x를 삽입하는 작업의 개략적인 골격은 다음과 같음.
<pre>
1. x를 삽입할 리프 노드 r을 찾음.
2. 노드 r에 공간의 여유가 있으면 키를 삽입하고 끝냄.
3. 노드 r에 여유가 없으면 형제 노드를 살펴 공간의 여유가 있으면 형제 노드에 키를 하나 넘기고 끝냄.
4. 형제 노드에 여유가 없으면 가운데 키를 부모 노드로 넘기고 노드를 두 개로 분리함. 분리 작업은 부모 노드에서의 삽입 작업을 포함함.
</pre>
<pre>
Sketch-BTreeInsert(t, x):
{
  x를 삽입할 리프 노드 r을 찾는다.
  x를 r에 삽입한다.
  if (r에 오버플로 발생) then clearOverflow(r);
}

clearOverflow(r)
{
  if (r의 형제 노드 중 공간 여유가 있는 노드가 있음) then {r의 남는 키를 넘긴다};
  else {
    r을 둘로 분할하고 가운데 키를 부모 노드로 넘긴다;
    if (부모 노드 p에 오버플로 발생) then clearOverflow(p);
  }
}
</pre>

**:pushpin: B-트리: 삭제**

B트리에서 키 x를 삭제하는 작업의 개략적인 골격은 다음과 같음.
<pre>
x를 키로 갖고 있는 노드를 찾는다.
이 노드가 리프 노드가 아니면 x의 직후 원소 y를 가진 리프 노드 r을 찾아 x와 y를 맞바꾼다.
리프 노드에서 x를 제거한다.
x제거 후 노드에 언더플로가 발생하면 이를 적절히 해소한다.
</pre>

언더플로가 발생할 때는 우선 키를 가져올 수 있는 형제 노드가 있는지 봄.<br>
그런 노드가 있으면 가져다 채우고 끝냄.<br>
그렇지 않으면 형제 노드와 병합을 해야함.<br>
병합은 두 노드를 하나로 합치는 것이므로 부모 노드의 키 중 하나가 필요 없음.<br>
이 필요없는 키와 두 노드를 합쳐 하나의 노드로 만듬.<br>
이 병합의 결과키가 하나 줄어든 부모 노드에서 언더플로가 발생할 수 있음.<br>
앞에서 발생한 언더플로와 성격은 같지만 발생 장소가 다름.<br>
재귀적으로 처리할 수 있음.<br>

<pre>
Sketch-BTreeDelete(t, x, v):
{
  if (v가 리프 노드가 아님) then {
    x의 직후 원소 y를 가진 리프 노드를 찾음;
    x와 y를 바꿈;
  }
  리프 노드에서 x를 제거하고 이 리프 노드를 r이라 함;
  if (r에서 언더플로 바생) then clearUnderflow(r);
}

clearUnderflow(r)
{
  if (r의 형제 노드 중 키를 하나 내놓을 수 있는 여분을 가진 노드가 있음)
  then {r이 키를 넘겨받는다;}
  else {
    r의 형제 노드와 r을 병합하고 부모 노드에서 키를 하나 받는다;
    if (부모 노드 p에 언더플로 발생) then clearUnderflow(p);
  }
}
</pre>

d진 검색 트리가 균형을 잘 맞추면 높이가 logd(n)에 근접할 수 있음.<br>
B트리에서 임의의 노드가 최대 d개의 자식을 가질 수 있다면 최소한 ⌊d/2⌋개의 자식은 가져야 함.(루트만 예외)<br>
그러므로 B트리의 깊이는 최악의 경우에도 logd(n)보다 깊을 수는 없음.<br>
logd(n)과 logd/2(n) 사이에서 결정될 것임.<br>
<br>
B트리에서 검색은 당연히 O(logn)임.<br>
삽입 작업은 일단 실패하는 검색을 한 번 수행하므로 O(logn)이 듬.<br>
오버플로가 발생하지 않으면 트리의 맨 아래에 노드 하나를 추가하는 것은 상수 시간이 들음.<br>
오버플로가 반복적으로 발생해서 루트 노드까지 파급될 수도 있으므로 트리의 높이에 비례하는 시간이 들 수 있음.<br>
이것이 최악의 경웅이고 O(logn)임.<br>
검색과 오버플로 처리를 합하여 삽입 작업은 O(logn)임.<br>
삭제 작업은 직후 원소를 찾기 위해 O(logn)의 시간이 듬.<br>
언더플로가 발생하지 않으면 노드에서 단순히 키 하나를 제거하는 것은 상수 시간이 듬.<br>
언더플로가 반복적으로 발생해서 루트 노드까지 파급될 수 있으므로 트리의 높이에 비례하는 시간이 들 수 있음.<br>
이것이 최악의 경우이고 O(logn)임.<br>
직후 원소 찾기와 언더플로 처리를 합하여 삭제 작업은 O(logn)임.<br>

**:pushpin: KD 트리**

하나의 키가 여러 개의 필드를 사용해야 하는 경우가 있음.<br>
여러 개의 필드를 연결하여 하나의 필드처럼 다루는 방법도 있음.<br>
다만 이렇게 하면 각 필드가 갖는 의미를 활용하지 못 함.<br>
예를 들어, 하나의 키가 (x, y, z)와 같이 세 개의 필드로 이루어질 경우, y의 값에 따라 어떤 작업을 해보고자 할 때 x, y, z를 합쳐서 하나로 처리한 경우에는 이것이 가능하지 않음.<br>
키가 두 개 이상의 필드로 이루어지는 경우 각 필드를 그대로 검색에 사용하는 트리를 다차원 검색 트리라고 함.<br>
이 경우 키는 여러 개의 필드로 이루어진 벡터가 됨.<br>
<br>
KD 트리는 이진 검색 트리를 확장한 것으로 k개(k>=2)의 필드로 이루어지는 키를 사용함.<br>
KD 트리는 검색 트리의 각 레벨이 하나씩의 차원만을 다룸.<br>
루트 노드(레벨 0)는 첫 번째 필드만 사용해서 분기를 하고, 그 다음 레벨은 두 번째 필드만 사용해 분기를 하고, ..., 이런 식으로 동일한 레벨에 있는 노드는 모두 동일한 하나의 필드만 이용해서 분기를 함.<br>
레벨 0(루트 노드)에서는 필드 0만으로 왼쪽이나 오른쪽으로 분기를 함. 레벨 i에서는 필드 i(mod k)만으로 분기를 함.<br>
KD 트리에서는 모든 필드 값이 같을 수는 없지만 일부 필드 값이 같은 경우가 발생할 수 있음.<br>
임의로 정하는 것이지만 여기서는 필드 값이 같은 경우에는 오른쪽으로 분기하도록 함.<br>
KD 트리상의 각 노드는 다차원 공간의 한 점에 해당함.<br>
KD 트리에서 검색을 이용해 내려간다는 것은 다차원 공간에서 이렇게 나누어진 결과에 따라 공간의 범위를 점점 좁혀나가는 것임.<br>
<br>
KD 트리에서 검색은 임의의 키가 입력되면 각 필드를 차례대로 사용해서 트리를 검색해서 내려가면 됨.<br>
삽입도 검색하듯이 트리를 따라 내려가다 리프 노드를 만나면 거기에서 왼쪽 또는 오른쪽에 매달아주면 됨.<br>
KD 트리에서 삭제는 이진 트리에서 삭제에 비해 복잡함.<br>
<pre>
자식이 없는 경우: 이진 검색 트리와 마찬가지로 노드 r만 제거하면 됨.
자식이 하나뿐인 경우: 이진 검색 트리에서는 노드 r의 부모가 바로 노드 r의 자식을 가리키도록 하면 바로 끝남. 그러나 KD 트리에서 이처럼 하면 자식 노드를 루트로 하는 서브트리가 한 레벨 위로 이동하므로 분기에 사용하는 필드가 달라짐. 원래의 트리에서 노드 r과 자식 노드는 분기에 사용하는 필드가 다름. 따라서 아주 높은 확률로 KD 트리의 성질이 깨짐. KD 트리에서는 자식이 하나뿐이더라도 자식이 둘인 경우와 같은 방법으로 삭제를 해주어야 함.
자식이 둘인 경우: 이진 검색 트리에서는 노드 r의 오른쪽 서브 트리의 최소 키 값을 가진 노드를 삭제하고 그 노드를 노드 r의 자리로 이동함. KD 트리는 오른쪽 서브 트리 중 노드 r에서 분기에 사용한 필드값이 가장 작은 노드를 찾아 삭제하고 그 노드를 노드 r의 자리로 이동하면 됨.
왼쪽 자식만 하나 있을 때는 왼쪽 서브 트리 중 노드 r에서의 분기에 사용한 필드 값이 가장 큰 노드를 찾아 삭제하고 그 노드를 노드 r의 자리로 이동하면 됨.
</pre>

KD 트리에서 삭제 작업의 전형적인 과정
<pre>
1. 루트 A를 삭제하고자 한다.
2. A의 오른쪽 서브 트리에서 x값이 가장 작은 노드 C를 찾음.
3. 노드 A자리에 노드 C를 옮겨놓는다. 이제 원래 노드 C가 있던 자리가 비었으므로 이를 해결해야 함. 이것은 자리만 바뀌었을 뿐 노드 C를 삭제하는 작업이므로 1의 작업과 성격이 같음. 여기서부터는 재귀적인 작업임.
4. C가 있는 자리는 분기를 위해 y값을 사용하므로 C의 오른쪽 서브 트리에서 y값이 가장 작은 노드 L을 찾음.
5. L을 C의 빈자리로 옮김.
6. L은 리프 노드였으므로 L이 위로 옮겨감으로써 영향을 받는 자식이 없음. L에 대한 부모 노드의 연결만 NIL로 바꾸면 끝.
</pre>

KD 트리에서 x값 또는 y값이 가장 작은 노드를 찾는 작업은 이진 트리에서 유일한 경로를 따라 내려가는 것처럼 간단하지 않음.<br>
깊이보다는 서브 트리의 노드 수에 더 영향을 받는다고 볼 수 있음.<br>
A에서 C로 내려가면 C의 두 서브 트리는 무조건 다 내려가봐야함.<br>
노드 C는 y값으로 분기를 했기 때문에 C 후손들의 x값을 짐작할 수 없기 때문임.<br>
먼저 C의 왼쪽 노드인 E로 내려가면 오른쪽 서브 트리는 E의 x값보다 큰 x값을 가지므로 볼 필요가 없음.<br>
E를 확인한 후 백트래킹해서 C의 오른쪽 노드인 F로 감.<br>
F의 오른쪽 서브 트리는 마찬가지로 볼 필요가 없음.<br>
대상이 되는 노드 중 일부만 방문하고 최솟값을 찾을 수 있음.<br>
  
**:pushpin: KDB 트리**

KDB 트리는 다차원 검색을 다룰 수 있도록 B트리를 확장한 것임.<br>
B트리는 각 노드를 키로 분기하지만, KDB 트리는 각 노드를 영역으로 분기함.<br>
KDB 트리는 루트 노드부터 시작해 전체 공간을 쪼개가면서 색인을 함.<br>
B트리처럼 KDB 트리의 한 노드는 디스크의 한 페이지와 일치함.<br>
KDB 트리의 노드느는 다음 두 종류가 있음.<br>

<pre>
영역 노드: 복수 개의 (영역, 페이지 번호) 쌍으로 구성됨. 모든 내부 노드는 영역 노드임. 
예를 들어, 2차원 공간에서 영역은 (x축 최소값, x축 최대값)과 (y축 최소값, y축 최대값)으로 표현됨.
키 노드: 복수 개의 (키, 페이지 번호) 쌍으로 구성됨. 모든 리프 노드는 키 노드임.
예를 들어, 2차원 공간에서 키는 (x0, y0)의 형태로 표현됨.
</pre>

k차원의 키를 사용한다면 영역을 다음과 같이 표현됨.<br>
(<min0, max0>, <min1, max1>, ... , <mink-1, maxk-1>)<br>
즉, 각 차원에서 범위가 주어지고 영역은 이들을 모두 만족하는 공간을 뜻함.<br>
이차원이라면 영역은 직사각형 블록의 모양임.<br>
한 노드에 있는 영역들끼리는 겹치는 부분이 없음.<br>
또 한 노드에 있는 모든 영역을 합하면 다시 영역을 이룸.<br>
키는 다음과 같이 표현됨.<br>
(x0, x1, ..., xk-1)<br>

<pre>
예를 들어, 다음과 같은 2차원 좌표를 저장하려고 한다고 가정해보자.

A: (2, 3)
B: (4, 1)
C: (6, 8)
D: (8, 2)
E: (9, 6)

루트 영역 노드

영역 1: (<0, 5>, <0, 5>), 페이지 번호: 1
영역 2: (<5, 10>, <0, 5>), 페이지 번호: 2
영역 3: (<0, 10>, <5, 10>), 페이지 번호: 3

페이지 번호 1의 키 노드

키 A: (2, 3), 페이지 번호: 1A (데이터 레코드 A를 가리킵니다.)
키 B: (4, 1), 페이지 번호: 1B (데이터 레코드 B를 가리킵니다.)

페이지 번호 2의 키 노드

키 D: (8, 2), 페이지 번호: 2D (데이터 레코드 D를 가리킵니다.)

페이지 번호 3의 키 노드

키 C: (6, 8), 페이지 번호: 3C (데이터 레코드 C를 가리킵니다.)
키 E: (9, 6), 페이지 번호: 3E (데이터 레코드 E를 가리킵니다.)
</pre>

(영역, 페이지 번호)의 페이지 번호는 해당 영역을 커버하는 자식 노드의 페이지 번호를 뜻함.<br>
(키, 페이지 번호)의 페이지 번호는 해당 키를 가진 레코드가 저장된 디스크 페이지를 뜻함.<br>
KDB 트리의 내부 노드들은 모두 영역 노드이며 리프 노드들은 모두 키 노드임.<br>
B트리의 일종이므로 모든 리프 노드들의 깊이는 같음.<br>
루트는 전체 공간을 커버함.<br>
<br>
KDB 트리에서 키 검색은 루트 노드부터 시작해서 해당 키가 포함되는 영역을 따라 리프 노드까지 내려가면 됨.<br>
각 노드의 영역들은 서로 겹치는 부분이 없으므로 임의의 키로 도달하는 리프는 유일함.<br>
리프 노드에 해당 키의 (키, 페이지 번호) 정보가 있으면 해당 페이지로 가서 레코드를 가져옴.<br>
도달한 리프에 해당 키의 (키, 페이지 번호) 정보가 없으면 검색은 실패함.<br>
<br>
KDB 트리에서는 영역 검색도 가능함.<br>
검색 영역은 노드의 영역들처럼 다음과 같이 표현됨.<br>
(<min0, max0>, <min1, max1>, ... , <mink-1, maxk-1>)<br>
일부 차원은 <mini, maxi>가 해당 차원의 범위 전체를 다 커버할 수도 있음.<br>
루트 노드부터 시작해 검색 영역과 겹치는 부분이 있는 노드는 모두 방문해 확인함.<br>
이런 과정을 거쳐 리프 노드를 만나면 검색 영역에 속하는 모든 키의 레코드를 가져옴.<br>
검색 영역에 따라 아주 많은 수의 리프 노드들을 방문하게 되는 경우도 있음.<br>
<br>
키의 삽입은 우선 삽입할 키가 속하는 리프 노드를 찾음.<br>
해당 리프 노드가 키를 더 수용할 수 있는 공간이 있으면 (키, 페이지 번호) 쌍을 삽입하고 끝냄.<br>
리프 노드가 키를 더 이상 수용할 수 없을 경우, 형제 노드와 재분배할 수 있으면 재분배를 하고 작업은 끝남.(해당 리프의 부모 노드에서 영역의 경계를 조정하는 작업이 포함됨.)<br>
그러나 재분배가 불가능하면 리프 노드를 분할하여 두 개로 만듬.<br>
즉, 리프 노드에 대응되는 영역을 두 개로 나눔.<br>
이에 따라 부모 노드에 있던 한 영역이 두 개로 나누어지므로 (영역, 페이지 번호) 쌍이 하나 늘어남.<br>
부모 노드가 (영역, 페이지 번호)를 하나 더 수용할 공간이 있으면 작업은 끝남.<br>
부모 노드가 이것을 수용할 수 없으면 부모 노드를 분할함.<br>
노드를 분할할 때는 자식 노드에서 분할을 위해 사용된 차원의 분할선을 노드 x 전체에 적용하여 자름.<br>
노드 x가 분할되면 이번에는 x의 부모 노드에서 x를 가리키던 (영역, 페이지 번호)를 두 개로 나누어야 함.<br>
<br>
KDB 트리에서 삭제도 B트리와 유사함.<br>
삭제 후 언더플로가 생기지 않으면 작업은 끝임.<br>
언더플로가 발생하면 이웃 영역과 경계를 재조정해서 재분배할 수 있으면 재분해하고 작업을 끝냄.<br>
재분배가 가능하지 않은 경우 병합을 함.<br>
병합은 부모 노드에서 (영역, 페이지 번호) 쌍 두 개가 하나로 통합됨.<br>
부모 노드에서 이 때문에 언더플로가 발생하면 아래에서 발생한 언더플로와 성격은 같지만 위치만 다른 재귀적 상황이 됨.<br>

**:pushpin: R트리**

R트리도 KDB 트리처럼 다차원 검색을 다룰 수 있도록 B트리를 확장한 것임.<br>
KDB 트리에서는 노드들이 전체 공간을 나누어 커버하는 데 반해 R트리는 키들을 모두 포함하는 최소 영역만 노드에 있음.<br>
B트리 계열이므로 R트리의 한 노드도 디스크의 한 페이지와 일치함.<br>
R트리의 노드도 KDB 트리처럼 두 종류가 있음.<br>

<pre>
영역 노드: 복수 개의 (영역, 페이지 번호) 쌍으로 구성됨. 모든 내부 노드는 영역 노드임.
키 노드: 복수 개의 (키, 페이지 번호) 쌍으로 구성됨. 모든 리프 노드는 점 노드임.
</pre>

B트리에서 각 노드가 가질 수 있는 키 개수의 상한과 하한이 있었듯이, R트리의 노드에서도 영역 개수의 상한과 하한이 있음.<br>
R트리는 다음 성질을 만족함.<br>

<pre>
루트를 제외한 모든 내부 노드는 ⌊k/2⌋~k개의 영역을 갖음.
모든 리프 노드는 같은 깊이를 가짐.
모든 레코드는 리프 노드에서만 가리킴.
</pre>

R트리는 한 노드에 있는 영역들이 서로 겹칠 수도 있음.<br>
이 경우에는 검색의 경로가 유일하지 않을 수도 있음.<br>
이런 중복이 여러 레벨에 걸쳐서 반복될 수도 있음.<br>
R트리는 범위 검색도 가능함.<br>
부모 노드를 분리하고 나면 부모 노드에서 오버플로가 다시 일어날 수도 있음.<br>
아래에서 발생한 오버플로와 똑같은 상황이므로 재귀적으로 처리함.<br>
<br>
R트리에서 삭제도 B트리에서 삭제와 유사함.<br>
R트리에서 레코드 정보는 리프 노드에만 있으므로 삭제 대상은 항상 리프 노드에 있음.<br>
리프에서 (키, 페이지 번호) 쌍을 삭제한 후 언더플로가 발생하지 않으면 작업은 끝남.<br>
언더플로가 발생하면 바로 옆의 형제 노드와 재분배해서 조정을 시도함.<br>
재분배가 가능하지 않으면 바로 옆의 형제 노드와 병합을 함.<br>
병합은 부모 노드의 영역 수를 하나 줄임.<br>
그 결과 부모 노드에서 언더플로가 발생하지 않으면 작업은 완료됨.<br>
부모 노드에서 언더플로가 발생한다면 이 상황은 아래의 자식 노드에서 발생했던 언더플로와 똑같은 상황이므로 재귀적으로 처리함.<br>
<br>
한 노드에 있는 영역들이 서로 겹칠 수도 있음.<br>
이것으로 삽입할 때에 노드를 조정할 수 있는 융통성은 커지지만 레코드를 검색할 때 경로가 유일하지 않을 수 있음.<br>
이 문제를 개선하여 영역들이 겹치지 않도록 한 것이 R*트리임.<br>
R트리는 키로 대표되는 레코드를 저장하는 대신 도형을 저장 및 검색할 수도 있음.<br>
이 경우 리프 노드는 키 노드가 아닌 영역 노드와 같은 모양이 되며, 리프 노드에서 임의의 도형은 그것을 수용할 수 있는 가장 작은 영역으로 표현됨.<br>
  
**:pushpin: 그리드 파일**

그리드 파일은 키의 내용으로 레코드가 저장된 곳을 한 번에 알아낼 수 있도록 설계된 다차원 저장 및 검색 수단임.<br>
그리드 파일은 공간을 서로 배타적인 격자(그리드) 영역으로 나눈 다음 해당 영역에 속하는 레코드들을 모아서 저장함으로써 임의의 레코드에 대한 저장과 검색을 한 번에 할 수 있게 함.<br>
그리드 하나당 디스크의 한 페이지가 대응됨.<br>
어떤 레코드가 어느 그리드에 속하는지, 임의의 그리드에 대응되는 페이지느 어디인지는 일차 스케일링 배열과 그리드 배열로 알아냄.<br>
일차 스케일링 배열에는 각 차원의 어느 지점에서 그리드가 경계를 이루는지 정보가 들어있음.<br>
(50, 75) (30)은 x차원은 50과 75로 경계를 나누고, y차원은 30으로 경계를 나누었음을 뜻함.<br>
그리드 배열은 이 일차 스케일링 배열을 기초로 다음과 같이 만들어짐.<br>
<pre>
1 2 4
5 3 3
</pre>
총 여섯 개의 그리드로 나뉘고 각 그리드는 페이지 1, 2, 4, 5, 3, 3에 저장되어 있음.<br>
이 중 두 개의 그리드는 한 페이지(3)에 저장되어 있음.<br>
그리드 배열의 크기가 지나치게 크지 않으면 이것은 메인 메모리에 놓아두고 쓸 수 있으므로 그리드 파일에서 임의의 레코드를 검색하기 위해서는 단 한 번의 디스크 접근이면 충분함.<br>
데이터가 방대해서 그리드 배열의 크기가 메인 메모리에서 수용할 수 없을 정도이면 디스크에 두어야 하므로 임의의 레코드를 검색하기 위해서는 두 번의 디스크 접근이면 됨.<br>
일차원 스케일링 배열이 너무 커서 메인 메모리가 수용할 수 없는 경우는 현실적으로 없음.<br>
<br>
그리드 파일에서 검색은 그리드 배열에서 해당 레코드가 저장된 페이지를 알아낸 다음 해당 페이지에 가서 하면 됨.<br>
삭제도 해당 레코드를 페이지에서 제거하면 됨.<br>
임의의 페이지에 저장된 레코드의 수가 너무 작을 때는 인접한 그리드와 병합해서 두 개 이상의 그리드를 한 페이지에 저장할 수도 있음.<br>
경우에 따라 한 레코드가 한 페이지를 다 차지하거나 복수 개의 페이지를 차지할 수도 있음.<br>
이 때는 하나의 그리드에 대응되는 페이지에 레코드를 직접 저장하는 대신 레코드 인덱스, 즉 (키, 페이지 번호)들만 모아두면 됨.<br>
이 경우 임의의 레코드에 접근하기 위한 디스크 접근 횟수는 1회 증가하게 됨.<br>
그 대신 하나의 그리드 페이지에 소속되는 레코드 수가 크게 늘어나 그리드 배열의 크기를 많이 줄일 수 있으므로 그리드 배열을 메인 메모리에 두고 쓸 수 있는 가능성이 높아짐.<br>
이러면 앞에서 늘어난 디스크 접근 횟수를 상쇄할 수 있음.<br>
그리드 파일은 저장 위치가 키 값을 변형하지 않은 채 결졍되기 때문에 키 값을 이용한 "범위 검색"이 가능함.<br>
예를 들어, x값이 10~20 사이고, y값이 40~50 사이인 레코드들을 간단히 검색할 수 있음.<br>

<h2><a id="7">:pencil2: Chapter7. 해시 테이블</a></h2>
  
**:pushpin: 해시 테이블**
  
검색 트리는 원소가 저장될 자리가 이미 트리에 존재하는 원소와 비교하여 결정되는 반면, 해시 테이블은 원소가 저장될 자리가 원소의 값에 의해 결정되는 자료구조임.<br>
저장된 자료와 비교하여 자리를 찾지 않고 단 한 번의 계산으로 자신의 자리를 찾음.<br>
해시 테이블에 원소가 차 있는 비율을 적재율이라고 함.<br>
해시 테이블의 크기가 m이고, 저장된 원소의 총수가 n이면 적재율은 n/m이고 보통 a로 표기함.<br>
이미 원소가 들어있는 상태에서 새로운 입력이 들어오면 충돌이라고 함.<br>
  
**:pushpin: 해시 함수**

해시 함수는 두 가지 성질을 가지도록 만들어야 함.<br>
<pre>
입력 원소가 해시 테이블 전체에 고루 저장되어야 함.
계산이 간단해야 함.
</pre>
이 성질을 잘 만족해야 서로 다른 두 원소가 한 주소를 놓고 충돌할 확률이 작아짐.<br>

**나누기 방법**<br>

h(x) = x mod m<br>
여기서 m은 해시 테이블의 크기임.<br>
해시 테이블에 있는 m개의 자리가 0부터 m-1의 주소값을 가지므로 m으로 나눈 나머지 연산을 사용하는 것이 자연스러움.<br>
해시 테이블 크기 m은 2의 멱수에 가깝지 않은 소수를 택하는 것이 좋음.<br>
m = 2^p이면 입력 원소의 하위 p비트에 의해 값이 결정되므로 해시 값을 분산시키기에 이상적이지 않음.<br>
해시 값은 입력 원소의 모든 비트를 이용하는 것이 확률적으로 좋은 분포를 갖도록 하는데 유리함.<br>
나누기 방법은 해시 테이블 크기보다 큰 수를 해시 테이블 크기 범위에 들어오도록 수축시킴.<br>

**곱하기 방법**<br>

곱하기 방법은 반대임.<br>
입력값을 0과 1사이으이 소수로 대응시킨 다음 해시 테이블 크기 m을 곱하여 0부터 m-1 사이로 팽창시킴.<br>
이 방법에서는 해시 함수의 특성을 결정짓는 0 < A < 1 범위의 상수 A를 미리 준비해놓아야 함.<br>
<pre>
x에 A를 곱한 다음 소수부만 취한다.
방금 취한 소수부에 m을 곱하여 그 정수부를 취한다.
</pre>
h(x) = ⌊m(xA mod 1)⌋<br>
크기 m이 65,536인 해시 테이블에서 A = 0.6180339887인 경우 1,025,390의 해시값은?<br>
xA = 633,725.871673093임.<br>
소수부만 추출해서 65,536을 곱하면 57,125. ... 임.<br>
따라서 해시 주소는 57,125임.<br>
곱하기 방법은 나누기 방법과는 달리 해시 테이블의 크기 m을 아무렇게나 잡아도 상관없음.<br>
따라서 컴퓨터 이진수 환경에 맞게 m = 2^p로 잡는 것이 자연스러움.<br>
대신 상수 A를 어떻게 잡느냐에 따라 해시 값 분포가 많은 영향을 받음.<br>
(√5 - 1) / 2를 크누스가 제안하였는데 이 것이 0.6180339887...임.<br>

**:pushpin: 충돌 해결: 체이닝**

체이닝에서는 같은 주소로 해싱되는 원소를 모두 하나의 연결 리스트에 매달아서 관리함.<br>
해시 테이블 크기가 m이면 최대 m개의 연결 리스트가 존재할 수 있음.<br>
체이닝에서 임의의 원소를 연결 리스트에 삽입할 때는 해당 리스트의 맨 앞에 삽입함.<br>
연결 리스트 맨 앞이 아닌 다른 자리에 삽입하는 것은 효율성이 떨어짐.<br>
저장된 순서가 55, 42, 3, 94이면 연결 리스트 순서는 94, 3, 42, 55임.<br>
원소를 검색할 때는 해당 연결 리스트의 원소들을 차례로 지나가면서 찾음.<br>
원소를 삭제할 때는 연결 리스트에서 해당 원소를 삭제함.<br>
체이닝은 적재율이 1을 넘어도 사용할 수 있음.<br>

<pre>
chainedHashInsert(T[], x):
{
  리스트 T[h(x)]의 맨 앞에 x를 삽입;
}

chainedHashSearch(T[], x):
{
  리스트 T[h(x)]에서 x값을 가지는 원소를 검색;
}

chainedHashDelete(T[], x):
{
  리스트 T[h(x)]에서 x의 노드를 삭제;
}
</pre>
  
**:pushpin: 충돌 해결: 개방 주소 방법**

개방 주소 방법은 체이닝과 같은 추가 공간을 허용하지 않음.<br>
충돌이 일어나더라도 어떻게든 주어진 테이블 공간에서 해결함.<br>
따라서 모든 원소가 반드시 자신의 해시값과 일치하는 주소에 저장된다는 보장이 없음.<br>
임의의 원소를 해시 테이블에 삽입하는 과정을 살펴보자.<br>
먼저 해시 함수를 계산함.<br>
계산된 주소를 차지하고 있는 다른 원소가 없으면 그 자리에 넣음.<br>
그 자리에 다른 원소가 있으면 충돌이 일어난 것임.<br>
정해진 규칙에 따라 다음 자리를 찾는데 빈 자리를 찾을 때가지 계속 찾음.<br>
이것을 순차적인 해시함수로 볼 수 있고 h0(x), h1(x), ... 식으로 표현할 수 있음.<br>
이 중 h0(x)는 h(x)를 뜻함.<br>

**선형 조사**<br>

가장 간단한 충돌 해결 방법으로 충돌이 일어난 바로 뒷자리를 보는 것임.<br>
i에 관한 일차 함수의 보폭으로 점프를 함.<br>
hi(x)는 h(x)에서 i만큼 떨어진 자리가 됨.<br>
테이블의 경계를 넘어갈 경우에는 맨 앞으로 가면 됨.<br>
hj(x) = (h(x) + i) mod m<br>
선형 조사의 경우 특정 영역에 원소가 몰릴 때는 치명적으로 성능이 떨어짐.<br>
이런 현상을 1차 군집이라고 함.<br>
영역이 커질수록 해당 영역으로 해싱될 확률이 커지므로 군집이 심하지 않은 영역에 비해 영역의 크기가 빨리 커짐.<br>
두 개의 군집된 영역이 붙으면 영역은 갑자기 커지기도 함.<br>

**이차원 조사**<br>

바로 뒷자리를 보는 대신에 보폭을 이차 함수로 넓혀가면서 봄.<br>
i번째 해시 함수를 h(x)에서 i^2만큼 떨어진 자리로 삼을 수 있음.<br>
즉, h(x), h(x) + 1, h(x) + 4, h(x) + 9, ...와 같이 볼 수 있음.<br>
hi(x) = (h(x) + c1i^2 + c2i) mod m<br>
특정 영역에 원소가 몰려도 그 영역을 빠르게 벗어날 수 있음.<br>
그러나 여러 개의 원소가 동일한 초기 해시 함수 값을 가지게 되면 모두 같은 순서로 조사를 할 수 밖에 없어 비효율적임.<br>
이를 2차 군집이라고 함.<br>

**더블 해싱**<br>

두 개의 함수를 사용함.<br>
hi(x) = (h(x) + i*f(x)) mod m<br>
h(x)와 f(x)는 서로 다른 해시 함수임.<br>
충돌이 생겨 다음에 볼 주소를 계산할 때 두 번째 해시 함수 값만큼씩 점프함.<br>
두 원소의 첫 번째 해시 값이 닽더라도 두 번째 함수 값이 같을 확률은 매우 작으므로 서로 다른 보폭으로 점프함.<br>
그러므로 2차 군집 문제는 발생하지 않음.<br>
권장하는 방법은 h(x) = x mod m으로 잡고, m보다 조금 작은 소수 m'에 대해 f(x) = 1 + (x mod m')으로 잡는 것임.<br>
h(x)와 h(y) 값은 같지만 f(x)와 f(y)는 달라 두 번째 보는 자리는 서로 달라짐.<br>
두 번째 해시 함수 값 f(x)가 해시 테이블 크기 m과 서로소인 값이어야 함.<br>
f(x)와 m이 1보다 큰 최소공약수 d를 가지면 x의 자리를 찾기 위해 해시 테이블 전체 중 기껏해야 1/d밖에 보지 못 함.<br>
해시 테이블 크기 m을 소수로 잡고, f(x) 값이 항상 m보다 작은 자연수가 되도록 하면 이들은 항상 서로소가 되므로 만족시키기 어려운 조건은 아님.<br>

**개방 주소 방법 알고리즘**<br>

<pre>
hashInsert(T[], X)
{
  i <- 0;
  repeat {
    j <- hi(x);
    if (T[j] = NIL or T[j] = DELETED)
      then {T[j] <- x; return j;}
      else i++;
  } until (i = m)
  error "테이블 오버플로";
}

hashSearch(T[], x)
{
  i <- 0;
  repeat {
    j <- hi(x);
    if (T[j] = x)
      then return j;
      else i++;
  } until (T[j] = NIL or i = m)
  return NIL;
}
</pre>

개방 주소 방법은 테이블에 주어진 공간만 사용할 수 있으므로 적재율이 1을 넘을 수는 없음.<br>
적재율이 높아지면 효율이 급격히 떨어지므로 적당한 임계점을 설정한 후 그것을 넘으면 해시 테이블의 크기를 대략 두 배로 키우고 모든 원소를 다시 해싱하는 것이 일반적임.<br>
m이 13이고 0, 1, 12가 이미 차있는 경우, 1을 삭제한 후에 38을 검색하면 주소 12, 0을 거쳐 1에서 빈자리를 발견하게 되어 38은 없다고 판단함.<br>
이를 방지하기 위해 DELETE라는 상수 값을 저장하여 삭제된 자리라는 것을 표시해줌.<br>
DELETE는 삽입 과정에서 이를 만나면 그 자리에 원소를 넣으면 되기 때문에 낭비적이지 않음.<br>
  
**:pushpin: 해시 테이블에서 검색 시간 분석**

저장된 다른 원소와 관계없이 임의의 원소가 테이블상의 모든 위치에 같은 확률로 해시된다고 가정하자.<br>

**정리 7-1**<br>

체이닝 방법을 이용하는 해싱에서 적재율이 a일 때, 실패하는 검색에서 조사 횟수의 기대치는 a이다.<br>
<br>
적재율이 a이면 각 연결 리스트에 매달린 원소 개수의 기대치는 a이다.<br>
그러므로 찾고자 하는 원소가 없을 때는 평균적으로 a개의 원소를 조사하고 나면 끝임.<br>
<br>
해당 연결 리스트에 원소가 하나도 없다면 아무런 조사없이 끝ㄴ남.<br>
여기서 연결 리스트의 헤드를 지나가는 것은 고려하지 않았는데 이것을 한 번의 조사로 간주하면 결과에 1을 더하면 됨.<br>
성공하는 검색의 경우는 해당 리스트에 적어도 하나의 원소가 있으므로 1보다 작을 수는 없음.<br>

**정리 7-2**<br>

체이닝을 이용하는 해싱에서 적재율이 a일 때, 성공하는 검색에서 조사 횟수의 기대치는 (1+a)/2 + a/2n임.<br>
<br>
검색이 성공하므로 마지막에는 검색하는 원소를 조사하고 끝냄.<br>
그러므로 적어도 한 번은 조사함.<br>
각 원소가 저장될 때는 리스트의 맨 앞에 삽입되므로 원소 x가 저장된 다음 몇 개의 원소가 x와 같은 리스트에 저장되었는지 알면 됨.<br>
x와 같은 리스트에 저장되는 원소는 해시 값이 x의 해시 값과 일치하는 원소임.<br>
해시 함수가 잘 설계되었다면 임의의 두 원소가 해시값을 가질 확률은 1/m임.<br>
해시 테이블에 총 n개의 원소가 저장되었다면 적재율 a = n/m임.<br>
원소 x가 n개의 원소 중 i번째로 저장되었다면 x보다 먼저 저장된 원소는 같은 해시 값을 갖더라도 연결 리스트에서 원소 x보다 뒤에 매달리므로 x를 성공적으로 검색하는 데 전혀 영향을 미치지 않음.<br>
문제는 x보다 뒤에 저장되었으면서 x와 충돌을 일으킨 원소들임.<br>
이들은 모두 리스트에서 x의 앞에 매달리므로 이들을 지나가야만 x에 이를 수 있음.<br>
원소 x가 i번쨰로 저장되었다면 x를 검색하기 위해 필요한 조사 횟수는 1 +  ∑(j=i+1 ~ n)1/m임.<br>
이를 모든 원소에 대해 평균하면 (1/n)∑(i=1~n) * (1 + ∑(j=i+1 ~ n)1/m)임.<br>
<pre>
(1/n)∑(i=1~n) * (1 + ∑(j=i+1 ~ n)1/m) =
1 + (1/mn) * ∑(i=1~n) * ∑(j=i+1 ~ n) 1 =
1 + (1/mn) * ∑(i=1~n) (n - i) =
1 + (n-1) / 2m =
1 + a / 2 - a / 2n
</pre>
체이닝의 경우 각 원소가 자신이 해시된 주소에서만 저장되므로 다른 주소에는 영향을 전혀 미치지 않지만, 개방 주소 방법에서는 자신이 해시되지 않은 주소에 저장될 수 있으므로 복잡해짐.<br>
개방 주소 방법에서 조사 순서 h0(x), h1(x), ..., hm-1(x)가 0부터 m-1 사이의 수로 이루어진 순열로 이루고, 모든 순열은 같은 확률로 일어난다고 가정하자.<br>
즉, 이 조사 순서가 테이블의 모든 위치를 각각 한 번씩 중복없이 볼 수 있고, 보는 순서는 임의의 순서라는 뜻임.<br>

**정리 7-3**<br>

해시 함수가 위와 같은 특성을 만족한다고 할 때, 적재율 a = n/m < 1인 개방 주소 해싱의 실패하는 검색에서 조사 횟수의 기대치는 최대 1/(1-a)이다.<br>
<br>
두 확률 변수 pi, qi를 다음과 같이 정의하자.<br>
pi = 빈자리를 찾기 전에 정확히 i번 이미 점유된 주소를 조사할 확률<br>
qi = 빈자리를 찾기 전에 적어도 i번 이미 점유된 주소를 조사할 확률<br>
조사 횟수의 기대치는 1 + ∑(i>=0)ipi가 됨.<br>
여기서 1은 마지막에 빈자리를 보는 것을 의미함.<br>
qi = (n/m) * (n-1) / (m-1) * ... * (n-i+1) / (m-i+1)<br>
pi = qi - q(i+1)<br>
qi = (n/m) * (n-1) / (m-1) * ... * (n-i+1) / (m-i+1) <= (n / m) ^ i = a^i<br>
1 + ∑(i>=0)ipi = 1 + 1 + ∑(i>=1)i(qi - q(i+1))<br>
               = 1 + ∑(i>=1)qi<br>
               <= 1 + ∑(i>=1)a^i<br>
               = 1 / (1-a)<br>
               
**정리 7-4**<br>

해시 함수가 위와 같은 특성을 만족한다고 할 때, 적재율 a = n/m < 1인 개방 주소 해싱의 성공하는 검색에서 조사 횟수의 기대치는 최대 (1/a) * log(1/(1-a))이다.<br>
<br>
i번째 원소가 삽입된 직후의 적재율은 i/m임.<br>
x가 i+1번째로 삽입된 원소라면 x가 삽입될 당시 실패하는 검색 후에 자리를 잡았을 것이므로, 정리 7-3에 따라 조사 횟수의 기대치는 최대 1 / (1 - 1/m) = m / (m-i)임.<br>
이를 모든 경우에 대해 평균하면 다음과 같음.<br>
(1/n) * ∑(i=0~n-1)m/(m-i) <= <br>
(1/a) * ∫(0~n)1/(m-x)dx = <br>
(1/a) * log(1 / (1-a))<br>
이론적으로는 체이닝이 개방 주소 방법보다 평균 조사 횟수가 적음.<br>
앞의 정리가 아니더라도 직관적으로 짐작할 수 있음.<br>
개방 주소 방법의 경우 자신과 직접 충돌을 일으키지 않은 원소라도 검색 과정에 방해를 할 수 있음.<br>
반면 체이닝은 충돌을 일으킨 원소만 같은 연결 리스트에 매달려 충돌을 일으키지 않은 원소들끼리는 검색에 지장을 주지 않음.<br>
그렇지만 이론적 성질이 조금 낫다고 해서 체이닝이 가장 매력적인 것은 아님.<br>
체이닝은 각 연결 리스트마다 헤드를 하나씩 두어야 하고 연결 리스트를 만들기 위해 각 원소마다 연결 공간이 필요함.<br>
따라서 적재율이 그리 높지 않을 때(예, 1/2이하)는 개방 주소 방법이 더 매력적인 경우가 많음.<br>
개방 주소 방법은 적재율이 절대로 1을 넘지 못 하는 반면, 체이닝은 제한이 없음.<br>
어떤 경우든 적재율이 높으면 해싱의 효율이 떨어지게 되므로 낮은 적재율을 유지할 필요가 있음.<br>
