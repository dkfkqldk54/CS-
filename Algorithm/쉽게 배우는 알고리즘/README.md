<h1>:green_book: <쉽게 배우는 알고리즘> 정리</h1>

<a href="#2">:pencil2: Chapter2. 알고리즘 설계와 분석의 기초</a>
- O 표기법
- Ω 표기법
- Θ 표기법
- o 표기법
- ω 표기법

<a href="#3">:pencil2: Chapter3. 점화식과 알고리즘 복잡도 분석</a>
- 반복 대치
- 추정 후 증명
- 마스터 정리

<a href="#4">:pencil2: Chapter4. 정렬</a>
- 기본적인 정렬 알고리즘: 선택 정렬
- 기본적인 정렬 알고리즘: 버블 정렬
- 기본적인 정렬 알고리즘: 삽입 정렬
- 고급 정렬 알고리즘: 병합 정렬
- 고급 정렬 알고리즘: 퀵 정렬
- 고급 정렬 알고리즘: 힙 정렬
- 특수 정렬 알고리즘: 기수 정렬
- 특수 정렬 알고리즘: 계수 정렬
  
<a href="#5">:pencil2: Chapter5. 선택 알고리즘</a>
- 평균 선형 시간 선택 알고리즘
- 최악의 경우에도 선형 시간을 보장하는 선택 알고리즘

<a href="#6">:pencil2: Chapter6. 검색 트리</a>
- 레코드, 키의 정의 및 검색 트리
- 이진 검색 트리
- 이진 검색 트리: 삽입
- 이진 검색 트리: 삭제
- 레드 블랙 트리
- 레드 블랙 트리: 삽입
- 레드 블랙 트리: 삭제
- B-트리

<h2><a id="2">:pencil2: Chapter2. 알고리즘 설계와 분석의 기초</a></h2>

**:pushpin: O 표기법**

O(g(n)) = {f(n) | ∃c > 0, n0 > 0 s.t. ∀n >= n0, f(n) <= cg(n)}<br>
O(g(n)) = {f(n) | 모든 n > n0에 대하여 f(n) <= cg(n)인 양의 상수 c와 n0가 존재한다}<br>
O(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 f(n) <= cg(n)인 양의 상수 c가 존재한다}<br>
<br>
O(g(n))은 충분히 큰 n에 대하여 g(n)에 상수만 곱하면 g(n)이 누를 수 있는 모든 함수의 집합임.<br>
n^2에 3보다 크거나 같은 상수를 곱하면 3n^2을 누를 수 있으므로 3n^2은 O(n^2)에 속함.<br>
<br>
5n^2 = O(n^2)임을 보여라.<br>
c를 6, n0을 1로 잡으면 모든 n >= n0(=1)에 대하여 5n^2 <= 6n^2임. 즉 정의를 만족하는 c와 n0이 존재함.<br>

**:pushpin: Ω 표기법**
  
  Ω(g(n)) = {f(n) | ∃c > 0, n0 > 0 s.t. ∀n >= n0, cg(n) <= f(n)}<br>
  Ω(g(n)) = {f(n) | 모든 n > n0에 대하여 cg(n) <= f(n)인 양의 상수 c와 n0가 존재한다}<br>
  Ω(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 cg(n) <= f(n)인 양의 상수 c가 존재한다}<br>
  <br>
  Ω(g(n))은 충분히 큰 n에 대하여 g(n)에 상수만 곱하면 g(n)이 압도당할 수 있는 모든 함수의 집함임.<br>
  <br>
  5n^2 = Ω(n^2)임을 보여라.<br>
  c를 4로 잡고, n0 = 1로 잡으면 모든 n >= n0(=1)에 대하여 4n^2 <= 5n^2임. 즉 정의를 만족하는 c와 n0이 존재함.<br>
  
**:pushpin: Θ 표기법**
 
  Θ(g(n)) = O(g(n)) ∩ Ω(g(n))<br>
  Θ(g(n)) = {f(n) | ∃c1, c2 > 0, n0 > 0 s.t. ∀n >= n0, c1g(n) <= f(n) <= c2g(n)}<br>
  Θ(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 c1g(n) <= f(n) <= c2g(n)인 양의 상수 c1, c2가 존재한다.}<br>
  <br>
  5n^2 = Θ(n^2)임을 보여라.<br>
  5n^2 = O(n^2), 5n^2 = Ω(n^2)임을 보였기 때문에 5n^2 = Θ(n^2)이다.<br>

**:pushpin: o 표기법**
  
  함수의 증가율이 점근적 의미에서 어느 한계보다 더 작다는 것을 표현하고자 할 때 사용됨.<br>
  예를 들어 함수 5n = o(n^2)이다. 5n의 증가율은 n^2의 증가율보다 작기 때문이다.<br>
  그렇지만 함수 (1/2) * n^2은 o(n^2)에 속하지 않음. (1/2) * n^2의 증가 속도가 n^2의 증가 속도와 점근적으로 동일하기 때문임.<br>
  <br>
  o(g(n)) = {f(n) | lim f(n) / g(n) = 0 }<br>
  o(g(n)) = {f(n) | ∃n0 > 0 s.t. ∀c>0 and n>=n0, f(n) < cg(n)}<br>
  o(g(n)) = {f(n) | n이 충분히 크면 모든 c>0에 대하여 f(n) < cg(n)이다.}<br>
  o(g(n))은 충분히 큰 n에 대하여 g(n)에 아무리 작은 상수를 곱해도 g(n)이 압도하는 모든 함수의 집합임.<br>
  <br>
  5n^2 = o(n^3)임을 보여라.<br>
  lim (n^2 -5) / n^3 = 0임.<br>
  
**:pushpin: ω 표기법**
  
  함수의 증가율이 점근적 의미에서 어느 한계보다 더 크다는 것을 표현하고자 할 때 사용함.<br>
  예를 들어 5n^3 = ω(n^2)임. 5n^3의 증가 속도는 n^2의 증가 속도보다 큼.<br>
  함수 2n^2은 ω(n^2)에 속하지 않음. 2n^2의 증가 속도는 n^2의 증가 속도와 점근적으로 동일하기 때문임.<br>
  ω표기는 함수의 기울기상의 여유 있는 하한을 나타냄.<br>
  <br>
  ω(g(n)) = {f(n) | lim f(n) / g(n) = ∞}<br>
  ω(g(n)) = {f(n) | ∃n0 > 0 s.t. ∀c>0 and n>=n0, cg(n) < f(n)}<br>
  ω(g(n)) = {f(n) | n이 충분히 크면 모든 c > 0에 대하여 cg(n) < f(n)이다.}<br>
  <br>
  n^3 / 4 = ω(n^2)임을 보여라.<br>
  lim ( (n^3 /4) / n^2 ) = ∞이므로 n^3 / 4 = ω(n^2)이다.<br>

<h2><a id="3">:pencil2: Chapter3. 점화식과 알고리즘 복잡도 분석</a></h2>


점화식은 어떤 함수를 자신과 똑같은 함수를 이용해 나타내는 것임.<br>
n!의 점화식은 f(n) = n * f(n-1), 피보나치 수열의 점화식은 f(n) = f(n-1) + f(n-2)임.<br>

**:pushpin: 반복 대치**

<pre>
factorial(n)
{
  1. if (n = 1) return 1;
  2. return n * factorial(n-1);
}
</pre>

이 알고리즘으로 n!을 구하는데 걸리는 시간을 T(n)이라고 하면, T(n) = T(n-1) + c임.<br>
c는 자기호출을 제외한 나머지의 수행 시간으로 1을 수행하는 시간과 2의 곱셈을 한번 수행하는 시간임.<br>
크기가 1이면 T(1) <= c임.<br>

<pre>
T(n) = T(n-1) + c
     = T(n-2) + 2c
     ...
     = T(1) + (n-1)c
     <= cn
</pre>

T(n) <= cn이므로 T(n) = O(n)이다.<br>

**병합 정렬**<br>

입력의 크기가 n인 배열을 정렬하는 병합 정렬은 배열을 이등분한 다음, 각각을 재귀적으로 병합 정렬해 이 둘을 병합함으로써 정렬을 끝냄.<br>
입력의 크기가 n인 배열에 대한 병합 정렬에서 대소 비교의 총 횟수를 T(n)이라고 하자.<br>
n = 2^k이라고 가정해도 일반성을 잃지 않으므로 n = 2^k으로 가정하겠음.<br>
T(n) <= 2T(n/2) + n임.<br>
n은 merge(A, p, q, r)에 필요한 최대 비교 횟수 n-1과 if (p<r) then에 있는 비교 한 번을 합한 것임.<br>

<pre>
T(n) <= 2T(n/2) + n
     <= 2^2T(n/2^2) + 2n
     ...
     <= 2^kT(n/2^k) + kn
     = nT(1) + nlogn
     = n + nlogn
      = O(nlogn)
</pre>

T(n) <= 2T(n/2) + n을 T(n) <= 2T(n/2) + O(n)으로 표시하기도 함.<br>
여기서 O(n)은 집합으로서 O(n)을 의미하지 않고, O(n)에 속하는 함수 하나를 대신하는 관행적 표현임.<br>

**n = 2^k**<br>

점근적 복잡도의 계산을 용이하게 하기 위해 n = 2^k이라는 가정을 종종 사용함.<br>
어떠한 n이라도 n과 2n 사이에 2의 멱수가 하나 있음.<br>
즉, n <= 2^k <= 2n인 2^k가 하나 존재함.<br>
임의의 상수 r에 대해 T(n) = O(n^r)이라면 T(2n) = O(2^r * n^r) = O(n^r)이므로 T(n) = T(2n)임.<br>
T(n) <= T(2^k) <= T(2n)이고 T(n) = T(2n) 이므로 T(n) = T(2^k) = T(2n)임.<br>
즉, n의 오른쪽에서 처음으로 마나는 2의 멱수에 대한 함수도 항상 n에 대한 함수와 같은 점근적 복잡도를 가지므로 n = 2^k로 가정해도 점근적 분석의 결과는 같음.<br>

**:pushpin: 추정 후 증명**

추정 후 증명은 식의 모양을 보고 점근적 복잡도를 추정한 다음 그것이 옳음을 귀납적으로 증명하는 방법임.<br>

<pre>
T(n) <= 2T(n/2) + n의 점근적 복잡도는 T(n) = O(nlogn)이다. 즉 충분히 큰 n에 대하여 T(n) <= cnlogn인 양의 상수 c가 존재한다.

경계조건: T(2) <= c2log2를 만족하는 c가 존재한다.

귀납적 가정과 전개: n/2에 대해 T(n/2) <= c(n/2)log(n/2)을 만족한다고 가정하면,
T(n) <= 2T(n/2) + n
     <= 2c(n/2)log(n/2) + n
     = cnlogn - cnlog2 +n
     <= cnlogn
     
이를 만족하는 상수 c가 존재한다. 따라서 T(n) = O(nlogn)이다.
</pre>

여기서 log1 = 0이므로 T(1) <= 1log1은 불가능하기 때문에 T(2)를 경계조건으로 잡은 것임.<br>
경계조건으로 T(2)가 아닌 훨씬 큰 상수 a에 대해 T(a) <= caloga임을 보여도 상관없음.<br>
일반적으로  T(n) = f(n)임을 보이기 위해 상수 a를 경계치로 잡으면 T(a)도 상수가 되므로 f(n)이 양수인 한 T(a) <= cf(a)를 만족시키는 c가 항상 존재함.<br>
그러므로 경계조건을 만족시키는 경계치는 항상 잡을 수 있기 때문에 경계조건을 확인하지 않아도 됨.<br>

<pre>
T(n) <= 2T(n/2 + 10) + n의 점근적 복잡도는 T(n) = O(nlogn)이다. 즉, 충분히 큰 n에 대하여 T(n) <= cnlogn인 양의 상수 c가 존재한다.

증명

T(n) <= 2T(n/2 + 10) + n
     <= 2c(n/2 + 10)log(n/2 + 10) + n
     = cnlog(n/2 + 10) + 20clog(n/2 + 10) + n
     <= cnlog(3n / 4) + 20clog(3n / 4) + n
     = cnlogn + cn(log3 - log4) + 20clog(3n / 4) + n
     <= cnlogn
     
이를 만족하는 상수 c가 존재한다. 따라서 T(n) = O(nlogn)이다.
이 때, n에 대한 제약이 있다.
n/2 + 10 < n이어야 하므로 n > 20이어야 논리적인 전개가 가능하다.
식을 간단하게 하기 위해서 n/2 + 10 <= 3n /4임을 가정했는데 이는 n >= 40이어야 가능하다.
</pre>

cn(log3 - log4) + 20clog(3n / 4) + n이 음수가 되어야 하는데, c(log3 - log4) + 1을 충분히 작은 음수로 만들 수 있으면 20clog(3n/4)이 n에 대한 차수가 낮아 압도할 수 있음.<br>

**주의**<br>
<pre>
T(n) <= 2T(n/2) + n의 점근적 복잡도가 T(n) = O(n)이라고 가정하고, 충분히 큰 n에 대하여 T(n) <= cn인 양의 상수 c가 존재한다는 것을 증명하려 할 때
T(n) <= 2T(n/2) + n
     <= 2c(n/2) + n
     = cn + n
     = c'n
     = O(n)
</pre>
c' = c + 1이라는 추가적은 상수를 만들어서 c'n <= cn이라고 주장하면 안 됨.<br>
처음에 쓴 c와 나중에 쓰는 c는 같은 상수여야 함.<br>

<pre>
T(n) <= 2T(n/2) + n
     <= 2c(n/2) + n
     = cn + n
     = O(n)
</pre>
n(c+1) = O(n)인건 맞지만 T(n) <= cn + n <= cn임을 증명하지 못 했음.<br>
따라서 귀납적 증명이 완결되지 않았으므로 틀렸음.<br>

<pre>
T(n) = 2T(n/2) + 1의 점근적 복잡도는 O(n)이다.

실패하는 증명

충분히 큰 n에 대해 T(n) <= cn인 양의 상수 c가 존재한다는 것을 증명하려 한다.
T(n) = 2T(n/2)  + 1
     <= 2c(n/2) + 1
     <= cn + 1
여기서 cn + 1 < cn임을 입증할 수 없음.

성공하는 증명

T(n) <= cn임을 증명하는 대신 T(n) <= cn - 2임을 증명할 수 있어도 T(n) = O(n)임.<br>
T(n) = 2T(n/2) + 1
     <= 2(cn/2 - 2) + 1
     = cn -3
     <= cn
</pre>

추정 후 증명법이 유용하게 사용되려면 우선 추정을 의미있게 해야 함.<br>
너무 여유롭게 추정해 증명하는 것은 별로 의미가 없음.<br>

**추가: 경계조건**<br>

재귀적으로 정의된 함수나 수열은 자신의 값을 계산하기 위해 자신과 같은 함수나 수열을 더 작은 입력값으로 호출하는 경우가 많음.<br>
이러한 호출은 일반적으로 입력값이 충분히 작아지거나 특정 조건을 만족할 때까지 반복됨.<br>
이 때, 함수나 수열이 호출되는 과정에서 더 이상 호출하지 않고 종료될 때, 이를 '경계조건' 혹은 '종료조건'이라고 부름.<br>
예를 들어, 피보나치 수열은 아래와 같은 점화식으로 정의됨.<br>
<pre>
F(0) = 0
F(1) = 1
F(n) = F(n-1) + F(n-2) (n >= 2)
</pre>
이 때, F(0)와 F(1)은 경계조건으로 사용됨.<br>
F(0)와 F(1)을 미리 정의함으로써, F(n)을 계산하기 위한 종료 조건으로 사용할 수 있음.<br>

**:pushpin: 마스터 정리**

<pre>
T(n) = aT(n/b) + f(n)
</pre>

마스터 정리는 특정한 모양을 가진 재귀식에서 바로 결과를 알 수 있는 유용한 정리임.<br>
입력의 크기가 n인 문제를 풀기 위해 입력의 크기가 n/b인 문제를 a개 풀고, 나머지 f(n)의 오버헤드가 필요한 알고리즘들이 해당됨.<br>
a >= 1, b >= 1에 대해 T(n) = aT(n/b) + f(n)인 점화식에서, h(n) = n^(logb(a))라고 할 때 T(n)의 점근적 복잡도는 다음과 같음.<br>
<pre>
어떤 양의 상수 ε에 대하여 f(n) / h(n) = O(1/n^ε)이면, T(n) = θ(h(n))이다.
어떤 양의 상수 ε에 대하여 f(n) / h(n) = Ω(n^ε)이고, 어떤 상수 c(<1)와 충분히 큰 모든 n에 대해 af(n/b) <= cf(n)이면 T(n) = θ(f(n))이다. 
f(n) / h(n) = θ(1)이면 T(n) = θ(h(n)logn)이다.
</pre>

마스터 정리의 근사 버전은 아래와 같음.
<pre>
lim f(n) / h(n) = 0이면 T(n) = θ(h(n))이다.
lim f(n) / h(n) = ∞이고, 충분히 큰 모든 n에 대해 af(n/b) < f(n)이면 T(n) = θ(f(n))이다.
f(n) / h(n) = θ(1)이면 T(n) = θ(h(n)logn)이다.
</pre>

마스터 정리의 근사 버전은 원형과 정확히 같지는 않음.<br>
lim f(n) / h(n) = ∞은 h(n)이 f(n)을 압도한다는 뜻이고, f(n) / h(n) = O(1/n^ε)은 h(n)이 f(n)을 적어도 다항식의 비율로 압도한다는 뜻임.<br>
lim f(n) / h(n) = lim 1 / logn의 경우에는 lim f(n) / h(n) = 0이지만 logn의 비율로 압도할 뿐 다항식의 비율로 압도하지는 않음.<br>
즉. f(n) / h(n) = O(1/n^ε)은 성립하지 않음.<br>
lim f(n) / h(n) = ∞와 f(n) / h(n) = Ω(n^ε) 사이에도 다항식 비율에 관한 차이가 있음.<br>
충분히 큰 모든 n에 대해 af(n/b) < f(n)과 어떤 상수 c(<1)와 충분히 큰 모든 n에 대해 af(n/b) <= cf(n)도 고정 비율 c이하의 보장이라는 미묘한 차이가 있지만 반례를 찾기가 극히 힘들기 때문에 사실상 이 둘을 구분 없이 사용해도 무방함.<br>

<pre>
T(n) = 2T(n/3) + c (c는 상수)

a = 2, b =  3, f(n) = c, h(n) = n^log3(2)
lim f(n) / h(n) = 0이므로 T(n) = θ(n^log3(2))이다.
</pre>

<pre>
T(n) = 2T(n/4) + n

a = 2, b = 4, f(n) = n, h(n) = n^log4(2)
lim f(n) / h(n) = ∞이고 af(n/b) = 2(n/4) = n/2 < n임.
또한 n/2 <= (1/2)f(n)이므로 c = 1/2에 대해 af(n/b) <= cf(n)을 만족함.
따라서 T(n) = θ(nlogn)임.
</pre>

<pre>
T(n) = 2T(n/2) + n

a = 2, b = 2, f(n) = n, h(n) = n
f(n) / h(n) = 1이므로 T(n) = θ(nlogn)임.
</pre>

마스터 정리를 쓸 수 없는 모양인 점화식의 변수를 치환함으로써 마스터 정리를 쓸 수 있는 모양으로 변형하는 방법도 있음.<br>

<pre>
T(n) = 2T(n^(1/2)) + logn

m = log2(n)으로 놓으면

T(2^m) = 2T(2^(m/2)) + m

이는 아래와 같이 다시 표현할 수 있음.

S(m) = 2S(m/2) + m

a = 2, b = 2, f(m) = m, h(m) = m이므로
S(m) = θ(mlogm) = θ(logn loglogn)임.
</pre>

마스터 정리에서 f(n)은 크기가 n인 문제(최상위 레벨)에서 발생하는 자기 호출 이외의 오버헤드로 크기가 다른 문제들 간의 관계를 반영하는 비용임.<br>
h(n)은 반복적인 자기호출 끝에 마지막으로 크기 1인 문제를 만나는 횟수임.<br>
자기호출 때문에 부담이 더 커지면 수행 시간은 h(n)이 결정하고, 관계를 반영하는 오버헤드가 더 커지면 f(n)이 결정함.<br>
단 f(n)은 상위 레벨의 오버헤드만 의미하므로 하위 레벨에서 발생하는 오버헤드는 반영하고 있지 않음.<br>
af(n/b) <= cf(n)은 자기호출로 만나는 하위레벨의 문제들에서 발생하는 자기호출 이외의 오버헤드들의 총합이 레벨이 내려가면서 적어도 감소해야 한다는 것을 의미함.<br>

<h2><a id="4">:pencil2: Chapter4. 정렬</a></h2>

**:pushpin: 기본적인 정렬 알고리즘: 선택 정렬**

우선 배열 A[1, ..., n]에서 가장 큰 원소를 찾아 이 원소와 배열의 끝자리에 있는 A[n]과 자리를 바꿈.<br>
그러면 방금 맨 뒷자리로 옮긴 원소, 즉 가장 큰 원소는 자기 자리를 찾았으므로 더 이상 신경 쓰지 않아도 됨.<br>
이 원소는 정렬이 끝났다고 볼 수 있으므로 이제 이 원소를 제외한 나머지 원소들로 같은 작업을 반복함.<br>

<pre>
간략한 기술

selectionSort(A[], n)
{
  for last <- n downto 2 {
    A[1, ..., last]중 가장 큰 수 A[k]를 찾는다;
    A[k] <-> A[last];
  }
}
</pre>

<pre>
기호적 기술
selectionSort(A[], n)
{
  for last <- n downto 2 {
    k <- theLargest(A, last);
    A[k] <-> A[last];
  }
}

theLargest(A[], last)
{
  largeset <- 1;
  for i <- 2 to last
    if (A[i] > A[largest]) then largest <- i;
  return largest;
}
</pre>

이 알고리즘에서 입력은 배열 A[1, ..., n]임.<br>
변수 last는 정렬할 배열의 맨 마지막 인덱스, 즉 배열의 크기를 나타냄.<br>
처음에는 배열의 크기가 n으로 시작하므로, A[1, ..., n]을 정렬 대상으로 삼음.<br>
가장 큰 수를 찾아 제자리에 놓을 때마다 last는 1씩 줄어들음.<br>
선택 정렬의 수행 시간은 모든 경우에 Θ(n^2)임.<br>
배열의 크기가 n일 때 n-1번 비교, ... , 배열의 크기가 2일 때 1번 비교함.<br>
따라서 1 + ... + n-1 = n(n-1) / 2임.<br>
수를 비교하는 횟수가 전체 시간을 좌우하므로 이것을 기준으로 수행 시간을 계산함.<br>

**:pushpin: 기본적인 정렬 알고리즘: 버블 정렬**

<pre>
간략한 기술

bubbleSort(A[], n)
{
  for last <- n downto 2
    for i <- 1 to last-1
      if (A[i] > A[i+1]) then A[i] <-> A[i+1];
}
</pre>

버블 정렬의 총 순환 횟수는 (n-1) + (n-2) + ... + 2 + 1 = n(n-1) / 2임.<br>
따라서 수행 시간은 Θ(n^2)임.<br>
버블 정렬 알고리즘은 중간에 배열이 이미 정렬이 되어 있는 상태라도 계속 끝까지 무의미한 순환을 계속함.

<pre>
수정된 bubble sort

bubbleSort(A[], n)
{
  for last <- n downto 2
      sorted <- TRUE;
    for i <- 1 to last-1 {
      if (A[i] > A[i+1]) {
        A[i] <-> A[i+1];
        sorted <- FALSE;
      }
    }
    if (sorted = TRUE) then return;
}
</pre>

**:pushpin: 기본적인 정렬 알고리즘: 삽입 정렬**

삽입 정렬은 이미 정렬되어 있는 i개짜리 배열에 하나의 원소를 더 더하여 정렬된 i+1개짜리 배열을 만드는 과정을 반복함.<br>
한 개짜리 배열에서 시작하여 그 크기를 하나씩 늘리는 정렬임.<br>

<pre>
insertionSort(A[], n)
{
  for i <- 2 to n
    A[1, ..., i]의 적합한 자리에 A[i]를 삽입한다.
}
</pre>

for 루프는 문제의 크기를 하나씩 키워나가는 역할을 함.<br>
A[i]에 관심을 두는 시점에는 A[1, ..., i-1]은 항상 정렬이 되어 있음.<br>
A[i]가 A[i-1]보다 크면 앞에 있는 모든 원소보다 크므로 A[i]는 그냥 제자리에 두면 됨.<br>
그렇지 않으면 A[i-1]부터 시작해서 왼쪽으로 차례로 훑으면서 A[i]가 들어갈 자리를 찾음.<br>
A[i]가 들어가는 자리부터 시작해서 이후의 원소들은 한 칸씩 오른쪽으로 밀려남.<br>

<pre>
insertionSort(A[], n)
{
  for i <- 2 to n {
    loc <- i - 1;
    newItem <- A[i]
    
    while (loc >= 1 and newItem < A[loc]) {
      A[loc + 1] = A[loc];
      loc--;
    }
    A[loc + 1] <- newItem;
  }
}
</pre>

for 루프는 n-1번 순환함.<br>
매 for 루프에서 while은 최대 i-1번 순환함.<br>
가장 운이 좋으면 while 문은 돌아가지 않음.<br>
최악의 경우 수행 시간은 (n-1) + (n-2) + ... + 2 + 1 = n(n-1)/2임.<br>
따라서 Θ(n^2)임.<br>
보통은 대략 A[1, ..., i-1]에서 평균적으로 절반 정도를 훑고 끝낼 것임.<br>
그러므로 전체 비교 횟수는 최악의 경우에 비해 절반 정도 될 것임.<br>
그래도 시간복잡도는 Θ(n^2)임.<br>
<br>
삽입 정렬은 거의 정렬되어 있는 상태로 입력되는 경우에는 가장 매력적인 알고리즘임.<br>
배열이 완전히 정렬된 채로 입력되면 while 루프는 한 번도 수행되지 않고, for 루프는 한번 순환할 때마다 상수 시간이 소요됨.<br>
for 루프는 n-1번 순환되므로 Θ(n)에 가까운 시간이 듬.<br>
배열이 거의 정렬되어 있을 때도 삽입이 매우 수월해져 Θ(n)에 가까운 시간이 듬.<br>
버블 정렬은 배열이 이미 정렬되어 있는 경우에 무의미한 순환을 줄이기 위해 방법이 있긴 했지만 오버헤드가 생김.<br>
삽입 정렬은 별도 장치가 없어도 효율적으로 끝남.<br>
따라서 상황에 따라 가끔 삽입 정렬을 섞어서 씀.<br>
<br>
선택 정렬과 버블 정렬이 n개짜리 배열에서 시작하여 아직 정렬되지 않은 배열의 크기를 하나씩 줄이는 데 반하여, 삽입 정렬은 1개짜리 배열에서 시작하여 이미 정렬된 배열의 크기를 하나씩 늘리는 정렬임.<br>
삽입 정렬에는 수학적 귀납법의 원리가 들어가 있음.<br>
배열의 크기가 1일 때는 성립함.(이미 정렬되어 있으므로)<br>
배열의 크기가 k일 때 성립하면(정렬되어 있으면), 적절한 삽입으로 크기가 k+1일 때도 성립함(정렬됨).<br>
이것으로 삽입 정렬은 올바르게 정렬을 한다는 것이 귀납적으로 증명됨.<br>

**:pushpin: 고급 정렬 알고리즘: 병합 정렬**

병합 정렬은 먼저 입력을 반으로 나눔.<br>
이렇게 나눈 전반부와 후반부를 각각 독립적으로 정렬함.<br>
마지막으로 정렬된 두부분을 합쳐서, 즉 병합하여 정렬된 배열을 얻음.<br>
여기서 전반부, 후반부를 각각 정렬할 때도 역시 반으로 나눈 다음 정렬해서 병합함.<br>
즉, 원래의 정렬 문제와 성격이 똑같고 단지 크기만 반으로 줄였을 뿐임.<br>
병합 정렬은 자신에 비해 크기가 반인 문제를 두 개 푼 다음, 이들을 병합하는 일을 재귀적으로 반복함.<br>

<pre>
mergeSort(A[], p, r)
{
  if (p < r) then {
    q <- (p+r) / 2;
    mergeSort(A, p, q);
    mergeSort(A, q+1, r);
    merge(A, p, q, r);
  }
}

merge(A[], p, q, r)
{
  정렬되어 있는 두 배열 A[p, ..., q]와 A[q+1, ..., r]을 합쳐 정렬된 하나의 배열 A[p,...,r]을 만듬.
}
</pre>

<pre>
merge(A[], p, q, r)
{
  i <- p; j <- q + 1; t <- 1;
  while (i <= q and j <= r) {
    if (A[i] <= A[j])
    then tmp[t++] = A[i++];
    else tmp[t++] = A[j++];
  }
  
  while (i <= q)       //왼쪽 배열이 남은 경우
    tmp[t++] = A[i++];
  while (j <= r)       //오른쪽 배열이 남은 경우
    tmp[t++] = A[j++];
  i <- p; t <- 1;
  while (i <= r)
    A[i++] <- tmp[t++];
}
</pre>

T(n) <= a (if n = 1)<br>
T(n) <= 2T(n/2) + cn (if n > 1)<br>
부등호를 쓰는 이유는 좌변과 우변이 정확히 일치하지 않을 수 있기 때문임.<br>
따라서 근사적으로 나타내는 표현의 하나라고 생각하면 됨.<br>
상수 a는 크기가 1인 문제를 푸는 시간을 나타냄.<br>
상수 c는 병합에 드는 시간을 충분히 잡아주기 위해서 n에 곱한 것임.<br>
비교의 횟수만으로 수행 시간을 분석한다면 c=1로 충분함.<br>
어쨋든 병합은 선형 시간이 소요됨.<br>
n=2^k라고 가정하고 전개하면 다음과 같음.<br>

<pre>
T(n) <= 2T(n/2) + cn
     <= 2^2T(n/2^2) + 2cn
     ...
     <= 2^kT(n/2^k) + kcn
     = an + cn * logn
     = Θ(nlogn)
</pre>

병합 정렬의 수행 시간은 최악의 경우 Θ(nlogn)임.

**:pushpin: 고급 정렬 알고리즘: 퀵 정렬**

퀵 정렬은 평균적으로 가장 좋은 성능을 가져 현장에서 가장 많이 쓰는 정렬 알고리즘임.<br>
우선 정렬할 배열에서 기준원소를 하나 고름.<br>
아무 원소나 임의로 골라도 되나 여기서는 맨 뒤의 원소를 기준원소로 삼음.<br>
이 기준원소를 중심으로 더 작거나 같은 수는 왼쪽으로, 큰 수는 오른쪽으로 재배치함.<br>
기준원소는 이렇게 분할된 양쪽 부분 배열 사이에 자리하게 됨.<br>
이렇게 분할된 왼쪽 부분 배열을 따로 정렬함.<br>
마찬가지로 오른쪽 부분 배열도 따로 정렬함.<br>
기준원소는 손대지 말고 제자리에 그대로 둠.<br>
왼쪽과 오른쪽 부분 배열을 정렬할 때 퀵 정렬을 재귀적으로 사용함.<br>

<pre>
quickSort(A[], p, r)
{
  if (p < r) then {
    q <- partition(A, p, r);
    quickSort(A, p, q-1);
    quickSort(A, q+1, r);
  }
}

partition(A[], p, r)
{
  배열 A[p, ..., r]의 원소들을 A[r]을 기준으로 양쪽으로 재배치하고 A[r]이 자리한 위치를 리턴함.
}
</pre>

병합 정렬은 먼저 재귀적으로 작은 문제를 해결한 다음 후 처리를 하는데 반해서, 퀵 정렬은 선행 작업을 한 다음 재귀적으로 작은 문제를 해결하면서 바로 끝냄.<br>

<pre>
partition(A[], p, r)
{
  x <- A[r];
  i <- p -1;
  for j <- p to r-1
    if (A[j] <= x) then A[++i] <-> A[j];
  A[i+1] <-> A[r]
  return i+1;
}
</pre>

퀵 정렬의 수행 시간을 분석해보자.<br>
우선 분할은 배열을 왼쪽부터 끝까지 한 번 훑어나가는 작업이므로 Θ(n)의 시간이 듬.<br>
퀵 정렬의 수행에서 가장 이상적인 경우는 분할이 항상 반반씩 균등하게 될 때임.<br>
이 때는 T(n) = 2T(n/2) + Θ(n)이므로 병합 정렬과 같은 모양임.<br>
따라서 Θ(nlogn)이 됨.<br>
최악의 경우는 계속해서 한쪽은 하나도 없고, 다른 쪽에 다 몰리도록 분할이 되는 경우임.<br>
T(n) = T(n-1) + Θ(n)이므로 Θ(n^2)이 됨.<br>
한 쪽이 완전히 비거나 이에 근접한 상태가 반복되면 이런 비효율적인 시간이 나옴.<br>
퀵 정렬의 수행 시간은 분할이 얼마나 균형잡히게 잘 되느냐에 달려 있음.<br>
평균 수행 시간은 분할했을 때 모든 가능한 경우를 평균내면 됨.<br>
기준 원소가 1등이면 1구역과 2구역의 크기가 0:n-1, 2등이면 1:n-2, ..., i등이면 i-1:n-i임.<br>
T(n) = T(i-1) + T(n-i) + Θ(n)임.<br>
기준원소는 동일한 확률로 1등부터 n등 중의 하나가 되므로 이들을 평균하면 다음과 같음.<br>
T(n) = (1/n) * Σ(i=1~n)[T(i-1) + T(n-i)] + Θ(n) = (2/n) * Σ(k=0~n-1)T(k) + Θ(n)임.<br>
이를 계산하면 T(n) = Θ(nlogn)임.<br>

<pre>
위 내용을 증명하겠음.

우선 T(2) <= c2log2를 만족하도록 충분히 큰 c를 잡을 수 있음.
2 <= k <= n인 모든 k에 대해 T(k) <= cklogk가 성립한다 가정하고 T(n) <= cnlogn이 됨을 증명하면 됨.

T(n) = (1/n) * Σ(i=1~n)[T(i-1) + T(n-i)] + Θ(n)
     = (2/n) * Σ(k=0~n-1)T(k) + Θ(n)
     = (2/n) * Σ(k=2~n-1)T(K) + Θ(n)  // k=0, k=1일 때는 Θ(n)에 흡수됨.
     <= (2/n) * Σ(k=2~n-1)cklogk + Θ(n)
     <= (2c/n) * [(1/2) * n^2 * logn - (1/8) * n^2] + Θ(n) // 이 부분 아래에 증명하겠음.
     = cnlogn - cn/4 + Θ(n)
     <= cnlogn
     
Σ(k=1~n-1)klogk = Σ(k=1~(n/2-1))klogk + Σ(k=(n/2)~n-1)klogk
왼쪽 항의 logk는 log(n/2)을 상한으로 잡을 수 있고, 오른쪽 항의 logk는 logn을 상한으로 잡을 수 있음.
Σ(k=1~n-1)klogk <= log(n/2)Σ(k=1~(n/2-1))k + lognΣ(k=(n/2)~n-1)k 
                 = lognΣ(k=1~(n/2-1))k + lognΣ(k=(n/2)~n-1)k - Σ(k=1~(n/2-1))k 
                 = lognΣ(k=1~(n-1))k - Σ(k=1~(n/2-1))k
                 <= logn * n * (n-1) / 2 - (1/2) * (n/2) * (n/2 - 1)
                 <= (1/2) * n^2 * logn - (1/8) * n^2

Σ(k=2~n-1)klogk <= Σ(k=1~n-1)klogk이므로 Σ(k=1~n-1)klogk <= 1/2) * n^2 * logn - (1/8) * n^2임.
</pre>

<pre>
퀵 정렬이 제대로 정렬한다는 것을 귀납적으로 증명

n = 1
원소가 하나이므로 이미 정렬됨

n < k
quicksort가 제대로 정렬한다고 가정

n = k
partition에 의해서 세 부분으로 나뉨
왼쪽은 k보다 작고, 오른쪽도 k보다 작으므로 귀납적 가정에 의해서 제대로 정렬됨.
따라서 n=k일 때도 정렬됨.
</pre>

**:pushpin: 고급 정렬 알고리즘: 힙 정렬**

힙은 이진 트리로서 맨 아래 층을 제외하고는 완전히 채워져 있고, 맨 아래층은 왼쪽부터 꽉 채워져 있음.<br>
힙의 모든 노드는 하나씩의 값을 갖고 있는데, 다음 힙 성질을 만족함.<br>
각 노드의 값은 자기 자식의 값보다 작다(최소힙, 힙에 값이 같은 원소가 두 개 이상 있는 경우에는 작다 대신 작거나 같다)<br>
리프 노드는 자식이 없으므로 논리상 이 성질은 자동 만족됨.<br>
모든 노드가 이 성질을 만족하면, 이진 트리의 루트 노드에는 최솟값이 자리하게 됨.<br>
반대로 최대힙의 루트 노드에는 최댓값이 자리하게 됨.<br>
힙 정렬은 먼저 주어진 배열을 힙으로 만듬.<br>
그런 다음 힙에서 가장 작은 값을 차례로 하나씩 제거하면서 힙의 크기를 줄여나감.<br>
나중에 힙에 아무 원소도 남지 않으면 힙 정렬이 끝남.<br>
정렬은 힙에서 원소들이 제거된 순서대로 함.<br>

**힙 만들기**<br>

일반적으로 A[k]의 자식은 A[2k]와 A[2k+1]이 됨.<br>
A[k]의 부모는 A[k/2]이 됨.<br>
이렇게 부모자식 관계를 배열의 인덱스를 사용해 간단히 계산할 수 있으므로 링크나 포인터가 필요없음.<br>
n개의 원소를 가진 배열 A[1, ..., n]이 주어졌다고 하자.<br>
heapify(A, k, n)은 A[k]에 매달린 두 서브 트리가 힙성질을 만족하는 상태에서 A[k]를 루트로 하는 서브 트리 전체가 힙성질을 만족하도록 수선하는 함수임.<br>
루트의 두 자식 중 작은 값을 x, 큰 값을 y라고 하자.<br>
x를 루트와 비교한다.<br>
루트의 값이 x보다 크지 않으면 힙성질이 만족되어 수선은 끝남.<br>
루트의 값이 x보다 크면 x와 루트의 값을 맞바꿈.<br>
이제 x는 새로운 루트가 되었고, 루트의 값은 한 단계 내려옴.<br>
이렇게 루트가 한 칸 아래로 내려온 노드를 r이라 하자.<br>
여기서 다시 r을 새로운 루트로 삼아 이 작업을 재귀적으로 반복함.<br>
이런 식으로 내려갈 수 있는 곳까지 내려감.<br>
중간에 루트가 자식 중 작은 값보다 크지 않은 경우를 만나면 중단함.<br>

<pre>
buildHeap(A[], n)
{
  for i <- n/2 downto 1
    heapify(A, i, n);
}

heapify(A[], k, n)
{
  left <- 2k; right <- 2k+1;
  
  if (right <= n) then {    // k가 두 자식을 가지는 경우
    if (A[left] < A[right]) 
      then smaller <- left;
    else smaller <- right;            
  }
  
  else if (left <= n) then smaller <- left;
  else return;
  
  if (A[smaller] < A[k]) then {
    A[k] <-> A[smaller];
    heapify(A, smaller, n);
  }
}
</pre>

리프 노드는 그 자체로 힙성질을 만족하므로 buildHeap()은 리프가 아닌 노드 중 맨 뒤에서부터 루트로 삼아 heapify()를 수행함.<br>
n/2은 리프가 아닌 노드 중 맨 마지막 노드의 인덱스임.<br>

buildHeap()에 소요되는 시간은 Θ(n)임.<br>
heapify()는 해당 서브 트리의 높이가 시간을 좌우함.<br>
어떤 서브 트리도 높이가 log2(n)을 넘지 않으므로 heapify를 한번 수행하는데 O(logn)이 소요됨.<br>
그런데 buildHeap()에서 heapify()를 호출하는 횟수는 ⌊n/2⌋이므로 전체적으로 O(nlogn)이 됨.<br>
그러나 이는 과하게 잡은 상한임.<br>
모든 heapify()의 시간을 O(logn)으로 잡은 것이 과함.<br>
맨 처음 호출되는 heapify()의 입력 트리는 높이가 고작 1이고, 이런 것들이 꽤 여러 개 있음.<br>
그 다음 레벨로 올라가면 높이는 증가하지만, 높이가 높은 부분 트리의 수는 줄어들음.<br>
따라서 이를 합산하면 O(nlogn)이 아닌 Θ(n)이 됨.<br>
이에 대한 증명은 아래에 했음.<br>

<pre>
buildHeap()의 수행 시간 계산

heapify()는 힙의 높이에 비례하는 시간이 소요됨.
즉, 힙의 높이가 h라면 O(h) 시간이 소요됨.
원소의 수가 총 n개인 힙의 높이는 ⌊log2(n)⌋임.
높이가 h인 트리의 노드 수는 기껏해야 ⌈n/2^(h+1)⌉임.
따라서 buildHeap()의 수행 시간은 다음과 같음.

Σ(h=0~⌊log2(n)⌋)⌈n/2^(h+1)⌉O(h) = O(nΣ(h=0~⌊log2(n)⌋)h/2^h)=O(n)임.

O(logn)이 될 것이라는 직관과는 달리 O(n)이 됨.
Σ(h=0~⌊log2(n)⌋)h/2^h이 2보다 크지 않기 때문임.

Σ(h=0~∞)x^h = 1/(1-x)
이를 양변에 미분하면 다음과 같음.
Σ(h=0~∞)h*x^(h-1) = 1/(1-x)^2
양변에 x를 곱하면 다음과 같음
Σ(h=0~∞)h*x^h = x/(1-x)^2
h/2^h는 h*x^h에서 x가 1/2인 경우와 같음.
Σ(h=0~⌊log2(n)⌋)h/2^h <= Σ(h=0~∞))h/2^h
                      = (1/2) / (1/2)^2
                      = 2
따라서 2가 Σ(h=0~⌊log2(n)⌋)h/2^h의 상한임.
</pre>

힙이 완성되었으면 정렬 작업을 함.<br>
루트 노트에 있는 원소를 제거하여 다른 곳에 저장함.<br>
루트 노드가 없어졌으므로 트리의 크기가 하나 줄음.<br>
맨 끝에 있는 원소를 루트 노드로 옮겨 새로운 루트로 삼음.<br>
루트 노드로 옮긴 원소가 있던 자리에 방금 제거한 루트 노드 원소를 저장함.<br>
이것으로 대부분의 경우 루트 노드와 자식 간에 힙성질이 깨짐.<br>
heapify()를 이용해 힙성질을 만족하도록 수선함.<br>

<pre>
heapSort(A, n)
{
  buildHeap(A, n);
  for i <- n downto 2 {
    A[1] <-> A[i];
    heapify(A, 1, i-1);
  }
}
</pre>

buildHeap()은 Θ(n)의 시간이 듬.<br>
for 루프는 n-1번 순환하고 각 순환에서 시간을 좌우하는 heapify()는 충분히 잡아서 O(logn)의 시간이면 됨.<br>
그러므로 힙 정렬의 총 수행 시간은 O(nlogn)임.<br>

**비교 정렬 시간의 하한**<br>

원소끼리 비교하는 것으로만 정렬을 하는 것을 비교 정렬이라고 함.<br>
비교 정렬은 최악의 경우 수행 시간이 절대 Ω(nlogn)을 밑돌 수 없음.<br>
이것을 결정 트리 모델을 사용해서 증명할 수 있음.<br>
<br>
편의상 정렬하고자 하는 모든 원소가 다르다고 가정함.<br>
이것으로 일반성을 잃지는 않음.<br>
임의의 비교 정렬은 결정 트리에서 탐색으로 볼 수 있음.<br>
삽입 정렬을 결정 트리 모델로 그려봄.<br>

<pre>
if (a1 < a2)
  if (a2 < a3) return a1 < a2 < a3
  else
    if (a1 < a3) return a1 < a3 < a2
    else return a3 < a1 < a2
else
  if (a1 < a3) return a2 < a1 < a3
  else
    if (a2 < a3)  return a2 < a3 < a1
    else return a3 < a2 < a1
</pre>

리프노드를 만날 때까지 비교를 계속 함.<br>
결정 트리의 루트에서 시작해 리프에 이르면 정렬은 끝남.<br>
정렬 알고리즘은 입력 수열의 모든 가능한 경우에 대해 다 제대로 정렬을 해줘야 하므로 결정 트리의 리프 노드는 n!개가 되어야 함.<br>
n!에 관해서는 스털링의 근사식을 얻을 수 있음.<br>

<pre>
n! = √(2πn)(n/e)^n * (1 + Θ(1/n))

logn! = log( √(2πn)(n/e)^n * (1 + Θ(1/n)) )
      = log√(2πn)(n/e)^n + nlogn - nloge
      = Θ(nlogn)
</pre>

정렬의 수행에서 최악의 경우는 결정 트리에서 가장 깊은 리프 노드까지 내려가는 것임.<br>
n!개의 리프 노드를 가진 트리의 높이는 적어도 ⌈log2(n!)⌉임.<br>
따라서 최악의 깊이는 적어도 ⌈log2(n!)⌉은 되어야 하므로 최악의 경우 수행 시간은 이 전개식의 결론을 이용하면 적어도 Θ(nlogn)임.<br>
즉 Ω(nlogn)임.<br>

**:pushpin: 특수 정렬 알고리즘: 기수 정렬**

지금까지 배운 정렬 알고리즘들은 모두 원소 두 개를 비교함으로써 정렬을 하는 비교 정렬이었음.<br>
즉, 원소의 상대적인 대소 관계만 판단할 뿐이지 원소의 분포나 자릿수 등은 고려하지 않았음.<br>
입력 원소들이 특수한 성질을 만족하는 경우에는 Θ(nlogn)이란 한계를 극복할 수 있음<br>
<br>
기수 정렬은 입력이 모두 k 자릿수 이하의 자연수인 특수한 경우에 사용할 수 있는 방법으로 Θ(n) 시간이 소요되는 정렬 알고리즘임.<br>
우선 가장 낮은 자릿수만 가지고 모든 수를 재배열함.<br>
그런 다음 가장 낮은 자릿수는 잊어버림.<br>
그리고 앞과 같은 방법으로 더 이상 자릿수가 남지 않을 때까지 계속함.<br>
이렇게 하면 마지막에는 정렬된 배열을 갖게 됨.<br>

<pre>
radixSort(A[], n, k)
{
  for <- i to k
    i번째 자릿수에 대해 A[1, ..., n]을 안정성을 유지하면서 정렬함.
}
</pre>

"안정성을 유지하면서 정렬한다"는 것은 값이 같은 원소끼리는 정렬 후에 원래의 순서가 바뀌지 않는 성질을 뜻함.<br>
2150과 2154는 4번째 자릿수가 2로 똑같음.<br>
2150이 2154보다 앞에 있으면 4번째 자리에 대해 정렬했을 때 이 순서는 유지되어야 함.<br>
안정성을 유지하기 위해 다른 정렬 알고리즘을 쓰면 이미 Θ(n)을 초과해버리므로 다른 방법을 사용해야 함.<br>
예를 들어, 0부터 9까지 표시된 10개의 공간을 준비해놓고 각각의 수를 가진 입력은 해당 공간에 차례대로 넣어주는 등 이 부분을 O(n)에 끝내야 함.<br>
알고리즘은 이런 일을 k번 반복하는데 k가 상수이므로 전체 시간은 여전히 O(n)임.<br>

**:pushpin: 특수 정렬 알고리즘: 계수 정렬**

계수 정렬은 정렬하고자 하는 원소들의 값이 O(n)을 넘지 않는 경우에 사용할 수 있음.<br>
예를 들어, 배열 A[1, ..., n]의 원소들이 k를 넘지 않는 자연수인 경우를 들 수 있음.<br>
계수 정렬은 먼저 배열의 원소를 훑어보고 1부터 k까지의 자연수가 각각 몇 번 나타나는지를 셈.<br>
이 정보가 있으면 A[1, ..., n]의 각 원소가 몇 번째에 놓이면 되는지를 계산해낼 수 있음.<br>

<pre>
countingSort(A[], B[], n)
{
  for i <- 1 to k
    C[i] <- 0
  for j <- 1 to n
    C{A[j]]++;
  for i <- 2 to k
    C[i] <- C[i] + C[i-1]
  for j <- n downto 1 {
    B[C[A[j]]] <- A[j];
    C[A[j]]--;
  }
}
</pre>

알고리즘에서 A[1, ..., n]을 정렬한 결과가 배열 B[1, ..., n]에 저장됨.<br>
계수 정렬의 수행 시간은 Θ(n)임.<br>
첫 번째 for 루프는 Θ(k), 두 번째 for 루프는 Θ(n), 세 번째 for 루프는 Θ(k), 마지막 for 루프는 Θ(n)의 시간이 소요됨.<br>
k가 O(n)을 초과하면 시간은 Θ(k)가 됨.<br>
k가 O(nlogn)을 초과하면 계수 정렬은 병합 정렬, 퀵 정렬, 힙 정렬보다 매력이 없어짐.<br>
그래서 일반적으로 계수 정렬은 k가 O(n)을 초과하지 않는 경우에 선형 시간에 정렬하기 위해 사용함.<br>
정렬할 원소가 꼭 양수일 필요도 없음.<br>
원소들이 모두 -k와 k 사이의 정수이고 k가 O(n)일 경우에도 여전히 계수 정렬을 사용하여 선형 시간에 정렬할 수 있음.<br>

<h2><a id="5">:pencil2: Chapter5. 선택 알고리즘</a></h2>
  
**:pushpin: 평균 선형 시간 선택 알고리즘**

n개의 원소가 규칙 없이 저장된 배열에서 i번째 작은 원소를 찾으려 함.<br>
먼저 퀵 정렬에서 사용한 분할 알고리즘을 상기해봄.<br>
분할 알고리즘은 기준원소보다 작거나 같은 원소는 기준원소의 왼쪽 그룹으로, 기준원소보다 큰 원소는 기준원소의 오른쪽 그룹으로 재배치함.<br>
분할 알고리즘이 리턴하는 값으로 기준 원소가 전체에서 몇 번째 작은 원소인지 알 수 있음.<br>
이것으로 기준원소가 전체에서 k번째 작은 원소란 사실을 알았다고 하자.<br>
이제 i와 k의 값을 비교함.<br>
i가 k보다 작으면, i번째 작은 수는 왼쪽 그룹에 있는 원소 중 하나임.<br>
i가 k와 같으면, 기준원소가 바로 i번째 작은 수임.<br>
i가 k보다 크면, i번째 작은 수는 오른쪽 그룹에 있는 원소 중 하나임.<br>

<pre>
select(A, p, r, i)
{
  if (p=r) then return A[p];
  q <- partition(A, p, r);
  k <- q - p + 1;
  if (i < k) then return select(A, p, q-1, i);
  else if (i = k) then return A[q];
  else return select(A, q+1, r, i-k);
}
</pre>

기준 원소가 전체 집합에서 k번째 작은 원소이면 두 그룹은 각각 k-1개와 n-k개 나뉘고 알고리즘의 수행 시간은 다음과 같음.<br>
T(n) <= max[T(k-1), T(n-k)] + Θ(n)<br>
입력 배열은 가능한 모든 경우가 고루 일어난다고 가정하면, 전체 배열에서 기준원소의 순위 k는 1부터 n까지 동일한 확률을 갖음.<br>
이들의 평균을 위의 관계식에 반영하면 다음과 같음.<br>

<pre>
T(n) 
<= max[T(k-1), T(n-k)] + Θ(n)
<= (1/n)*∑(k=1~n)max[T(k-1), T(n-k)] + Θ(n)<br>
<= (2/n)*∑(⌊n/2⌋~(n-1))T(k) + Θ(n)<br>
⌊n/2⌋<=k<n인 모든 k에 대해 T(k) <= ck라 가정하면<br>
<= (2/n)*∑(⌊n/2⌋~(n-1))ck + Θ(n)<br>
=  (2/n) * (∑(1~(n-1))ck - ∑(⌊n/2⌋~(n-1))ck + Θ(n)<br>
=  (n/2) * [c * (n-1) * n / 2 - c * (⌊n/2⌋ -1) * ⌊n/2⌋ / 2] + Θ(n)
<= 2c / n * ((n-1) * n / 2 - (n / 2 -2) * (n / 2 - 1)) + Θ(n)
=  c(n-1) - (c/n) * (n^2/4 - 3*n/2 + 2) + Θ(n)
=  cn + (-cn /  4  + c / 2 - 2c / n + Θ(n))
<= cn
상수 c를 충분히 크게 잡으면 -cn/4이 Θ(n)을 압도해서 -cn /  4  + c / 2 - 2c / n + Θ(n)이 음수가 되도록 할 수 있음.
</pre>

그러므로 T(n)=O(n)임. T(n)=Ω(n)임은 명백하므로 T(n)=Θ(n)임.<br>
이 알고리즘의 경우 평균적인 경우 Θ(n)의 시간이 소요되지만, 최악의 경우에는 Θ(n^2)의 시간이 소요됨.<br>
최악의 예는 분할 결과 0:n-1로 계속 분할이 되고 찾고자 하는 원소가 운 나쁘게도 큰 그룹에 속하는 일이 반복되는 경우임.<br>
이 때 수행 시간의 점화식은 다음과 같음.<br>
T(n) = T(n-1) + Θ(n)<br>
이것을 전개하면 T(n)=Θ(n^2)이 됨.<br>
항상 이렇게 되지 않아도 이에 준할 정도로 자주 분할의 균형이 깨지면 역시 T(n)=Θ(n^2)이 됨.

**:pushpin: 최악의 경우에도 선형 시간을 보장하는 선택 알고리즘**
  
계속 1:9로 분할이 되고 이 중 나쁜 경우로 큰 그룹(9에 해당하는 부분)에서 탐색을 하게 된다고 하자.<br>
이 경우에는 다음 점화식으로 표현할 수 있음.<br>
T(n) = T(9n/10) + Θ(n)<br>
입력의 크기가 n인 문제를 풀기 위해 Θ(n)의 오버헤드를 사용한 다음 입력의 크기가 9n/10인 문제를 재귀적으로 호출한다.<br>
이것은 계산하면 T(n) = Θ(n)이 됨.<br>
1:99로 분할되어도 여전히 점근적 시간은 T(n) = Θ(n)이 됨.<br>
분할의 균형이 아주 나빠보여도 일정한 상수비만 넘지 않으면 점근적 복잡도는 항상 Θ(n)이 됨.<br>
이 절의 알고리즘은 분할의 균형을 어느 정도까지 보장함으로써 최악의 경우 Θ(n)을 보장함.<br>
그렇지만 분할의 균형만 적당히 맞춘다고 Θ(n)이 무조건 보장되는 것은 아님.<br>
균형을 맞추는 오버헤드가 너무 커져버리면 목표를 이룰 수 없음.<br>

<pre>
linearSelect(A, p, r, i)
{
  1. 원소의 총 수가 5개 이하인 i번째 원소를 찾고 알고리즘을 끝낸다.
  2. 전체 원소를 5개씩의 원소를 가진 ⌈n/5⌉개의 그룹으로 나눈다.(원소의 총수가 5의 배수가 아니면 이 중 한 그룹은 5개 미만이 된다.)
  3. 각 그룹에서 중앙값(원소가 5개이면 3번째 원소)를 찾는다. 이렇게 찾은 중앙값들을 m1, m2, ..., ,m⌈n/5⌉이라 하자.
  4. m1, m2, ..., m⌈n/5⌉들의 중앙값 M을 재귀적으로 구한다. 원소의 총수가 홀수이면 중앙값이 하나이므로 문제가 없고, 원소의 총수가 짝수이면 두 중앙값 중 임의로 선택한다.
  5. M을 기준원소로 삼아 전체 원소를 분할한다.
  6. 분할된 두 그룹 중 적합한 쪽을 선택해 단계 1~6을 재귀적으로 반복한다.
}
</pre>

x를 M보다 작은 원소들, o를 M보다 큰 원소들이라 하고 a를 M보다 크거나 작을 수 있는 원소들이라 가정하자.<br>
가장 바람직한 것은 a가 M의 대소 관계에 따라 왼쪽 그룹과 오른쪽 그룹으로 흩어지는 것이다.<br>
최악의 경우에는 모두 한쪽으로 몰릴 수도 있음.<br>
그럼 이런 최악의 경우에 분할의 균형은 어느 정도까지 나빠질 수 있을까?<br>
o그룹에는 적어도 3n/10-3개의 원소가 포함됨.<br>
M까지 포함하면 3n/10-2개임.<br>
이들을 제외한 나머지 원소는 많아야 n-(3n/10-2) = 7n/10 + 2개임.<br>
최악의 경우에는 이렇게 분할되고, 찾고자 하는 원소가 이 7n/10+2개짜리 그룹에 속함.<br>
이것으로 분할 비율은 최악의 경우에도 7n/10+2 : 3n/10-3이 되어 대략 7:3보다는 나빠지지 않을 수 있게 됨.<br>
select에서 이렇게 분할의 비율이 어느 정도 보장된다면 바로 선형 시간 알고리즘이 됨.<br>
그렇지만 이렇게 분할의 균형을 어느 정도 보장하기 위해서 무시 못할 오버헤드가 듬.<br>
이것은 좋은 기준 원소를 정하는 오버헤드임.<br>
이것으로 얻는 이득이 오버헤드를 극복한다면 좋은 결과를 이끌어낼 수 있음.<br>
<br>
단계 1은 원소의 총 수가 고작 5개 이하인 경우로 상수 시간이 소요될 뿐더러 반복적인 자기호출의 맨 마지막에 단 한 번만 수행되므로 전체 수행 시간에 영향을 주지 않음.
단계 2는 n개의 원소를 5개짜리 그룹으로 나누는 것이므로 각 원소를 한 번씩만 지나가면서 소속 그룹을 정해주면 되어 Θ(n)임.
단계 3은 각 그룹에서 중앙값을 찾는 데 상수 시간이 들고, 이런 작업을 ⌈n/5⌉번 하므로 Θ(n)임.
단계 5는 partition이므로 Θ(n)의 시간이 소요됨.
단계 2, 3, 5를 모두 합쳐서 Θ(n)이 소요됨.
단계 4와 단계 6은 자기호출을 하는 부분임.
즉, 동일한 linearSelect() 알고리즘을 호출하는데 단계 4는 입력의 크기가 ⌈n/5⌉, 단계 6은 입력의 크기가 최대 7n/10 + 2임.
<pre>
따라서 앞 알고리즘의 수행 시간 점화식은 다음과 같음.
T(n) <= T(⌈n/5⌉) + T(7n/10 + 2) + Θ(n)
여기서 기준 원소를 잘 선택하는 오버헤드 부분은  T(⌈n/5⌉) + Θ(n)에 해당함.
이 식을 전개하면 다음과 같음.
T(n) <= T(⌈n/5⌉) + T(7n/10 + 2) + Θ(n)
     <= T(n/5 + 1) T(7n/10 + 2) + Θ(n)
n0 <= k < n인 모든 k에 대해서 T(k) <= ck라고 가정하면 다음과 같음(n0는 경계치)
     <= c(n/5+1) + c(7n/10+2) + Θ(n)
     = c(9n/10 + 3) + Θ(n)
     = cn - cn/10 + 3c + Θ(n)
     <= cn
(-cn/10이 3c + Θ(n)을 압도할 수 있도록 하는 상수 c가 존재하기 때문에 성립함.)
따라서 T(n) <= cn이므로 T(n)=O(n)이다.
T(n) = Ω(n)임은 명백하므로 T(n) = Θ(n)임.
</pre>

n보다 작은 k에 대해서 T(k) <= ck라고 가정해서 위와 같은 전개가 가능했음.<br>
그런데 이것을 만족하려면 n/5+1과 7n/10+2가 각각 n보다 작아야함.<br>
이를 만족하려면 n이 7이상이면 됨.(즉, 경계치 n0가 7이상이 됨.)<br>
알고리즘의 점근적 시간 분석은 충분히 큰 n에 대한 것이므로 n이 7이상이라는 조건은 아주 가벼운 것임.<br>
n/5 <= k < n인 모든 k에 대해 T(k) <= ck라고 가정해도 됨.<br>

<h2><a id="6">:pencil2: Chapter6. 검색 트리</a></h2>

**:pushpin: 레코드, 키의 정의 및 검색 트리**

레코드는 개체에 대한 모든 정보를 포함하고 있음.<br>
사람의 레코드라면 주민번호, 이름, 주소 등이 담김.<br>
이 각각의 정보를 나타내는 부분을 필드라고 함.<br>
검색 트리에 레코드를 다 저장할 수도 있으나 보통은 해당 레코드를 대표할 수 있는 필드만으로 검색 트리를 만듬.<br>
사람 레코드의 경우 주민번호만 있으면 그 레코드를 대표할 수 있으므로 주민번호를 대표 필드로 삼아 트리를 만들 수 있음.<br>
이렇게 다른 레코드와 중복되지 않으면서 레코드를 대표할 수 있는 필드를 검색키 또는 키라고 함.<br>
키는 필드 하나로 구성할 수도 있고, 복수 개의 필드로 구성할 수도 있음.<br>
검색 색인을 만들기 위해서는 레코드 대신 키와 해당 레코드가 저장된 위치 정보만 있으면 됨.<br>
<br>
검색 트리는 한 노드에서 최대 몇 개의 자식 노드로 분기할 수 있느냐에 따라 이진 검색 트리와 다진 검색 트리로 나눔.<br>
이진 검색 트리는 최대 두 개의 자식 노드를 가질 수 있고, 다진 검색 트리는 세 개 이상의 자식 노드로 분기할 수 있음.<br>
일반적으로 k진 다진 검색 트리라 하면 자식을 최대 k개까지 가질 수 있는 검색트리를 뜻함.<br>
검색 트리는 저장되는 장소에 따라 내부 검색 트리와 외부 검색 트리로 나뉨.<br>
내부 검색 트리는 검색 트리가 메인 메모리 내에 존재하고, 외부 검색 트리는 검색 트리가 외부(주로 디스크)에 존재함.<br>
메인 메모리에서 모든 키를 수용할 수 있으면 검색 트리 전체를 메인 메모리로 한 번만 탑재한 후 내부 검색 트리로 사용할 수 있음.<br>
반면, 메인 메모리에서 모든 키를 수용할 수 없을 정도로 크면 디스크 공간에 저장된 상태로 검색을 해야 함.<br>
그러므로 외부 검색 트리의 경우에는 디스크 접근 시간이 검색의 효율을 좌우함.<br>
검색 트리는 검색키가 포함하는 필드의 수에 따라 일차원 검색 트리와 다차원 검색 트리로 나뉨.<br>
키를 구성하는 필드가 하나이면 일차원 검색 트리, 두 개 이상이면 다차원 검색 트리임.<br>
이진 검색 트리, 다진 검색 트리, B트리, AVL트리, 레드 블랙 트리 등은 모두 일차원 검색 트리임.<br>
KD트리, KDB트리, R트리 등은 다차원 검색 트리임.<br>

**:pushpin: 이진 검색 트리**

이진 검색 트리 특성
<pre>
1. 이진 검색 트리의 각 노드는 키 값을 하나식 갖음. 각 노드의 키 값은 모두 달라야함.
2. 최상위 레벨에 루트 노드가 있고, 각 노드는 최대 두 개의 자식 노드를 갖음.
3. 임의의 노드의 키 값은 자신의 왼쪽에 있는 모든 노드의 키 값보다 크고, 오른쪽에 있는 모든 노드의 키 값보다 작음.
</pre>

이진 검색 트리 검색
<pre>
treeSearch(t, x):
{
  if (t = NIL or key[t] = x) then return t;
  if (x < key[t])
    then return treeSearch(left[t], x);
    else return treeSearch(right[t], x);
}
</pre>

**:pushpin: 이진 검색 트리: 삽입**

원소 x를 이진 검색 트리에 삽입하려면 우선 이진 검색 트리에 x를 키 값으로 가진 노드가 없어야 함.<br>
원소 x를 삽입할 자리를 찾기 위해서는 우선 실패하는 검색을 한 번 수행해야 함.<br>
즉, 루트 노드에서 x에 대한 검색을 수행해 임의의 리프 노드에 이르러 더 이상 내려갈 곳이 없음이 확인되면 x를 그 리프 노드의 자식으로 매달음.<br>
이진 검색 트리의 모양은 원소들이 삽입되는 순서에 따라 결정됨.<br>
<br>
이진 검색 트리에서 삽입
<pre>
treeInsert(t, x):
{
  if (t = NIL) then {
    key[r] <- x; left[r] <- NIL; right[r] <- NIL;
    return r
  }
  
  if (x < key[t])
    then {left[t] <- treeInsert(left[t], x); return t;}
    else {right[t] <- treeInsert(right[t], x); return t;}
}
</pre>
n개의 원소로 이진 검색 트리를 만들 때, 이진 검색 트리가 가장 이상적으로 균형이 잡히면 최악의 경우라 하더라도 검색 시간은 Θ(logn)임.<br>
가장 나쁘게 기울면 평균 검색 시간이 Θ(n)이 됨.<br>
가능한 모든 삽입 순서에 따른 이진 검색 트리를 모두 고려하면 평균 검색 시간은 Θ(logn)임.<br>
삽입은 실패하는 검색 후 상수 시간의 후처리를 하므로 점근적 수행 시간은 검색과 동일함.<br>

Note
<pre>
n개의 키로 만들 수 있는 상대적인 순서는 모두 n!개임.
가능한 모든 순서를 고려할 때 임의의 키를 검색하는 데 필요한 평균 시간은 O(logn)임.
이진 트리에서 각 노드의 깊이를 더한 것을 내부 경로 길이(Internal Path Length,IPL)임.
7개의 노드가 있을 때 IPL이 26이면 평균 2.6번의 비교가 필요한 것임.
</pre>

평균 검색 시간 증명
<pre>
키의 총 수가 n개인 모든 이진 검색 트리의 평균 IPL은 O(nlogn)임.

D(n)을 키가 n개인 모든 이진 검색 트리의 평균 IPL이라고 하자.
D(0) = 0, D(1) = 1, D(2) = 3, D(n)의 점화식은 다음과 같음.

D(n) = (1/n) * ∑(i=0~n-1)(D(i) + D(n-i-1)) + n = (2/n) * ∑(i=0~n-1)(D(i)) + n

n = 2일 때 D(2) <= c2log2가 만족하도록 c를 잡을 수 있음.
모든 2 <= k < n에 대해 D(k) <= cklogk가 성립한다고 가정하자.(귀납적 가정)

D(n) = (2/n)∑(i=0~n-1)D(i) + n = (2/n)∑(i=2~n-1)D(i) + (2/n)D(1) + n
    <= (2/n)∑(i=0~n-1)cilogi + n + 2/n
    <= (2/n)∫(1~n)cxlogxdx + n + 2/n
    = (2c/n)[(1/2) * (n**2) * logn - (1/4) * (n ** 2) + 1/4] + n + (2/n)
    = cnlogn - cn/2 + c/2n + n + 2/n
    = cnlogn + (2-c)n/2 + (c+4) / 2n
    <= cnlogn

c = 3이면 2<=n이면 모든 n에 대해 D(n) <= cnlogn이므로 D(n) = O(nlogn)임.

이 정리는 임의 노드의 평균 깊이는 O(logn)이 됨을 의미함. 따라서 임의 노드를 검색할 때 성공적인 검색의 평균 시간은 O(logn)임.
</pre>

**:pushpin: 이진 검색 트리: 삭제**

노드 r을 삭제하고자 할 때는 다음 세 가지 경우에 따라 각각 다르게 처리를 해주어야 함.<br>
case 1: r이 리프 노드인 경우<br>
case 2: r의 자식 노드가 하나인 경우<br>
case 3: r의 자식 노드가 두 개인 경우<br>
<br>
이 중에서 r의 자식이 둘인 경우를 살펴보자.<br>
r의 부모가 r을 가리키던 포인터는 하나임.<br>
r자리에 옮겨놓아도 이진 검색 트리의 성질을 꺠지 않는 원소를 찾아야함.<br>
왼쪽 서브 트리에서 가장 큰 원소(크기 순으로 r의 직전 원소)와 오른쪽 서브 트리에서 가장 작은 원소(크기 순으로 r의 직후 원소)임.<br>
둘 중 하나를 택해 키를 r의 자리로 옮김.(여기서는 r의 직후 원소를 선택함.)<br>
그런 다음 직후 원소가 들어 있던 노드를 삭제함.<br>
다행히 직후 원소는 절대 왼쪽 자식을 가질 수 없음.<br>
그러므로 이 직후 원소의 삭제는 case1이나 case2에 속하게 되어 비교적 간단한 삭제 작업이 됨.<br>

<pre>
treeDelete(t, r, p):
{
  if (r=t) then root <- deleteNode(t);
  else if (r = left[p])
    then left[p] <- deleteNode(r);
    else right[p] <- deleteNode(r);
}

deleteNode(r)
{
  if (left[r] = right[r] = NIL) then return NIL;
  else if (left[r] = NIL and right[r] != NIL) then return right[r];
  else if (left[r] != NIL and right[r] = NIL) then return left[r];
  else {
    s <- right[r];
    while (left[s] != NIL)
      {parent <- s; s <- left[s];}
    key[r] <- key[s];
    if (s = right[r]) then right[r] <- right[s]
                      else left[parent] <- right[s];
    return r;
  }
}
</pre>

case1과 case2는 상수 시간이 들음.<br>
case3은 노드 r의 직후 원소를 찾는데 최악의 경우 트리의 높이에 비례하는 시간이 들음.<br>
직후 원소를 찾은 다음에 삭제하는 것은 case1 또는 case2에 해당되므로 상수 시간이 들음.<br>
따라서 삭제 작업을 위한 최악의 시간은 트리의 높이에 따라 O(logn)과 O(n) 사이에서 결정됨.<br>

**:pushpin: 레드 블랙 트리**

이진 검색 트리는 균형이 깨지면 Θ(n)에 근접한 시간이 소요될 수 있음.<br>
그래서 필요한 것이 균형잡힌 이진 검색 트리임.<br>
균형잡힌 이진 검색 트리는 최악의 경우에도 이진 트리의 균형이 잘 맞도록 유지함.<br>
레드 블랙 트리는 이진 검색 트리의 모든 노드에 레드 또는 블랙의 색상을 칠함.<br>
단 다음의 성질을 만족해야 함.<br>
<pre>
1. 루트는 블랙이다.
2. 모든 리프(NIL)은 블랙이다.
3. 노드가 레드이면 그 노드의 자식은 반드시 블랙이다.
4. 루트 노드에서 임의의 리프 노드에 이르는 경로에서 만나는 블랙 노드의 수는 모두 같다.
</pre>
노드 하나를 할당하여 이를 리프로 정하고 모든 NIL 리프에 대한 포인터가 이 노드를 가리키도록 처리함.<br>
이렇게 하면 공간을 절약할 수 있을 뿐더러 경계조건을 다루기도 편리해짐.<br>
레드 블랙 트리에서 검색은 트리의 내용을 건드리지 않으므로 이진 검색 트리에서 검색과 동일함.<br>
삽입과 삭제도 기본적으로는 이진 검색 트리와 동일하지만 삽입이나 삭제 후 레드 블랙 특성에 위반하는 경우가 생길 수 있음.<br>
이 때는 적절한 작업을 해서 레드 블랙 특성을 만족하도록 바로잡아 주어야 함.<br>

**:pushpin: 레드 블랙 트리: 삽입**

레드 블랙 트리에서 노드를 삽입할 때는 먼저 이진 검색 트리의 삽입 알고리즘에 따라 삽입을 한 다음 새 노드의 색상을 레드로 색칠함. 이 노드를 x라고 하자.<br>
새 노드는 항상 맨 아래쪽에 매달리므로 삽입 직후에 x의 아래쪽은 블랙 노드인 리프 2개만 있어 레드 블랙 특성에서 문제가 생기지 않음.<br>
x의 위쪽과 관련해서 문제가 생기는지만 확인하면 됨.<br>
x의 부모 노드 p가 블랙이면 그것으로 삽입은 완료됨.<br>
p가 레드인 경우만 해결하면 됨.<br>
p가 레드이면 레드 노드가 2개 연속으로 있으므로 레드 블랙 특성 3을 위반함.<br>
그런데 삽입 전에는 레드 블랙 트리였으므로 특성 3에 따라 p의 부모 노드는 반드시 블랙임. 이를 p^2이라 하자.<br>
마찬가지로 특성 3에 따라 x의 형제 노드도 반드시 블랙임.<br>
x 주변에서 레드나 블랙 두 가지 다 가능한 것은 p의 형제 노드 s뿐임.<br>
s의 색상에 따라 다음 두 가지 경우로 나뉨.<br>

<pre>
case 1: s가 레드
case 2: s가 블랙
case 2-1: x가 p의 오른쪽 자식
case 2-2: x가 p의 왼쪽 자식
</pre>

case1<br>
p와 s의 색상을 레드에서 블랙으로 바꾸고 p^2의 색상을 블랙에서 레드로 바꿈.<br>
p^2가 루트이면 p^2의 색상을 다시 블랙으로 바꾸고 끝냄.<br>
p^2가 루트가 아니면 p^2가 부모 색상을 확인해야 함.<br>
p^2의 부모 색상이 블랙이면 레드 블랙 특성이 모두 만족함.<br>
p^2의 부모 색상이 레드이면 레드 블랙 특성 3이 위반되어 처음과 똑같은 문제가 발생함.<br>
이것은 원래 x에 대해서 발생했던 문제와 똑같은 문제가 p^2에 대해서 발생했음을 뜻함.<br>
p^2을 문제 발생 노드로 하여 재귀적으로 다시 시작함.<br>
<br>
case2-1<br>
p를 중심으로 왼쪽으로 회전함.<br>
case2-2로 이동함.<br>
<br>
case2-2<br>
p^2을 중심으로 오른쪽으로 회전하고 p와 p^2의 색상을 맞바꿈.<br>
<br>
case2를 만나면 case2-2의 수선을 마지막으로 상황이 종료됨.<br>
case1을 만나면 상황이 끝날 수도 있고 똑같은 상황이 다른 노드에서 시작될 수도 있음.<br>
이런 상황이 재귀적으로 반복되어 루트까지 올라갈 수도 있음.<br>

**:pushpin: 레드 블랙 트리: 삭제**

레드 블랙 트리에서 노드를 삭제할 때는 기본적으로 이진 검색 트리에서 삭제 방법에 따라 노드를 삭제한 후 색상을 맞춰줌.<br>
이진 검색 트리에서 임의의 노드 d를 삭제할 때 d의 자식이 둘이면 d의 오른쪽 서브 트리에서 최소 원소(노드 d의 직후 원소)를 가진 노드 m의 키를 노드 d로 옮긴 다음 노드 m을 삭제함.<br>
노드 d의 색상을 건드리지 않은 채 키만 바뀌는 것은 레드 블랙 특성에는 영향을 미치지 않음.<br>
문제가 되는 것은 최소 원소 m을 삭제한 후 m 주변의 레드 블랙 특성의 위반 여부임.<br>
최소 원소 노드 m은 왼쪽 자식을 갖지 않음.<br>
따라서 최소 원소 노드 m은 최대 한 개의 자식만을 가질 수 있으므로 두 개의 자식을 가진 노드의 삭제 작업은 자식이 없거나 한 개만을 가진 노드의 삭제 작업으로 귀결됨.<br>
따라서 레드 블랙 트리에서 삭제 작업을 자식이 없거나 한 개만을 가진 노드의 삭제에 국한해도 무방함.<br>
<br>
삭제하려고 하는 노드 m의 자식을 x라고 하자. 자식이 없으면 x는 NIL 노드가 되는데 이를 구별할 필요는 없음.<br>
m은 자기 부모 노드의 왼쪽 자식일 수도 있고, 오른쪽 자식일 수도 있음.<br>
두 경우는 완전히 대칭적이므로 m이 자기 부모의 왼쪽 자식임을 가정하겠음.<br>
m이 레드이면 삭제 후 아무런 조치가 필요없음.<br>
따라서 m이 블랙인 경우만 고려하면 됨.<br>
m이 블랙이더라도 x가 레드이면 삭제 후 x의 색상을 블랙으로 바꾸어버리면 레드 블랙 특성을 만족함.<br>
m과 x의 색상이 모두 블랙일 때가 까다로움.<br>
m과 x가 블랙인 상태에서 m이 삭제되면 x는 m의 부모 p의 자식이 되고 루트에서 x를 통과하는 경로의 블랙 노드 개수가 한 개 모자라서 레드 블랙 특성 4가 깨짐.<br>
p의 색상에 따라 case1(레드)와 case2(블랙)으로 나뉨.<br>
p가 레드이면 s(x의 형제노드)는 반드시 블랙이고, l(x의 왼자식)과 r(x의 오른자식)은 모든 색상의 조합이 가능함.<br>

case1
<pre>
p가 레드이므로 s는 반드시 블랙. l의 색상, r의 색상에 따라
case 1-1: <블랙, 블랙>
case 1-2: <레드, 레드> 또는 <블랙, 레드> = <*, 레드>
case 1-3: <레드, 블랙>
</pre>

case2
<pre>
p가 블랙이므로, s의 색상, l의 색상, r의 색상에 따라
case 2-1: <블랙, 블랙, 블랙>
case 2-2: <블랙, 레드, 레드> 또는 <블랙, 블랙, 레드> = <블랙, *, 레드>
case 2-3: <블랙, 레드, 블랙>
case 2-4: <레드, 블랙, 블랙>
</pre>

이 때, case 1-2와 2-2는 p의 색상만 다름. p의 색상이 처리 방법에 영향을 미치지 않으므로 통합함.<br>
case 1-3과 2-3도 마찬가지 이유로 통합함.<br>

case 통합
<pre>
case 1-1: <블랙, 블랙, 블랙>
case *-2: <*, *, 레드> 
case *-3: <*, 레드, 블랙>
case 2-4: <레드, 블랙, 블랙>
</pre>

case 1-1<br>
단순히 p와 s의 색상을 맞바꿈. x에 이르는 경로상에서 블랙이 하나 추가되었으므로 x에 이르는 경로에서 블랙 노드가 하나 모자라던 것이 해소됨. 루트에서 s를 지나는 경로상의 블랙 노드의 수에는 변화가 없음. 특성 4가 만족됨.<br>
case *-2<br>
p를 중심으로 왼쪽으로 회전시키고, p와 s의 색상을 맞바꾼 다음, r의 색상을 레드에서 블랙으로 바꿈.<br>
x에 이르는 경로상에서 블랙이 하나 추가되었으므로 x를 지나는 경로에서 블랙 노드 하나가 모자라던 것이 해소됨.<br>
딸려있던 서브 트리들은 루트에서 지나가는 경로상에 있는 블랙 노드의 수에 변화가 없음.<br>

**:pushpin: B-트리**

