<h1>:green_book: <쉽게 배우는 알고리즘> 정리</h1>

<a href="#1">:pencil2: Chapter1. 알고리즘 설계와 분석의 기초</a>
- O 표기법
- Ω 표기법
- Θ 표기법
- o 표기법
- ω 표기법

<a href="#2">:pencil2: Chapter2. 점화식과 알고리즘 복잡도 분석</a>
- 반복 대치
- 추정 후 증명
- 마스터 정리

<a href="#3">:pencil2: Chapter3. 정렬</a>
- 기본적인 정렬 알고리즘: 선택 정렬
- 기본적인 정렬 알고리즘: 버블 정렬
- 기본적인 정렬 알고리즘: 삽입 정렬
- 고급 정렬 알고리즘: 병합 정렬
- 고급 정렬 알고리즘: 퀵 정렬
- 고급 정렬 알고리즘: 힙 정렬
- 특수 정렬 알고리즘: 기수 정렬
- 특수 정렬 알고리즘: 계수 정렬

<h2><a id="1">:pencil2: Chapter1. 알고리즘 설계와 분석의 기초</a></h2>

**:pushpin: O 표기법**

O(g(n)) = {f(n) | ∃c > 0, n0 > 0 s.t. ∀n >= n0, f(n) <= cg(n)}<br>
O(g(n)) = {f(n) | 모든 n > n0에 대하여 f(n) <= cg(n)인 양의 상수 c와 n0가 존재한다}<br>
O(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 f(n) <= cg(n)인 양의 상수 c가 존재한다}<br>
<br>
O(g(n))은 충분히 큰 n에 대하여 g(n)에 상수만 곱하면 g(n)이 누를 수 있는 모든 함수의 집합임.<br>
n^2에 3보다 크거나 같은 상수를 곱하면 3n^2을 누를 수 있으므로 3n^2은 O(n^2)에 속함.<br>
<br>
5n^2 = O(n^2)임을 보여라.<br>
c를 6, n0을 1로 잡으면 모든 n >= n0(=1)에 대하여 5n^2 <= 6n^2임. 즉 정의를 만족하는 c와 n0이 존재함.<br>

**:pushpin: Ω 표기법**
  
  Ω(g(n)) = {f(n) | ∃c > 0, n0 > 0 s.t. ∀n >= n0, cg(n) <= f(n)}<br>
  Ω(g(n)) = {f(n) | 모든 n > n0에 대하여 cg(n) <= f(n)인 양의 상수 c와 n0가 존재한다}<br>
  Ω(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 cg(n) <= f(n)인 양의 상수 c가 존재한다}<br>
  <br>
  Ω(g(n))은 충분히 큰 n에 대하여 g(n)에 상수만 곱하면 g(n)이 압도당할 수 있는 모든 함수의 집함임.<br>
  <br>
  5n^2 = Ω(n^2)임을 보여라.<br>
  c를 4로 잡고, n0 = 1로 잡으면 모든 n >= n0(=1)에 대하여 4n^2 <= 5n^2임. 즉 정의를 만족하는 c와 n0이 존재함.<br>
  
**:pushpin: Θ 표기법**
 
  Θ(g(n)) = O(g(n)) ∩ Ω(g(n))<br>
  Θ(g(n)) = {f(n) | ∃c1, c2 > 0, n0 > 0 s.t. ∀n >= n0, c1g(n) <= f(n) <= c2g(n)}<br>
  Θ(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 c1g(n) <= f(n) <= c2g(n)인 양의 상수 c1, c2가 존재한다.}<br>
  <br>
  5n^2 = Θ(n^2)임을 보여라.<br>
  5n^2 = O(n^2), 5n^2 = Ω(n^2)임을 보였기 때문에 5n^2 = Θ(n^2)이다.<br>

**:pushpin: o 표기법**
  
  함수의 증가율이 점근적 의미에서 어느 한계보다 더 작다는 것을 표현하고자 할 때 사용됨.<br>
  예를 들어 함수 5n = o(n^2)이다. 5n의 증가율은 n^2의 증가율보다 작기 때문이다.<br>
  그렇지만 함수 (1/2) * n^2은 o(n^2)에 속하지 않음. (1/2) * n^2의 증가 속도가 n^2의 증가 속도와 점근적으로 동일하기 때문임.<br>
  <br>
  o(g(n)) = {f(n) | lim f(n) / g(n) = 0 }<br>
  o(g(n)) = {f(n) | ∃n0 > 0 s.t. ∀c>0 and n>=n0, f(n) < cg(n)}<br>
  o(g(n)) = {f(n) | n이 충분히 크면 모든 c>0에 대하여 f(n) < cg(n)이다.}<br>
  o(g(n))은 충분히 큰 n에 대하여 g(n)에 아무리 작은 상수를 곱해도 g(n)이 압도하는 모든 함수의 집합임.<br>
  <br>
  5n^2 = o(n^3)임을 보여라.<br>
  lim (n^2 -5) / n^3 = 0임.<br>
  
**:pushpin: ω 표기법**
  
  함수의 증가율이 점근적 의미에서 어느 한계보다 더 크다는 것을 표현하고자 할 때 사용함.<br>
  예를 들어 5n^3 = ω(n^2)임. 5n^3의 증가 속도는 n^2의 증가 속도보다 큼.<br>
  함수 2n^2은 ω(n^2)에 속하지 않음. 2n^2의 증가 속도는 n^2의 증가 속도와 점근적으로 동일하기 때문임.<br>
  ω표기는 함수의 기울기상의 여유 있는 하한을 나타냄.<br>
  <br>
  ω(g(n)) = {f(n) | lim f(n) / g(n) = ∞}<br>
  ω(g(n)) = {f(n) | ∃n0 > 0 s.t. ∀c>0 and n>=n0, cg(n) < f(n)}<br>
  ω(g(n)) = {f(n) | n이 충분히 크면 모든 c > 0에 대하여 cg(n) < f(n)이다.}<br>
  <br>
  n^3 / 4 = ω(n^2)임을 보여라.<br>
  lim ( (n^3 /4) / n^2 ) = ∞이므로 n^3 / 4 = ω(n^2)이다.<br>

<h2><a id="2">:pencil2: Chapter2. 점화식과 알고리즘 복잡도 분석</a></h2>


점화식은 어떤 함수를 자신과 똑같은 함수를 이용해 나타내는 것임.<br>
n!의 점화식은 f(n) = n * f(n-1), 피보나치 수열의 점화식은 f(n) = f(n-1) + f(n-2)임.<br>

**:pushpin: 반복 대치**

<pre>
factorial(n)
{
  1. if (n = 1) return 1;
  2. return n * factorial(n-1);
}
</pre>

이 알고리즘으로 n!을 구하는데 걸리는 시간을 T(n)이라고 하면, T(n) = T(n-1) + c임.<br>
c는 자기호출을 제외한 나머지의 수행 시간으로 1을 수행하는 시간과 2의 곱셈을 한번 수행하는 시간임.<br>
크기가 1이면 T(1) <= c임.<br>

<pre>
T(n) = T(n-1) + c
     = T(n-2) + 2c
     ...
     = T(1) + (n-1)c
     <= cn
</pre>

T(n) <= cn이므로 T(n) = O(n)이다.<br>

**병합 정렬**<br>

입력의 크기가 n인 배열을 정렬하는 병합 정렬은 배열을 이등분한 다음, 각각을 재귀적으로 병합 정렬해 이 둘을 병합함으로써 정렬을 끝냄.<br>
입력의 크기가 n인 배열에 대한 병합 정렬에서 대소 비교의 총 횟수를 T(n)이라고 하자.<br>
n = 2^k이라고 가정해도 일반성을 잃지 않으므로 n = 2^k으로 가정하겠음.<br>
T(n) <= 2T(n/2) + n임.<br>
n은 merge(A, p, q, r)에 필요한 최대 비교 횟수 n-1과 if (p<r) then에 있는 비교 한 번을 합한 것임.<br>

<pre>
T(n) <= 2T(n/2) + n
     <= 2^2T(n/2^2) + 2n
     ...
     <= 2^kT(n/2^k) + kn
     = nT(1) + nlogn
     = n + nlogn
      = O(nlogn)
</pre>

T(n) <= 2T(n/2) + n을 T(n) <= 2T(n/2) + O(n)으로 표시하기도 함.<br>
여기서 O(n)은 집합으로서 O(n)을 의미하지 않고, O(n)에 속하는 함수 하나를 대신하는 관행적 표현임.<br>

**n = 2^k**<br>

점근적 복잡도의 계산을 용이하게 하기 위해 n = 2^k이라는 가정을 종종 사용함.<br>
어떠한 n이라도 n과 2n 사이에 2의 멱수가 하나 있음.<br>
즉, n <= 2^k <= 2n인 2^k가 하나 존재함.<br>
임의의 상수 r에 대해 T(n) = O(n^r)이라면 T(2n) = O(2^r * n^r) = O(n^r)이므로 T(n) = T(2n)임.<br>
T(n) <= T(2^k) <= T(2n)이고 T(n) = T(2n) 이므로 T(n) = T(2^k) = T(2n)임.<br>
즉, n의 오른쪽에서 처음으로 마나는 2의 멱수에 대한 함수도 항상 n에 대한 함수와 같은 점근적 복잡도를 가지므로 n = 2^k로 가정해도 점근적 분석의 결과는 같음.<br>

**:pushpin: 추정 후 증명**

추정 후 증명은 식의 모양을 보고 점근적 복잡도를 추정한 다음 그것이 옳음을 귀납적으로 증명하는 방법임.<br>

<pre>
T(n) <= 2T(n/2) + n의 점근적 복잡도는 T(n) = O(nlogn)이다. 즉 충분히 큰 n에 대하여 T(n) <= cnlogn인 양의 상수 c가 존재한다.

경계조건: T(2) <= c2log2를 만족하는 c가 존재한다.

귀납적 가정과 전개: n/2에 대해 T(n/2) <= c(n/2)log(n/2)을 만족한다고 가정하면,
T(n) <= 2T(n/2) + n
     <= 2c(n/2)log(n/2) + n
     = cnlogn - cnlog2 +n
     <= cnlogn
     
이를 만족하는 상수 c가 존재한다. 따라서 T(n) = O(nlogn)이다.
</pre>

여기서 log1 = 0이므로 T(1) <= 1log1은 불가능하기 때문에 T(2)를 경계조건으로 잡은 것임.<br>
경계조건으로 T(2)가 아닌 훨씬 큰 상수 a에 대해 T(a) <= caloga임을 보여도 상관없음.<br>
일반적으로  T(n) = f(n)임을 보이기 위해 상수 a를 경계치로 잡으면 T(a)도 상수가 되므로 f(n)이 양수인 한 T(a) <= cf(a)를 만족시키는 c가 항상 존재함.<br>
그러므로 경계조건을 만족시키는 경계치는 항상 잡을 수 있기 때문에 경계조건을 확인하지 않아도 됨.<br>

<pre>
T(n) <= 2T(n/2 + 10) + n의 점근적 복잡도는 T(n) = O(nlogn)이다. 즉, 충분히 큰 n에 대하여 T(n) <= cnlogn인 양의 상수 c가 존재한다.

증명

T(n) <= 2T(n/2 + 10) + n
     <= 2c(n/2 + 10)log(n/2 + 10) + n
     = cnlog(n/2 + 10) + 20clog(n/2 + 10) + n
     <= cnlog(3n / 4) + 20clog(3n / 4) + n
     = cnlogn + cn(log3 - log4) + 20clog(3n / 4) + n
     <= cnlogn
     
이를 만족하는 상수 c가 존재한다. 따라서 T(n) = O(nlogn)이다.
이 때, n에 대한 제약이 있다.
n/2 + 10 < n이어야 하므로 n > 20이어야 논리적인 전개가 가능하다.
식을 간단하게 하기 위해서 n/2 + 10 <= 3n /4임을 가정했는데 이는 n >= 40이어야 가능하다.
</pre>

cn(log3 - log4) + 20clog(3n / 4) + n이 음수가 되어야 하는데, c(log3 - log4) + 1을 충분히 작은 음수로 만들 수 있으면 20clog(3n/4)이 n에 대한 차수가 낮아 압도할 수 있음.<br>

**주의**<br>
<pre>
T(n) <= 2T(n/2) + n의 점근적 복잡도가 T(n) = O(n)이라고 가정하고, 충분히 큰 n에 대하여 T(n) <= cn인 양의 상수 c가 존재한다는 것을 증명하려 할 때
T(n) <= 2T(n/2) + n
     <= 2c(n/2) + n
     = cn + n
     = c'n
     = O(n)
</pre>
c' = c + 1이라는 추가적은 상수를 만들어서 c'n <= cn이라고 주장하면 안 됨.<br>
처음에 쓴 c와 나중에 쓰는 c는 같은 상수여야 함.<br>

<pre>
T(n) <= 2T(n/2) + n
     <= 2c(n/2) + n
     = cn + n
     = O(n)
</pre>
n(c+1) = O(n)인건 맞지만 T(n) <= cn + n <= cn임을 증명하지 못 했음.<br>
따라서 귀납적 증명이 완결되지 않았으므로 틀렸음.<br>

<pre>
T(n) = 2T(n/2) + 1의 점근적 복잡도는 O(n)이다.

실패하는 증명

충분히 큰 n에 대해 T(n) <= cn인 양의 상수 c가 존재한다는 것을 증명하려 한다.
T(n) = 2T(n/2)  + 1
     <= 2c(n/2) + 1
     <= cn + 1
여기서 cn + 1 < cn임을 입증할 수 없음.

성공하는 증명

T(n) <= cn임을 증명하는 대신 T(n) <= cn - 2임을 증명할 수 있어도 T(n) = O(n)임.<br>
T(n) = 2T(n/2) + 1
     <= 2(cn/2 - 2) + 1
     = cn -3
     <= cn
</pre>

추정 후 증명법이 유용하게 사용되려면 우선 추정을 의미있게 해야 함.<br>
너무 여유롭게 추정해 증명하는 것은 별로 의미가 없음.<br>

**추가: 경계조건**<br>

재귀적으로 정의된 함수나 수열은 자신의 값을 계산하기 위해 자신과 같은 함수나 수열을 더 작은 입력값으로 호출하는 경우가 많음.<br>
이러한 호출은 일반적으로 입력값이 충분히 작아지거나 특정 조건을 만족할 때까지 반복됨.<br>
이 때, 함수나 수열이 호출되는 과정에서 더 이상 호출하지 않고 종료될 때, 이를 '경계조건' 혹은 '종료조건'이라고 부름.<br>
예를 들어, 피보나치 수열은 아래와 같은 점화식으로 정의됨.<br>
<pre>
F(0) = 0
F(1) = 1
F(n) = F(n-1) + F(n-2) (n >= 2)
</pre>
이 때, F(0)와 F(1)은 경계조건으로 사용됨.<br>
F(0)와 F(1)을 미리 정의함으로써, F(n)을 계산하기 위한 종료 조건으로 사용할 수 있음.<br>

**:pushpin: 마스터 정리**

<pre>
T(n) = aT(n/b) + f(n)
</pre>

마스터 정리는 특정한 모양을 가진 재귀식에서 바로 결과를 알 수 있는 유용한 정리임.<br>
입력의 크기가 n인 문제를 풀기 위해 입력의 크기가 n/b인 문제를 a개 풀고, 나머지 f(n)의 오버헤드가 필요한 알고리즘들이 해당됨.<br>
a >= 1, b >= 1에 대해 T(n) = aT(n/b) + f(n)인 점화식에서, h(n) = n^(logb(a))라고 할 때 T(n)의 점근적 복잡도는 다음과 같음.<br>
<pre>
어떤 양의 상수 ε에 대하여 f(n) / h(n) = O(1/n^ε)이면, T(n) = θ(h(n))이다.
어떤 양의 상수 ε에 대하여 f(n) / h(n) = Ω(n^ε)이고, 어떤 상수 c(<1)와 충분히 큰 모든 n에 대해 af(n/b) <= cf(n)이면 T(n) = θ(f(n))이다. 
f(n) / h(n) = θ(1)이면 T(n) = θ(h(n)logn)이다.
</pre>

마스터 정리의 근사 버전은 아래와 같음.
<pre>
lim f(n) / h(n) = 0이면 T(n) = θ(h(n))이다.
lim f(n) / h(n) = ∞이고, 충분히 큰 모든 n에 대해 af(n/b) < f(n)이면 T(n) = θ(f(n))이다.
f(n) / h(n) = θ(1)이면 T(n) = θ(h(n)logn)이다.
</pre>

마스터 정리의 근사 버전은 원형과 정확히 같지는 않음.<br>
lim f(n) / h(n) = ∞은 h(n)이 f(n)을 압도한다는 뜻이고, f(n) / h(n) = O(1/n^ε)은 h(n)이 f(n)을 적어도 다항식의 비율로 압도한다는 뜻임.<br>
lim f(n) / h(n) = lim 1 / logn의 경우에는 lim f(n) / h(n) = 0이지만 logn의 비율로 압도할 뿐 다항식의 비율로 압도하지는 않음.<br>
즉. f(n) / h(n) = O(1/n^ε)은 성립하지 않음.<br>
lim f(n) / h(n) = ∞와 f(n) / h(n) = Ω(n^ε) 사이에도 다항식 비율에 관한 차이가 있음.<br>
충분히 큰 모든 n에 대해 af(n/b) < f(n)과 어떤 상수 c(<1)와 충분히 큰 모든 n에 대해 af(n/b) <= cf(n)도 고정 비율 c이하의 보장이라는 미묘한 차이가 있지만 반례를 찾기가 극히 힘들기 때문에 사실상 이 둘을 구분 없이 사용해도 무방함.<br>

<pre>
T(n) = 2T(n/3) + c (c는 상수)

a = 2, b =  3, f(n) = c, h(n) = n^log3(2)
lim f(n) / h(n) = 0이므로 T(n) = θ(n^log3(2))이다.
</pre>

<pre>
T(n) = 2T(n/4) + n

a = 2, b = 4, f(n) = n, h(n) = n^log4(2)
lim f(n) / h(n) = ∞이고 af(n/b) = 2(n/4) = n/2 < n임.
또한 n/2 <= (1/2)f(n)이므로 c = 1/2에 대해 af(n/b) <= cf(n)을 만족함.
따라서 T(n) = θ(nlogn)임.
</pre>

<pre>
T(n) = 2T(n/2) + n

a = 2, b = 2, f(n) = n, h(n) = n
f(n) / h(n) = 1이므로 T(n) = θ(nlogn)임.
</pre>

마스터 정리를 쓸 수 없는 모양인 점화식의 변수를 치환함으로써 마스터 정리를 쓸 수 있는 모양으로 변형하는 방법도 있음.<br>

<pre>
T(n) = 2T(n^(1/2)) + logn

m = log2(n)으로 놓으면

T(2^m) = 2T(2^(m/2)) + m

이는 아래와 같이 다시 표현할 수 있음.

S(m) = 2S(m/2) + m

a = 2, b = 2, f(m) = m, h(m) = m이므로
S(m) = θ(mlogm) = θ(logn loglogn)임.
</pre>

마스터 정리에서 f(n)은 크기가 n인 문제(최상위 레벨)에서 발생하는 자기 호출 이외의 오버헤드로 크기가 다른 문제들 간의 관계를 반영하는 비용임.<br>
h(n)은 반복적인 자기호출 끝에 마지막으로 크기 1인 문제를 만나는 횟수임.<br>
자기호출 때문에 부담이 더 커지면 수행 시간은 h(n)이 결정하고, 관계를 반영하는 오버헤드가 더 커지면 f(n)이 결정함.<br>
단 f(n)은 상위 레벨의 오버헤드만 의미하므로 하위 레벨에서 발생하는 오버헤드는 반영하고 있지 않음.<br>
af(n/b) <= cf(n)은 자기호출로 만나는 하위레벨의 문제들에서 발생하는 자기호출 이외의 오버헤드들의 총합이 레벨이 내려가면서 적어도 감소해야 한다는 것을 의미함.<br>

<h2><a id="3">:pencil2: Chapter3. 정렬</a></h2>

**:pushpin: 기본적인 정렬 알고리즘: 선택 정렬**

우선 배열 A[1, ..., n]에서 가장 큰 원소를 찾아 이 원소와 배열의 끝자리에 있는 A[n]과 자리를 바꿈.<br>
그러면 방금 맨 뒷자리로 옮긴 원소, 즉 가장 큰 원소는 자기 자리를 찾았으므로 더 이상 신경 쓰지 않아도 됨.<br>
이 원소는 정렬이 끝났다고 볼 수 있으므로 이제 이 원소를 제외한 나머지 원소들로 같은 작업을 반복함.<br>

<pre>
간략한 기술

selectionSort(A[], n)
{
  for last <- n downto 2 {
    A[1, ..., last]중 가장 큰 수 A[k]를 찾는다;
    A[k] <-> A[last];
  }
}
</pre>

<pre>
기호적 기술
selectionSort(A[], n)
{
  for last <- n downto 2 {
    k <- theLargest(A, last);
    A[k] <-> A[last];
  }
}

theLargest(A[], last)
{
  largeset <- 1;
  for i <- 2 to last
    if (A[i] > A[largest]) then largest <- i;
  return largest;
}
</pre>

이 알고리즘에서 입력은 배열 A[1, ..., n]임.<br>
변수 last는 정렬할 배열의 맨 마지막 인덱스, 즉 배열의 크기를 나타냄.<br>
처음에는 배열의 크기가 n으로 시작하므로, A[1, ..., n]을 정렬 대상으로 삼음.<br>
가장 큰 수를 찾아 제자리에 놓을 때마다 last는 1씩 줄어들음.<br>
선택 정렬의 수행 시간은 모든 경우에 Θ(n^2)임.<br>
배열의 크기가 n일 때 n-1번 비교, ... , 배열의 크기가 2일 때 1번 비교함.<br>
따라서 1 + ... + n-1 = n(n-1) / 2임.<br>
수를 비교하는 횟수가 전체 시간을 좌우하므로 이것을 기준으로 수행 시간을 계산함.<br>

**:pushpin: 기본적인 정렬 알고리즘: 버블 정렬**

<pre>
간략한 기술

bubbleSort(A[], n)
{
  for last <- n downto 2
    for i <- 1 to last-1
      if (A[i] > A[i+1]) then A[i] <-> A[i+1];
}
</pre>

버블 정렬의 총 순환 횟수는 (n-1) + (n-2) + ... + 2 + 1 = n(n-1) / 2임.<br>
따라서 수행 시간은 Θ(n^2)임.<br>
버블 정렬 알고리즘은 중간에 배열이 이미 정렬이 되어 있는 상태라도 계속 끝까지 무의미한 순환을 계속함.

<pre>
수정된 bubble sort

bubbleSort(A[], n)
{
  for last <- n downto 2
      sorted <- TRUE;
    for i <- 1 to last-1 {
      if (A[i] > A[i+1]) {
        A[i] <-> A[i+1];
        sorted <- FALSE;
      }
    }
    if (sorted = TRUE) then return;
}
</pre>

**:pushpin: 기본적인 정렬 알고리즘: 삽입 정렬**

삽입 정렬은 이미 정렬되어 있는 i개짜리 배열에 하나의 원소를 더 더하여 정렬된 i+1개짜리 배열을 만드는 과정을 반복함.<br>
한 개짜리 배열에서 시작하여 그 크기를 하나씩 늘리는 정렬임.<br>

<pre>
insertionSort(A[], n)
{
  for i <- 2 to n
    A[1, ..., i]의 적합한 자리에 A[i]를 삽입한다.
}
</pre>

for 루프는 문제의 크기를 하나씩 키워나가는 역할을 함.<br>
A[i]에 관심을 두는 시점에는 A[1, ..., i-1]은 항상 정렬이 되어 있음.<br>
A[i]가 A[i-1]보다 크면 앞에 있는 모든 원소보다 크므로 A[i]는 그냥 제자리에 두면 됨.<br>
그렇지 않으면 A[i-1]부터 시작해서 왼쪽으로 차례로 훑으면서 A[i]가 들어갈 자리를 찾음.<br>
A[i]가 들어가는 자리부터 시작해서 이후의 원소들은 한 칸씩 오른쪽으로 밀려남.<br>

<pre>
insertionSort(A[], n)
{
  for i <- 2 to n {
    loc <- i - 1;
    newItem <- A[i]
    
    while (loc >= 1 and newItem < A[loc]) {
      A[loc + 1] = A[loc];
      loc--;
    }
    A[loc + 1] <- newItem;
  }
}
</pre>

for 루프는 n-1번 순환함.<br>
매 for 루프에서 while은 최대 i-1번 순환함.<br>
가장 운이 좋으면 while 문은 돌아가지 않음.<br>
최악의 경우 수행 시간은 (n-1) + (n-2) + ... + 2 + 1 = n(n-1)/2임.<br>
따라서 Θ(n^2)임.<br>
보통은 대략 A[1, ..., i-1]에서 평균적으로 절반 정도를 훑고 끝낼 것임.<br>
그러므로 전체 비교 횟수는 최악의 경우에 비해 절반 정도 될 것임.<br>
그래도 시간복잡도는 Θ(n^2)임.<br>
<br>
삽입 정렬은 거의 정렬되어 있는 상태로 입력되는 경우에는 가장 매력적인 알고리즘임.<br>
배열이 완전히 정렬된 채로 입력되면 while 루프는 한 번도 수행되지 않고, for 루프는 한번 순환할 때마다 상수 시간이 소요됨.<br>
for 루프는 n-1번 순환되므로 Θ(n)에 가까운 시간이 듬.<br>
배열이 거의 정렬되어 있을 때도 삽입이 매우 수월해져 Θ(n)에 가까운 시간이 듬.<br>
버블 정렬은 배열이 이미 정렬되어 있는 경우에 무의미한 순환을 줄이기 위해 방법이 있긴 했지만 오버헤드가 생김.<br>
삽입 정렬은 별도 장치가 없어도 효율적으로 끝남.<br>
따라서 상황에 따라 가끔 삽입 정렬을 섞어서 씀.<br>
<br>
선택 정렬과 버블 정렬이 n개짜리 배열에서 시작하여 아직 정렬되지 않은 배열의 크기를 하나씩 줄이는 데 반하여, 삽입 정렬은 1개짜리 배열에서 시작하여 이미 정렬된 배열의 크기를 하나씩 늘리는 정렬임.<br>
삽입 정렬에는 수학적 귀납법의 원리가 들어가 있음.<br>
배열의 크기가 1일 때는 성립함.(이미 정렬되어 있으므로)<br>
배열의 크기가 k일 때 성립하면(정렬되어 있으면), 적절한 삽입으로 크기가 k+1일 때도 성립함(정렬됨).<br>
이것으로 삽입 정렬은 올바르게 정렬을 한다는 것이 귀납적으로 증명됨.<br>

**:pushpin: 고급 정렬 알고리즘: 병합 정렬**

병합 정렬은 먼저 입력을 반으로 나눔.<br>
이렇게 나눈 전반부와 후반부를 각각 독립적으로 정렬함.<br>
마지막으로 정렬된 두부분을 합쳐서, 즉 병합하여 정렬된 배열을 얻음.<br>
여기서 전반부, 후반부를 각각 정렬할 때도 역시 반으로 나눈 다음 정렬해서 병합함.<br>
즉, 원래의 정렬 문제와 성격이 똑같고 단지 크기만 반으로 줄였을 뿐임.<br>
병합 정렬은 자신에 비해 크기가 반인 문제를 두 개 푼 다음, 이들을 병합하는 일을 재귀적으로 반복함.<br>

<pre>
mergeSort(A[], p, r)
{
  if (p<r) then {
    q <- (p+r) / 2;
    mergeSort(A, p, q);
    mergeSort(A, q+1, r);
    merge(A, p, q, r);
  }
}

merge(A[], p, q, r)
{
  정렬되어 있는 두 배열 A[p, ..., q]와 A[q+1, ..., r]을 합쳐 정렬된 하나의 배열 A[p,...,r]을 만듬.
}
</pre>

<pre>
merge(A[], p, q, r)
{
  i <- p; j <- q + 1; t <- 1;
  while (i <= q and j <= r) {
    if (A[i] <= A[j])
    then tmp[t++] = A[i++];
    else tmp[t++] = A[j++];
  }
  
  while (i <= q)       //왼쪽 배열이 남은 경우
    tmp[t++] = A[i++];
  while (j <= r)       //오른쪽 배열이 남은 경우
    tmp[t++] = A[j++];
  i <- p; t <- 1;
  while (i <= r)
    A[i++] <- tmp[t++];
}
</pre>

T(n) <= a (if n = 1)<br>
T(n) <= 2T(n/2) + cn (if n > 1)<br>
부등호를 쓰는 이유는 좌변과 우변이 정확히 일치하지 않을 수 있기 때문임.<br>
따라서 근사적으로 나타내는 표현의 하나라고 생각하면 됨.<br>
상수 a는 크기가 1인 문제를 푸는 시간을 나타냄.<br>
상수 c는 병합에 드는 시간을 충분히 잡아주기 위해서 n에 곱한 것임.<br>
비교의 횟수만으로 수행 시간을 분석한다면 c=1로 충분함.<br>
어쨋든 병합은 선형 시간이 소요됨.<br>
n=2^k라고 가정하고 전개하면 다음과 같음.<br>

<pre>
T(n) <= 2T(n/2) + cn
     <= 2^2T(n/2^2) + 2cn
     ...
     <= 2^kT(n/2^k) + kcn
     = an + cn * logn
     = Θ(nlogn)
</pre>

병합 정렬의 수행 시간은 최악의 경우 Θ(nlogn)임.

**:pushpin: 고급 정렬 알고리즘: 퀵 정렬**

퀵 정렬은 평균적으로 가장 좋은 성능을 가져 현장에서 가장 많이 쓰는 정렬 알고리즘임.<br>
우선 정렬할 배열에서 기준원소를 하나 고름.<br>
아무 원소나 임의로 골라도 되나 여기서는 맨 뒤의 원소를 기준원소로 삼음.<br>
이 기준원소를 중심으로 더 작거나 같은 수는 왼쪽으로, 큰 수는 오른쪽으로 재배치함.<br>
기준원소는 이렇게 분할된 양쪽 부분 배열 사이에 자리하게 됨.<br>
이렇게 분할된 왼쪽 부분 배열을 따로 정렬함.<br>
마찬가지로 오른쪽 부분 배열도 따로 정렬함.<br>
기준원소는 손대지 말고 제자리에 그대로 둠.<br>
왼쪽과 오른쪽 부분 배열을 정렬할 때 퀵 정렬을 재귀적으로 사용함.<br>

<pre>
quickSort(A[], p, r)
{
  if (p<r) then {
    q <- partition(A, p, r);
    quickSort(A, p, q-1);
    quickSort(A, q+1, r);
  }
}

partition(A[], p, r)
{
  배열 A[p, ..., r]의 원소들을 A[r]을 기준으로 양쪽으로 재배치하고 A[r]이 자리한 위치를 리턴함.
}
</pre>

병합 정렬은 먼저 재귀적으로 작은 문제를 해결한 다음 후 처리를 하는데 반해서, 퀵 정렬은 선행 작업을 한 다음 재귀적으로 작은 문제를 해결하면서 바로 끝냄.<br>

<pre>
partition(A[], p, r)
{
  x <- A[r];
  i <- p -1;
  for j <- p to r-1
    if (A[j] <= x) then A[++i] <-> A[j];
  A[i+1] <-> A[r]
  return i+1;
}
</pre>

퀵 정렬의 수행 시간을 분석해보자.<br>
우선 분할은 배열을 왼쪽부터 끝까지 한 번 훑어나가는 작업이므로 Θ(n)의 시간이 듬.<br>
퀵 정렬의 수행에서 가장 이상적인 경우는 분할이 항상 반반씩 균등하게 될 때임.<br>
이 때는 T(n) = 2T(n/2) + Θ(n)이므로 병합 정렬과 같은 모양임.<br>
따라서 Θ(nlogn)이 됨.<br>
최악의 경우는 계속해서 한쪽은 하나도 없고, 다른 쪽에 다 몰리도록 분할이 되는 경우임.<br>
T(n) = T(n-1) + Θ(n)이므로 Θ(n^2)이 됨.<br>
한 쪽이 완전히 비거나 이에 근접한 상태가 반복되면 이런 비효율적인 시간이 나옴.<br>
퀵 정렬의 수행 시간은 분할이 얼마나 균형잡히게 잘 되느냐에 달려 있음.<br>
평균 수행 시간은 분할했을 때 모든 가능한 경우를 평균내면 됨.<br>
기준 원소가 1등이면 1구역과 2구역의 크기가 0:n-1, 2등이면 1:n-2, ..., i등이면 i-1:n-i임.<br>
T(n) = T(i-1) + T(n-i) + Θ(n)임.<br>
기준원소는 동일한 확률로 1등부터 n등 중의 하나가 되므로 이들을 평균하면 다음과 같음.<br>
T(n) = (1/n) * Σ(i=1~n)[T(i-1) + T(n-i)] + Θ(n) = (2/n) * Σ(k=0~n-1)T(k) + Θ(n)임.<br>
이를 계산하면 T(n) = Θ(nlogn)임.<br>

<pre>
위 내용을 증명하겠음.

우선 T(2) <= c2log2를 만족하도록 충분히 큰 c를 잡을 수 있음.
2 <= k <= n인 모든 k에 대해 T(k) <= cklogk가 성립한다 가정하고 T(n) <= cnlogn이 됨을 증명하면 됨.

T(n) = (1/n) * Σ(i=1~n)[T(i-1) + T(n-i)] + Θ(n)
     = (2/n) * Σ(k=0~n-1)T(k) + Θ(n)
     = (2/n) * Σ(k=2~n-1)T(K) + Θ(n)  // k=0, k=1일 때는 Θ(n)에 흡수됨.
     <= (2/n) * Σ(k=2~n-1)cklogk + Θ(n)
     <= (2c/n) * [(1/2) * n^2 * logn - (1/8) * n^2] + Θ(n) // 이 부분 아래에 증명하겠음.
     = cnlogn - cn/4 + Θ(n)
     <= cnlogn
     
Σ(k=1~n-1)klogk = Σ(k=1~(n/2-1))klogk + Σ(k=(n/2)~n-1)klogk
왼쪽 항의 logk는 log(n/2)을 상한으로 잡을 수 있고, 오른쪽 항의 logk는 logn을 상한으로 잡을 수 있음.
Σ(k=1~n-1)klogk <= log(n/2)Σ(k=1~(n/2-1))k + lognΣ(k=(n/2)~n-1)k 
                 = lognΣ(k=1~(n/2-1))k + lognΣ(k=(n/2)~n-1)k - Σ(k=1~(n/2-1))k 
                 = lognΣ(k=1~(n-1))k - Σ(k=1~(n/2-1))k
                 <= logn * n * (n-1) / 2 - (1/2) * (n/2) * (n/2 - 1)
                 <= (1/2) * n^2 * logn - (1/8) * n^2

Σ(k=2~n-1)klogk <= Σ(k=1~n-1)klogk이므로 Σ(k=1~n-1)klogk <= 1/2) * n^2 * logn - (1/8) * n^2임.
</pre>

<pre>
퀵 정렬이 제대로 정렬한다는 것을 귀납적으로 증명

n = 1
원소가 하나이므로 이미 정렬됨

n < k
quicksort가 제대로 정렬한다고 가정

n = k
partition에 의해서 세 부분으로 나뉨
왼쪽은 k보다 작고, 오른쪽도 k보다 작으므로 귀납적 가정에 의해서 제대로 정렬됨.
따라서 n=k일 때도 정렬됨.
</pre>

**:pushpin: 고급 정렬 알고리즘: 힙 정렬**

힙은 이진 트리로서 맨 아래 층을 제외하고는 완전히 채워져 있고, 맨 아래층은 왼쪽부터 꽉 채워져 있음.<br>
힙의 모든 노드는 하나씩의 값을 갖고 있는데, 다음 힙 성질을 만족함.<br>
각 노드의 값은 자기 자식의 값보다 작다(최소힙, 힙에 값이 같은 원소가 두 개 이상 있는 경우에는 작다 대신 작거나 같다)<br>
리프 노드는 자식이 없으므로 논리상 이 성질은 자동 만족됨.<br>
모든 노드가 이 성질을 만족하면, 이진 트리의 루트 노드에는 최솟값이 자리하게 됨.<br>
반대로 최대힙의 루트 노드에는 최댓값이 자리하게 됨.<br>
힙 정렬은 먼저 주어진 배열을 힙으로 만듬.<br>
그런 다음 힙에서 가장 작은 값을 차례로 하나씩 제거하면서 힙의 크기를 줄여나감.<br>
나중에 힙에 아무 원소도 남지 않으면 힙 정렬이 끝남.<br>
정렬은 힙에서 원소들이 제거된 순서대로 함.<br>

**힙 만들기**<br>

일반적으로 A[k]의 자식은 A[2k]와 A[2k+1]이 됨.<br>
A[k]의 부모는 A[k/2]이 됨.<br>
이렇게 부모자식 관계를 배열의 인덱스를 사용해 간단히 계산할 수 있으므로 링크나 포인터가 필요없음.<br>
n개의 원소를 가진 배열 A[1, ..., n]이 주어졌다고 하자.<br>
heapify(A, k, n)은 A[k]에 매달린 두 서브 트리가 힙성질을 만족하는 상태에서 A[k]를 루트로 하는 서브 트리 전체가 힙성질을 만족하도록 수선하는 함수임.<br>
루트의 두 자식 중 작은 값을 x, 큰 값을 y라고 하자.<br>
x를 루트와 비교한다.<br>
루트의 값이 x보다 크지 않으면 힙성질이 만족되어 수선은 끝남.<br>
루트의 값이 x보다 크면 x와 루트의 값을 맞바꿈.<br>
이제 x는 새로운 루트가 되었고, 루트의 값은 한 단계 내려옴.<br>
이렇게 루트가 한 칸 아래로 내려온 노드를 r이라 하자.<br>
여기서 다시 r을 새로운 루트로 삼아 이 작업을 재귀적으로 반복함.<br>
이런 식으로 내려갈 수 있는 곳까지 내려감.<br>
중간에 루트가 자식 중 작은 값보다 크지 않은 경우를 만나면 중단함.<br>

<pre>
buildHeap(A[], n)
{
  for i <- n/2 downto 1
    heapify(A, i, n);
}

heapify(A[], k, n)
{
  left <- 2k; right <- 2k+1;
  
  if (right <= n) then {    // k가 두 자식을 가지는 경우
    if (A[left] < A[right]) 
      then smaller <- left;
    else smaller <- right;            
  }
  
  else if (left <= n) then smaller <- left;
  else return;
  
  if (A[smaller] < A[k]) then {
    A[k] <-> A[smaller];
    heapify(A, smaller, n);
  }
}
</pre>

리프 노드는 그 자체로 힙성질을 만족하므로 buildHeap()은 리프가 아닌 노드 중 맨 뒤에서부터 루트로 삼아 heapify()를 수행함.<br>
n/2은 리프가 아닌 노드 중 맨 마지막 노드의 인덱스임.<br>

buildHeap()에 소요되는 시간은 Θ(n)임.<br>
heapify()는 해당 서브 트리의 높이가 시간을 좌우함.<br>
어떤 서브 트리도 높이가 log2(n)을 넘지 않으므로 heapify를 한번 수행하는데 O(logn)이 소요됨.<br>
그런데 buildHeap()에서 heapify()를 호출하는 횟수는 ⌊n/2⌋이므로 전체적으로 O(nlogn)이 됨.<br>
그러나 이는 과하게 잡은 상한임.<br>
모든 heapify()의 시간을 O(logn)으로 잡은 것이 과함.<br>
맨 처음 호출되는 heapify()의 입력 트리는 높이가 고작 1이고, 이런 것들이 꽤 여러 개 있음.<br>
그 다음 레벨로 올라가면 높이는 증가하지만, 높이가 높은 부분 트리의 수는 줄어들음.<br>
따라서 이를 합산하면 O(nlogn)이 아닌 Θ(n)이 됨.<br>
이에 대한 증명은 아래에 했음.<br>

<pre>
buildHeap()의 수행 시간 계산

heapify()는 힙의 높이에 비례하는 시간이 소요됨.
즉, 힙의 높이가 h라면 O(h) 시간이 소요됨.
원소의 수가 총 n개인 힙의 높이는 ⌊log2(n)⌋임.
높이가 h인 트리의 노드 수는 기껏해야 ⌈n/2^(h+1)⌉임.
따라서 buildHeap()의 수행 시간은 다음과 같음.

Σ(h=0~⌊log2(n)⌋)⌈n/2^(h+1)⌉O(h) = O(nΣ(h=0~⌊log2(n)⌋)h/2^h)=O(n)임.

O(logn)이 될 것이라는 직관과는 달리 O(n)이 됨.
Σ(h=0~⌊log2(n)⌋)h/2^h이 2보다 크지 않기 때문임.

Σ(h=0~∞)x^h = 1/(1-x)
이를 양변에 미분하면 다음과 같음.
Σ(h=0~∞)h*x^(h-1) = 1/(1-x)^2
양변에 x를 곱하면 다음과 같음
Σ(h=0~∞)h*x^h = x/(1-x)^2
h/2^h는 h*x^h에서 x가 1/2인 경우와 같음.
Σ(h=0~⌊log2(n)⌋)h/2^h <= Σ(h=0~∞))h/2^h
                      = (1/2) / (1/2)^2
                      = 2
따라서 2가 Σ(h=0~⌊log2(n)⌋)h/2^h의 상한임.
</pre>

**:pushpin: 특수 정렬 알고리즘: 기수 정렬**

**:pushpin: 특수 정렬 알고리즘: 계수 정렬**
