<h1>:green_book: <쉽게 배우는 알고리즘> 정리</h1>

<a href="#1">:pencil2: Chapter1. 알고리즘 설계와 분석의 기초</a>
- O 표기법
- Ω 표기법
- Θ 표기법
- o 표기법
- ω 표기법

<a href="#2">:pencil2: Chapter2. 점화식과 알고리즘 복잡도 분석</a>
- 반복 대치
- 추정 후 증명
- 마스터 정리

<a href="#3">:pencil2: Chapter3. 정렬</a>
- 기본적인 정렬 알고리즘: 선택 정렬
- 기본적인 정렬 알고리즘: 버블 정렬
- 기본적인 정렬 알고리즘: 삽입 정렬
- 고급 정렬 알고리즘: 병합 정렬
- 고급 정렬 알고리즘: 퀵 정렬
- 고급 정렬 알고리즘: 힙 정렬
- 특수 정렬 알고리즘: 기수 정렬
- 특수 정렬 알고리즘: 계수 정렬
  
<h2><a id="4">:pencil2: Chapter4. 알고리즘 설계와 분석의 기초</a></h2>
- 평균 선형 시간 선택 알고리즘
- 최악의 경우에도 선형 시간을 보장하는 선택 알고리즘

<h2><a id="1">:pencil2: Chapter1. 알고리즘 설계와 분석의 기초</a></h2>

**:pushpin: O 표기법**

O(g(n)) = {f(n) | ∃c > 0, n0 > 0 s.t. ∀n >= n0, f(n) <= cg(n)}<br>
O(g(n)) = {f(n) | 모든 n > n0에 대하여 f(n) <= cg(n)인 양의 상수 c와 n0가 존재한다}<br>
O(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 f(n) <= cg(n)인 양의 상수 c가 존재한다}<br>
<br>
O(g(n))은 충분히 큰 n에 대하여 g(n)에 상수만 곱하면 g(n)이 누를 수 있는 모든 함수의 집합임.<br>
n^2에 3보다 크거나 같은 상수를 곱하면 3n^2을 누를 수 있으므로 3n^2은 O(n^2)에 속함.<br>
<br>
5n^2 = O(n^2)임을 보여라.<br>
c를 6, n0을 1로 잡으면 모든 n >= n0(=1)에 대하여 5n^2 <= 6n^2임. 즉 정의를 만족하는 c와 n0이 존재함.<br>

**:pushpin: Ω 표기법**
  
  Ω(g(n)) = {f(n) | ∃c > 0, n0 > 0 s.t. ∀n >= n0, cg(n) <= f(n)}<br>
  Ω(g(n)) = {f(n) | 모든 n > n0에 대하여 cg(n) <= f(n)인 양의 상수 c와 n0가 존재한다}<br>
  Ω(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 cg(n) <= f(n)인 양의 상수 c가 존재한다}<br>
  <br>
  Ω(g(n))은 충분히 큰 n에 대하여 g(n)에 상수만 곱하면 g(n)이 압도당할 수 있는 모든 함수의 집함임.<br>
  <br>
  5n^2 = Ω(n^2)임을 보여라.<br>
  c를 4로 잡고, n0 = 1로 잡으면 모든 n >= n0(=1)에 대하여 4n^2 <= 5n^2임. 즉 정의를 만족하는 c와 n0이 존재함.<br>
  
**:pushpin: Θ 표기법**
 
  Θ(g(n)) = O(g(n)) ∩ Ω(g(n))<br>
  Θ(g(n)) = {f(n) | ∃c1, c2 > 0, n0 > 0 s.t. ∀n >= n0, c1g(n) <= f(n) <= c2g(n)}<br>
  Θ(g(n)) = {f(n) | 충분히 큰 모든 n에 대하여 c1g(n) <= f(n) <= c2g(n)인 양의 상수 c1, c2가 존재한다.}<br>
  <br>
  5n^2 = Θ(n^2)임을 보여라.<br>
  5n^2 = O(n^2), 5n^2 = Ω(n^2)임을 보였기 때문에 5n^2 = Θ(n^2)이다.<br>

**:pushpin: o 표기법**
  
  함수의 증가율이 점근적 의미에서 어느 한계보다 더 작다는 것을 표현하고자 할 때 사용됨.<br>
  예를 들어 함수 5n = o(n^2)이다. 5n의 증가율은 n^2의 증가율보다 작기 때문이다.<br>
  그렇지만 함수 (1/2) * n^2은 o(n^2)에 속하지 않음. (1/2) * n^2의 증가 속도가 n^2의 증가 속도와 점근적으로 동일하기 때문임.<br>
  <br>
  o(g(n)) = {f(n) | lim f(n) / g(n) = 0 }<br>
  o(g(n)) = {f(n) | ∃n0 > 0 s.t. ∀c>0 and n>=n0, f(n) < cg(n)}<br>
  o(g(n)) = {f(n) | n이 충분히 크면 모든 c>0에 대하여 f(n) < cg(n)이다.}<br>
  o(g(n))은 충분히 큰 n에 대하여 g(n)에 아무리 작은 상수를 곱해도 g(n)이 압도하는 모든 함수의 집합임.<br>
  <br>
  5n^2 = o(n^3)임을 보여라.<br>
  lim (n^2 -5) / n^3 = 0임.<br>
  
**:pushpin: ω 표기법**
  
  함수의 증가율이 점근적 의미에서 어느 한계보다 더 크다는 것을 표현하고자 할 때 사용함.<br>
  예를 들어 5n^3 = ω(n^2)임. 5n^3의 증가 속도는 n^2의 증가 속도보다 큼.<br>
  함수 2n^2은 ω(n^2)에 속하지 않음. 2n^2의 증가 속도는 n^2의 증가 속도와 점근적으로 동일하기 때문임.<br>
  ω표기는 함수의 기울기상의 여유 있는 하한을 나타냄.<br>
  <br>
  ω(g(n)) = {f(n) | lim f(n) / g(n) = ∞}<br>
  ω(g(n)) = {f(n) | ∃n0 > 0 s.t. ∀c>0 and n>=n0, cg(n) < f(n)}<br>
  ω(g(n)) = {f(n) | n이 충분히 크면 모든 c > 0에 대하여 cg(n) < f(n)이다.}<br>
  <br>
  n^3 / 4 = ω(n^2)임을 보여라.<br>
  lim ( (n^3 /4) / n^2 ) = ∞이므로 n^3 / 4 = ω(n^2)이다.<br>

<h2><a id="2">:pencil2: Chapter2. 점화식과 알고리즘 복잡도 분석</a></h2>


점화식은 어떤 함수를 자신과 똑같은 함수를 이용해 나타내는 것임.<br>
n!의 점화식은 f(n) = n * f(n-1), 피보나치 수열의 점화식은 f(n) = f(n-1) + f(n-2)임.<br>

**:pushpin: 반복 대치**

<pre>
factorial(n)
{
  1. if (n = 1) return 1;
  2. return n * factorial(n-1);
}
</pre>

이 알고리즘으로 n!을 구하는데 걸리는 시간을 T(n)이라고 하면, T(n) = T(n-1) + c임.<br>
c는 자기호출을 제외한 나머지의 수행 시간으로 1을 수행하는 시간과 2의 곱셈을 한번 수행하는 시간임.<br>
크기가 1이면 T(1) <= c임.<br>

<pre>
T(n) = T(n-1) + c
     = T(n-2) + 2c
     ...
     = T(1) + (n-1)c
     <= cn
</pre>

T(n) <= cn이므로 T(n) = O(n)이다.<br>

**병합 정렬**<br>

입력의 크기가 n인 배열을 정렬하는 병합 정렬은 배열을 이등분한 다음, 각각을 재귀적으로 병합 정렬해 이 둘을 병합함으로써 정렬을 끝냄.<br>
입력의 크기가 n인 배열에 대한 병합 정렬에서 대소 비교의 총 횟수를 T(n)이라고 하자.<br>
n = 2^k이라고 가정해도 일반성을 잃지 않으므로 n = 2^k으로 가정하겠음.<br>
T(n) <= 2T(n/2) + n임.<br>
n은 merge(A, p, q, r)에 필요한 최대 비교 횟수 n-1과 if (p<r) then에 있는 비교 한 번을 합한 것임.<br>

<pre>
T(n) <= 2T(n/2) + n
     <= 2^2T(n/2^2) + 2n
     ...
     <= 2^kT(n/2^k) + kn
     = nT(1) + nlogn
     = n + nlogn
      = O(nlogn)
</pre>

T(n) <= 2T(n/2) + n을 T(n) <= 2T(n/2) + O(n)으로 표시하기도 함.<br>
여기서 O(n)은 집합으로서 O(n)을 의미하지 않고, O(n)에 속하는 함수 하나를 대신하는 관행적 표현임.<br>

**n = 2^k**<br>

점근적 복잡도의 계산을 용이하게 하기 위해 n = 2^k이라는 가정을 종종 사용함.<br>
어떠한 n이라도 n과 2n 사이에 2의 멱수가 하나 있음.<br>
즉, n <= 2^k <= 2n인 2^k가 하나 존재함.<br>
임의의 상수 r에 대해 T(n) = O(n^r)이라면 T(2n) = O(2^r * n^r) = O(n^r)이므로 T(n) = T(2n)임.<br>
T(n) <= T(2^k) <= T(2n)이고 T(n) = T(2n) 이므로 T(n) = T(2^k) = T(2n)임.<br>
즉, n의 오른쪽에서 처음으로 마나는 2의 멱수에 대한 함수도 항상 n에 대한 함수와 같은 점근적 복잡도를 가지므로 n = 2^k로 가정해도 점근적 분석의 결과는 같음.<br>

**:pushpin: 추정 후 증명**

추정 후 증명은 식의 모양을 보고 점근적 복잡도를 추정한 다음 그것이 옳음을 귀납적으로 증명하는 방법임.<br>

<pre>
T(n) <= 2T(n/2) + n의 점근적 복잡도는 T(n) = O(nlogn)이다. 즉 충분히 큰 n에 대하여 T(n) <= cnlogn인 양의 상수 c가 존재한다.

경계조건: T(2) <= c2log2를 만족하는 c가 존재한다.

귀납적 가정과 전개: n/2에 대해 T(n/2) <= c(n/2)log(n/2)을 만족한다고 가정하면,
T(n) <= 2T(n/2) + n
     <= 2c(n/2)log(n/2) + n
     = cnlogn - cnlog2 +n
     <= cnlogn
     
이를 만족하는 상수 c가 존재한다. 따라서 T(n) = O(nlogn)이다.
</pre>

여기서 log1 = 0이므로 T(1) <= 1log1은 불가능하기 때문에 T(2)를 경계조건으로 잡은 것임.<br>
경계조건으로 T(2)가 아닌 훨씬 큰 상수 a에 대해 T(a) <= caloga임을 보여도 상관없음.<br>
일반적으로  T(n) = f(n)임을 보이기 위해 상수 a를 경계치로 잡으면 T(a)도 상수가 되므로 f(n)이 양수인 한 T(a) <= cf(a)를 만족시키는 c가 항상 존재함.<br>
그러므로 경계조건을 만족시키는 경계치는 항상 잡을 수 있기 때문에 경계조건을 확인하지 않아도 됨.<br>

<pre>
T(n) <= 2T(n/2 + 10) + n의 점근적 복잡도는 T(n) = O(nlogn)이다. 즉, 충분히 큰 n에 대하여 T(n) <= cnlogn인 양의 상수 c가 존재한다.

증명

T(n) <= 2T(n/2 + 10) + n
     <= 2c(n/2 + 10)log(n/2 + 10) + n
     = cnlog(n/2 + 10) + 20clog(n/2 + 10) + n
     <= cnlog(3n / 4) + 20clog(3n / 4) + n
     = cnlogn + cn(log3 - log4) + 20clog(3n / 4) + n
     <= cnlogn
     
이를 만족하는 상수 c가 존재한다. 따라서 T(n) = O(nlogn)이다.
이 때, n에 대한 제약이 있다.
n/2 + 10 < n이어야 하므로 n > 20이어야 논리적인 전개가 가능하다.
식을 간단하게 하기 위해서 n/2 + 10 <= 3n /4임을 가정했는데 이는 n >= 40이어야 가능하다.
</pre>

cn(log3 - log4) + 20clog(3n / 4) + n이 음수가 되어야 하는데, c(log3 - log4) + 1을 충분히 작은 음수로 만들 수 있으면 20clog(3n/4)이 n에 대한 차수가 낮아 압도할 수 있음.<br>

**주의**<br>
<pre>
T(n) <= 2T(n/2) + n의 점근적 복잡도가 T(n) = O(n)이라고 가정하고, 충분히 큰 n에 대하여 T(n) <= cn인 양의 상수 c가 존재한다는 것을 증명하려 할 때
T(n) <= 2T(n/2) + n
     <= 2c(n/2) + n
     = cn + n
     = c'n
     = O(n)
</pre>
c' = c + 1이라는 추가적은 상수를 만들어서 c'n <= cn이라고 주장하면 안 됨.<br>
처음에 쓴 c와 나중에 쓰는 c는 같은 상수여야 함.<br>

<pre>
T(n) <= 2T(n/2) + n
     <= 2c(n/2) + n
     = cn + n
     = O(n)
</pre>
n(c+1) = O(n)인건 맞지만 T(n) <= cn + n <= cn임을 증명하지 못 했음.<br>
따라서 귀납적 증명이 완결되지 않았으므로 틀렸음.<br>

<pre>
T(n) = 2T(n/2) + 1의 점근적 복잡도는 O(n)이다.

실패하는 증명

충분히 큰 n에 대해 T(n) <= cn인 양의 상수 c가 존재한다는 것을 증명하려 한다.
T(n) = 2T(n/2)  + 1
     <= 2c(n/2) + 1
     <= cn + 1
여기서 cn + 1 < cn임을 입증할 수 없음.

성공하는 증명

T(n) <= cn임을 증명하는 대신 T(n) <= cn - 2임을 증명할 수 있어도 T(n) = O(n)임.<br>
T(n) = 2T(n/2) + 1
     <= 2(cn/2 - 2) + 1
     = cn -3
     <= cn
</pre>

추정 후 증명법이 유용하게 사용되려면 우선 추정을 의미있게 해야 함.<br>
너무 여유롭게 추정해 증명하는 것은 별로 의미가 없음.<br>

**추가: 경계조건**<br>

재귀적으로 정의된 함수나 수열은 자신의 값을 계산하기 위해 자신과 같은 함수나 수열을 더 작은 입력값으로 호출하는 경우가 많음.<br>
이러한 호출은 일반적으로 입력값이 충분히 작아지거나 특정 조건을 만족할 때까지 반복됨.<br>
이 때, 함수나 수열이 호출되는 과정에서 더 이상 호출하지 않고 종료될 때, 이를 '경계조건' 혹은 '종료조건'이라고 부름.<br>
예를 들어, 피보나치 수열은 아래와 같은 점화식으로 정의됨.<br>
<pre>
F(0) = 0
F(1) = 1
F(n) = F(n-1) + F(n-2) (n >= 2)
</pre>
이 때, F(0)와 F(1)은 경계조건으로 사용됨.<br>
F(0)와 F(1)을 미리 정의함으로써, F(n)을 계산하기 위한 종료 조건으로 사용할 수 있음.<br>

**:pushpin: 마스터 정리**

<pre>
T(n) = aT(n/b) + f(n)
</pre>

마스터 정리는 특정한 모양을 가진 재귀식에서 바로 결과를 알 수 있는 유용한 정리임.<br>
입력의 크기가 n인 문제를 풀기 위해 입력의 크기가 n/b인 문제를 a개 풀고, 나머지 f(n)의 오버헤드가 필요한 알고리즘들이 해당됨.<br>
a >= 1, b >= 1에 대해 T(n) = aT(n/b) + f(n)인 점화식에서, h(n) = n^(logb(a))라고 할 때 T(n)의 점근적 복잡도는 다음과 같음.<br>
<pre>
어떤 양의 상수 ε에 대하여 f(n) / h(n) = O(1/n^ε)이면, T(n) = θ(h(n))이다.
어떤 양의 상수 ε에 대하여 f(n) / h(n) = Ω(n^ε)이고, 어떤 상수 c(<1)와 충분히 큰 모든 n에 대해 af(n/b) <= cf(n)이면 T(n) = θ(f(n))이다. 
f(n) / h(n) = θ(1)이면 T(n) = θ(h(n)logn)이다.
</pre>

마스터 정리의 근사 버전은 아래와 같음.
<pre>
lim f(n) / h(n) = 0이면 T(n) = θ(h(n))이다.
lim f(n) / h(n) = ∞이고, 충분히 큰 모든 n에 대해 af(n/b) < f(n)이면 T(n) = θ(f(n))이다.
f(n) / h(n) = θ(1)이면 T(n) = θ(h(n)logn)이다.
</pre>

마스터 정리의 근사 버전은 원형과 정확히 같지는 않음.<br>
lim f(n) / h(n) = ∞은 h(n)이 f(n)을 압도한다는 뜻이고, f(n) / h(n) = O(1/n^ε)은 h(n)이 f(n)을 적어도 다항식의 비율로 압도한다는 뜻임.<br>
lim f(n) / h(n) = lim 1 / logn의 경우에는 lim f(n) / h(n) = 0이지만 logn의 비율로 압도할 뿐 다항식의 비율로 압도하지는 않음.<br>
즉. f(n) / h(n) = O(1/n^ε)은 성립하지 않음.<br>
lim f(n) / h(n) = ∞와 f(n) / h(n) = Ω(n^ε) 사이에도 다항식 비율에 관한 차이가 있음.<br>
충분히 큰 모든 n에 대해 af(n/b) < f(n)과 어떤 상수 c(<1)와 충분히 큰 모든 n에 대해 af(n/b) <= cf(n)도 고정 비율 c이하의 보장이라는 미묘한 차이가 있지만 반례를 찾기가 극히 힘들기 때문에 사실상 이 둘을 구분 없이 사용해도 무방함.<br>

<pre>
T(n) = 2T(n/3) + c (c는 상수)

a = 2, b =  3, f(n) = c, h(n) = n^log3(2)
lim f(n) / h(n) = 0이므로 T(n) = θ(n^log3(2))이다.
</pre>

<pre>
T(n) = 2T(n/4) + n

a = 2, b = 4, f(n) = n, h(n) = n^log4(2)
lim f(n) / h(n) = ∞이고 af(n/b) = 2(n/4) = n/2 < n임.
또한 n/2 <= (1/2)f(n)이므로 c = 1/2에 대해 af(n/b) <= cf(n)을 만족함.
따라서 T(n) = θ(nlogn)임.
</pre>

<pre>
T(n) = 2T(n/2) + n

a = 2, b = 2, f(n) = n, h(n) = n
f(n) / h(n) = 1이므로 T(n) = θ(nlogn)임.
</pre>

마스터 정리를 쓸 수 없는 모양인 점화식의 변수를 치환함으로써 마스터 정리를 쓸 수 있는 모양으로 변형하는 방법도 있음.<br>

<pre>
T(n) = 2T(n^(1/2)) + logn

m = log2(n)으로 놓으면

T(2^m) = 2T(2^(m/2)) + m

이는 아래와 같이 다시 표현할 수 있음.

S(m) = 2S(m/2) + m

a = 2, b = 2, f(m) = m, h(m) = m이므로
S(m) = θ(mlogm) = θ(logn loglogn)임.
</pre>

마스터 정리에서 f(n)은 크기가 n인 문제(최상위 레벨)에서 발생하는 자기 호출 이외의 오버헤드로 크기가 다른 문제들 간의 관계를 반영하는 비용임.<br>
h(n)은 반복적인 자기호출 끝에 마지막으로 크기 1인 문제를 만나는 횟수임.<br>
자기호출 때문에 부담이 더 커지면 수행 시간은 h(n)이 결정하고, 관계를 반영하는 오버헤드가 더 커지면 f(n)이 결정함.<br>
단 f(n)은 상위 레벨의 오버헤드만 의미하므로 하위 레벨에서 발생하는 오버헤드는 반영하고 있지 않음.<br>
af(n/b) <= cf(n)은 자기호출로 만나는 하위레벨의 문제들에서 발생하는 자기호출 이외의 오버헤드들의 총합이 레벨이 내려가면서 적어도 감소해야 한다는 것을 의미함.<br>

<h2><a id="3">:pencil2: Chapter3. 정렬</a></h2>

**:pushpin: 기본적인 정렬 알고리즘: 선택 정렬**

우선 배열 A[1, ..., n]에서 가장 큰 원소를 찾아 이 원소와 배열의 끝자리에 있는 A[n]과 자리를 바꿈.<br>
그러면 방금 맨 뒷자리로 옮긴 원소, 즉 가장 큰 원소는 자기 자리를 찾았으므로 더 이상 신경 쓰지 않아도 됨.<br>
이 원소는 정렬이 끝났다고 볼 수 있으므로 이제 이 원소를 제외한 나머지 원소들로 같은 작업을 반복함.<br>

<pre>
간략한 기술

selectionSort(A[], n)
{
  for last <- n downto 2 {
    A[1, ..., last]중 가장 큰 수 A[k]를 찾는다;
    A[k] <-> A[last];
  }
}
</pre>

<pre>
기호적 기술
selectionSort(A[], n)
{
  for last <- n downto 2 {
    k <- theLargest(A, last);
    A[k] <-> A[last];
  }
}

theLargest(A[], last)
{
  largeset <- 1;
  for i <- 2 to last
    if (A[i] > A[largest]) then largest <- i;
  return largest;
}
</pre>

이 알고리즘에서 입력은 배열 A[1, ..., n]임.<br>
변수 last는 정렬할 배열의 맨 마지막 인덱스, 즉 배열의 크기를 나타냄.<br>
처음에는 배열의 크기가 n으로 시작하므로, A[1, ..., n]을 정렬 대상으로 삼음.<br>
가장 큰 수를 찾아 제자리에 놓을 때마다 last는 1씩 줄어들음.<br>
선택 정렬의 수행 시간은 모든 경우에 Θ(n^2)임.<br>
배열의 크기가 n일 때 n-1번 비교, ... , 배열의 크기가 2일 때 1번 비교함.<br>
따라서 1 + ... + n-1 = n(n-1) / 2임.<br>
수를 비교하는 횟수가 전체 시간을 좌우하므로 이것을 기준으로 수행 시간을 계산함.<br>

**:pushpin: 기본적인 정렬 알고리즘: 버블 정렬**

<pre>
간략한 기술

bubbleSort(A[], n)
{
  for last <- n downto 2
    for i <- 1 to last-1
      if (A[i] > A[i+1]) then A[i] <-> A[i+1];
}
</pre>

버블 정렬의 총 순환 횟수는 (n-1) + (n-2) + ... + 2 + 1 = n(n-1) / 2임.<br>
따라서 수행 시간은 Θ(n^2)임.<br>
버블 정렬 알고리즘은 중간에 배열이 이미 정렬이 되어 있는 상태라도 계속 끝까지 무의미한 순환을 계속함.

<pre>
수정된 bubble sort

bubbleSort(A[], n)
{
  for last <- n downto 2
      sorted <- TRUE;
    for i <- 1 to last-1 {
      if (A[i] > A[i+1]) {
        A[i] <-> A[i+1];
        sorted <- FALSE;
      }
    }
    if (sorted = TRUE) then return;
}
</pre>

**:pushpin: 기본적인 정렬 알고리즘: 삽입 정렬**

삽입 정렬은 이미 정렬되어 있는 i개짜리 배열에 하나의 원소를 더 더하여 정렬된 i+1개짜리 배열을 만드는 과정을 반복함.<br>
한 개짜리 배열에서 시작하여 그 크기를 하나씩 늘리는 정렬임.<br>

<pre>
insertionSort(A[], n)
{
  for i <- 2 to n
    A[1, ..., i]의 적합한 자리에 A[i]를 삽입한다.
}
</pre>

for 루프는 문제의 크기를 하나씩 키워나가는 역할을 함.<br>
A[i]에 관심을 두는 시점에는 A[1, ..., i-1]은 항상 정렬이 되어 있음.<br>
A[i]가 A[i-1]보다 크면 앞에 있는 모든 원소보다 크므로 A[i]는 그냥 제자리에 두면 됨.<br>
그렇지 않으면 A[i-1]부터 시작해서 왼쪽으로 차례로 훑으면서 A[i]가 들어갈 자리를 찾음.<br>
A[i]가 들어가는 자리부터 시작해서 이후의 원소들은 한 칸씩 오른쪽으로 밀려남.<br>

<pre>
insertionSort(A[], n)
{
  for i <- 2 to n {
    loc <- i - 1;
    newItem <- A[i]
    
    while (loc >= 1 and newItem < A[loc]) {
      A[loc + 1] = A[loc];
      loc--;
    }
    A[loc + 1] <- newItem;
  }
}
</pre>

for 루프는 n-1번 순환함.<br>
매 for 루프에서 while은 최대 i-1번 순환함.<br>
가장 운이 좋으면 while 문은 돌아가지 않음.<br>
최악의 경우 수행 시간은 (n-1) + (n-2) + ... + 2 + 1 = n(n-1)/2임.<br>
따라서 Θ(n^2)임.<br>
보통은 대략 A[1, ..., i-1]에서 평균적으로 절반 정도를 훑고 끝낼 것임.<br>
그러므로 전체 비교 횟수는 최악의 경우에 비해 절반 정도 될 것임.<br>
그래도 시간복잡도는 Θ(n^2)임.<br>
<br>
삽입 정렬은 거의 정렬되어 있는 상태로 입력되는 경우에는 가장 매력적인 알고리즘임.<br>
배열이 완전히 정렬된 채로 입력되면 while 루프는 한 번도 수행되지 않고, for 루프는 한번 순환할 때마다 상수 시간이 소요됨.<br>
for 루프는 n-1번 순환되므로 Θ(n)에 가까운 시간이 듬.<br>
배열이 거의 정렬되어 있을 때도 삽입이 매우 수월해져 Θ(n)에 가까운 시간이 듬.<br>
버블 정렬은 배열이 이미 정렬되어 있는 경우에 무의미한 순환을 줄이기 위해 방법이 있긴 했지만 오버헤드가 생김.<br>
삽입 정렬은 별도 장치가 없어도 효율적으로 끝남.<br>
따라서 상황에 따라 가끔 삽입 정렬을 섞어서 씀.<br>
<br>
선택 정렬과 버블 정렬이 n개짜리 배열에서 시작하여 아직 정렬되지 않은 배열의 크기를 하나씩 줄이는 데 반하여, 삽입 정렬은 1개짜리 배열에서 시작하여 이미 정렬된 배열의 크기를 하나씩 늘리는 정렬임.<br>
삽입 정렬에는 수학적 귀납법의 원리가 들어가 있음.<br>
배열의 크기가 1일 때는 성립함.(이미 정렬되어 있으므로)<br>
배열의 크기가 k일 때 성립하면(정렬되어 있으면), 적절한 삽입으로 크기가 k+1일 때도 성립함(정렬됨).<br>
이것으로 삽입 정렬은 올바르게 정렬을 한다는 것이 귀납적으로 증명됨.<br>

**:pushpin: 고급 정렬 알고리즘: 병합 정렬**

병합 정렬은 먼저 입력을 반으로 나눔.<br>
이렇게 나눈 전반부와 후반부를 각각 독립적으로 정렬함.<br>
마지막으로 정렬된 두부분을 합쳐서, 즉 병합하여 정렬된 배열을 얻음.<br>
여기서 전반부, 후반부를 각각 정렬할 때도 역시 반으로 나눈 다음 정렬해서 병합함.<br>
즉, 원래의 정렬 문제와 성격이 똑같고 단지 크기만 반으로 줄였을 뿐임.<br>
병합 정렬은 자신에 비해 크기가 반인 문제를 두 개 푼 다음, 이들을 병합하는 일을 재귀적으로 반복함.<br>

<pre>
mergeSort(A[], p, r)
{
  if (p<r) then {
    q <- (p+r) / 2;
    mergeSort(A, p, q);
    mergeSort(A, q+1, r);
    merge(A, p, q, r);
  }
}

merge(A[], p, q, r)
{
  정렬되어 있는 두 배열 A[p, ..., q]와 A[q+1, ..., r]을 합쳐 정렬된 하나의 배열 A[p,...,r]을 만듬.
}
</pre>

<pre>
merge(A[], p, q, r)
{
  i <- p; j <- q + 1; t <- 1;
  while (i <= q and j <= r) {
    if (A[i] <= A[j])
    then tmp[t++] = A[i++];
    else tmp[t++] = A[j++];
  }
  
  while (i <= q)       //왼쪽 배열이 남은 경우
    tmp[t++] = A[i++];
  while (j <= r)       //오른쪽 배열이 남은 경우
    tmp[t++] = A[j++];
  i <- p; t <- 1;
  while (i <= r)
    A[i++] <- tmp[t++];
}
</pre>

T(n) <= a (if n = 1)<br>
T(n) <= 2T(n/2) + cn (if n > 1)<br>
부등호를 쓰는 이유는 좌변과 우변이 정확히 일치하지 않을 수 있기 때문임.<br>
따라서 근사적으로 나타내는 표현의 하나라고 생각하면 됨.<br>
상수 a는 크기가 1인 문제를 푸는 시간을 나타냄.<br>
상수 c는 병합에 드는 시간을 충분히 잡아주기 위해서 n에 곱한 것임.<br>
비교의 횟수만으로 수행 시간을 분석한다면 c=1로 충분함.<br>
어쨋든 병합은 선형 시간이 소요됨.<br>
n=2^k라고 가정하고 전개하면 다음과 같음.<br>

<pre>
T(n) <= 2T(n/2) + cn
     <= 2^2T(n/2^2) + 2cn
     ...
     <= 2^kT(n/2^k) + kcn
     = an + cn * logn
     = Θ(nlogn)
</pre>

병합 정렬의 수행 시간은 최악의 경우 Θ(nlogn)임.

**:pushpin: 고급 정렬 알고리즘: 퀵 정렬**

퀵 정렬은 평균적으로 가장 좋은 성능을 가져 현장에서 가장 많이 쓰는 정렬 알고리즘임.<br>
우선 정렬할 배열에서 기준원소를 하나 고름.<br>
아무 원소나 임의로 골라도 되나 여기서는 맨 뒤의 원소를 기준원소로 삼음.<br>
이 기준원소를 중심으로 더 작거나 같은 수는 왼쪽으로, 큰 수는 오른쪽으로 재배치함.<br>
기준원소는 이렇게 분할된 양쪽 부분 배열 사이에 자리하게 됨.<br>
이렇게 분할된 왼쪽 부분 배열을 따로 정렬함.<br>
마찬가지로 오른쪽 부분 배열도 따로 정렬함.<br>
기준원소는 손대지 말고 제자리에 그대로 둠.<br>
왼쪽과 오른쪽 부분 배열을 정렬할 때 퀵 정렬을 재귀적으로 사용함.<br>

<pre>
quickSort(A[], p, r)
{
  if (p<r) then {
    q <- partition(A, p, r);
    quickSort(A, p, q-1);
    quickSort(A, q+1, r);
  }
}

partition(A[], p, r)
{
  배열 A[p, ..., r]의 원소들을 A[r]을 기준으로 양쪽으로 재배치하고 A[r]이 자리한 위치를 리턴함.
}
</pre>

병합 정렬은 먼저 재귀적으로 작은 문제를 해결한 다음 후 처리를 하는데 반해서, 퀵 정렬은 선행 작업을 한 다음 재귀적으로 작은 문제를 해결하면서 바로 끝냄.<br>

<pre>
partition(A[], p, r)
{
  x <- A[r];
  i <- p -1;
  for j <- p to r-1
    if (A[j] <= x) then A[++i] <-> A[j];
  A[i+1] <-> A[r]
  return i+1;
}
</pre>

퀵 정렬의 수행 시간을 분석해보자.<br>
우선 분할은 배열을 왼쪽부터 끝까지 한 번 훑어나가는 작업이므로 Θ(n)의 시간이 듬.<br>
퀵 정렬의 수행에서 가장 이상적인 경우는 분할이 항상 반반씩 균등하게 될 때임.<br>
이 때는 T(n) = 2T(n/2) + Θ(n)이므로 병합 정렬과 같은 모양임.<br>
따라서 Θ(nlogn)이 됨.<br>
최악의 경우는 계속해서 한쪽은 하나도 없고, 다른 쪽에 다 몰리도록 분할이 되는 경우임.<br>
T(n) = T(n-1) + Θ(n)이므로 Θ(n^2)이 됨.<br>
한 쪽이 완전히 비거나 이에 근접한 상태가 반복되면 이런 비효율적인 시간이 나옴.<br>
퀵 정렬의 수행 시간은 분할이 얼마나 균형잡히게 잘 되느냐에 달려 있음.<br>
평균 수행 시간은 분할했을 때 모든 가능한 경우를 평균내면 됨.<br>
기준 원소가 1등이면 1구역과 2구역의 크기가 0:n-1, 2등이면 1:n-2, ..., i등이면 i-1:n-i임.<br>
T(n) = T(i-1) + T(n-i) + Θ(n)임.<br>
기준원소는 동일한 확률로 1등부터 n등 중의 하나가 되므로 이들을 평균하면 다음과 같음.<br>
T(n) = (1/n) * Σ(i=1~n)[T(i-1) + T(n-i)] + Θ(n) = (2/n) * Σ(k=0~n-1)T(k) + Θ(n)임.<br>
이를 계산하면 T(n) = Θ(nlogn)임.<br>

<pre>
위 내용을 증명하겠음.

우선 T(2) <= c2log2를 만족하도록 충분히 큰 c를 잡을 수 있음.
2 <= k <= n인 모든 k에 대해 T(k) <= cklogk가 성립한다 가정하고 T(n) <= cnlogn이 됨을 증명하면 됨.

T(n) = (1/n) * Σ(i=1~n)[T(i-1) + T(n-i)] + Θ(n)
     = (2/n) * Σ(k=0~n-1)T(k) + Θ(n)
     = (2/n) * Σ(k=2~n-1)T(K) + Θ(n)  // k=0, k=1일 때는 Θ(n)에 흡수됨.
     <= (2/n) * Σ(k=2~n-1)cklogk + Θ(n)
     <= (2c/n) * [(1/2) * n^2 * logn - (1/8) * n^2] + Θ(n) // 이 부분 아래에 증명하겠음.
     = cnlogn - cn/4 + Θ(n)
     <= cnlogn
     
Σ(k=1~n-1)klogk = Σ(k=1~(n/2-1))klogk + Σ(k=(n/2)~n-1)klogk
왼쪽 항의 logk는 log(n/2)을 상한으로 잡을 수 있고, 오른쪽 항의 logk는 logn을 상한으로 잡을 수 있음.
Σ(k=1~n-1)klogk <= log(n/2)Σ(k=1~(n/2-1))k + lognΣ(k=(n/2)~n-1)k 
                 = lognΣ(k=1~(n/2-1))k + lognΣ(k=(n/2)~n-1)k - Σ(k=1~(n/2-1))k 
                 = lognΣ(k=1~(n-1))k - Σ(k=1~(n/2-1))k
                 <= logn * n * (n-1) / 2 - (1/2) * (n/2) * (n/2 - 1)
                 <= (1/2) * n^2 * logn - (1/8) * n^2

Σ(k=2~n-1)klogk <= Σ(k=1~n-1)klogk이므로 Σ(k=1~n-1)klogk <= 1/2) * n^2 * logn - (1/8) * n^2임.
</pre>

<pre>
퀵 정렬이 제대로 정렬한다는 것을 귀납적으로 증명

n = 1
원소가 하나이므로 이미 정렬됨

n < k
quicksort가 제대로 정렬한다고 가정

n = k
partition에 의해서 세 부분으로 나뉨
왼쪽은 k보다 작고, 오른쪽도 k보다 작으므로 귀납적 가정에 의해서 제대로 정렬됨.
따라서 n=k일 때도 정렬됨.
</pre>

**:pushpin: 고급 정렬 알고리즘: 힙 정렬**

힙은 이진 트리로서 맨 아래 층을 제외하고는 완전히 채워져 있고, 맨 아래층은 왼쪽부터 꽉 채워져 있음.<br>
힙의 모든 노드는 하나씩의 값을 갖고 있는데, 다음 힙 성질을 만족함.<br>
각 노드의 값은 자기 자식의 값보다 작다(최소힙, 힙에 값이 같은 원소가 두 개 이상 있는 경우에는 작다 대신 작거나 같다)<br>
리프 노드는 자식이 없으므로 논리상 이 성질은 자동 만족됨.<br>
모든 노드가 이 성질을 만족하면, 이진 트리의 루트 노드에는 최솟값이 자리하게 됨.<br>
반대로 최대힙의 루트 노드에는 최댓값이 자리하게 됨.<br>
힙 정렬은 먼저 주어진 배열을 힙으로 만듬.<br>
그런 다음 힙에서 가장 작은 값을 차례로 하나씩 제거하면서 힙의 크기를 줄여나감.<br>
나중에 힙에 아무 원소도 남지 않으면 힙 정렬이 끝남.<br>
정렬은 힙에서 원소들이 제거된 순서대로 함.<br>

**힙 만들기**<br>

일반적으로 A[k]의 자식은 A[2k]와 A[2k+1]이 됨.<br>
A[k]의 부모는 A[k/2]이 됨.<br>
이렇게 부모자식 관계를 배열의 인덱스를 사용해 간단히 계산할 수 있으므로 링크나 포인터가 필요없음.<br>
n개의 원소를 가진 배열 A[1, ..., n]이 주어졌다고 하자.<br>
heapify(A, k, n)은 A[k]에 매달린 두 서브 트리가 힙성질을 만족하는 상태에서 A[k]를 루트로 하는 서브 트리 전체가 힙성질을 만족하도록 수선하는 함수임.<br>
루트의 두 자식 중 작은 값을 x, 큰 값을 y라고 하자.<br>
x를 루트와 비교한다.<br>
루트의 값이 x보다 크지 않으면 힙성질이 만족되어 수선은 끝남.<br>
루트의 값이 x보다 크면 x와 루트의 값을 맞바꿈.<br>
이제 x는 새로운 루트가 되었고, 루트의 값은 한 단계 내려옴.<br>
이렇게 루트가 한 칸 아래로 내려온 노드를 r이라 하자.<br>
여기서 다시 r을 새로운 루트로 삼아 이 작업을 재귀적으로 반복함.<br>
이런 식으로 내려갈 수 있는 곳까지 내려감.<br>
중간에 루트가 자식 중 작은 값보다 크지 않은 경우를 만나면 중단함.<br>

<pre>
buildHeap(A[], n)
{
  for i <- n/2 downto 1
    heapify(A, i, n);
}

heapify(A[], k, n)
{
  left <- 2k; right <- 2k+1;
  
  if (right <= n) then {    // k가 두 자식을 가지는 경우
    if (A[left] < A[right]) 
      then smaller <- left;
    else smaller <- right;            
  }
  
  else if (left <= n) then smaller <- left;
  else return;
  
  if (A[smaller] < A[k]) then {
    A[k] <-> A[smaller];
    heapify(A, smaller, n);
  }
}
</pre>

리프 노드는 그 자체로 힙성질을 만족하므로 buildHeap()은 리프가 아닌 노드 중 맨 뒤에서부터 루트로 삼아 heapify()를 수행함.<br>
n/2은 리프가 아닌 노드 중 맨 마지막 노드의 인덱스임.<br>

buildHeap()에 소요되는 시간은 Θ(n)임.<br>
heapify()는 해당 서브 트리의 높이가 시간을 좌우함.<br>
어떤 서브 트리도 높이가 log2(n)을 넘지 않으므로 heapify를 한번 수행하는데 O(logn)이 소요됨.<br>
그런데 buildHeap()에서 heapify()를 호출하는 횟수는 ⌊n/2⌋이므로 전체적으로 O(nlogn)이 됨.<br>
그러나 이는 과하게 잡은 상한임.<br>
모든 heapify()의 시간을 O(logn)으로 잡은 것이 과함.<br>
맨 처음 호출되는 heapify()의 입력 트리는 높이가 고작 1이고, 이런 것들이 꽤 여러 개 있음.<br>
그 다음 레벨로 올라가면 높이는 증가하지만, 높이가 높은 부분 트리의 수는 줄어들음.<br>
따라서 이를 합산하면 O(nlogn)이 아닌 Θ(n)이 됨.<br>
이에 대한 증명은 아래에 했음.<br>

<pre>
buildHeap()의 수행 시간 계산

heapify()는 힙의 높이에 비례하는 시간이 소요됨.
즉, 힙의 높이가 h라면 O(h) 시간이 소요됨.
원소의 수가 총 n개인 힙의 높이는 ⌊log2(n)⌋임.
높이가 h인 트리의 노드 수는 기껏해야 ⌈n/2^(h+1)⌉임.
따라서 buildHeap()의 수행 시간은 다음과 같음.

Σ(h=0~⌊log2(n)⌋)⌈n/2^(h+1)⌉O(h) = O(nΣ(h=0~⌊log2(n)⌋)h/2^h)=O(n)임.

O(logn)이 될 것이라는 직관과는 달리 O(n)이 됨.
Σ(h=0~⌊log2(n)⌋)h/2^h이 2보다 크지 않기 때문임.

Σ(h=0~∞)x^h = 1/(1-x)
이를 양변에 미분하면 다음과 같음.
Σ(h=0~∞)h*x^(h-1) = 1/(1-x)^2
양변에 x를 곱하면 다음과 같음
Σ(h=0~∞)h*x^h = x/(1-x)^2
h/2^h는 h*x^h에서 x가 1/2인 경우와 같음.
Σ(h=0~⌊log2(n)⌋)h/2^h <= Σ(h=0~∞))h/2^h
                      = (1/2) / (1/2)^2
                      = 2
따라서 2가 Σ(h=0~⌊log2(n)⌋)h/2^h의 상한임.
</pre>

힙이 완성되었으면 정렬 작업을 함.<br>
루트 노트에 있는 원소를 제거하여 다른 곳에 저장함.<br>
루트 노드가 없어졌으므로 트리의 크기가 하나 줄음.<br>
맨 끝에 있는 원소를 루트 노드로 옮겨 새로운 루트로 삼음.<br>
루트 노드로 옮긴 원소가 있던 자리에 방금 제거한 루트 노드 원소를 저장함.<br>
이것으로 대부분의 경우 루트 노드와 자식 간에 힙성질이 깨짐.<br>
heapify()를 이용해 힙성질을 만족하도록 수선함.<br>

<pre>
heapSort(A, n)
{
  buildHeap(A, n);
  for i <- n downto 2 {
    A[1] <-> A[i];
    heapify(A, 1, i-1);
  }
}
</pre>

buildHeap()은 Θ(n)의 시간이 듬.<br>
for 루프는 n-1번 순환하고 각 순환에서 시간을 좌우하는 heapify()는 충분히 잡아서 O(logn)의 시간이면 됨.<br>
그러므로 힙 정렬의 총 수행 시간은 O(nlogn)임.<br>

**비교 정렬 시간의 하한**<br>

원소끼리 비교하는 것으로만 정렬을 하는 것을 비교 정렬이라고 함.<br>
비교 정렬은 최악의 경우 수행 시간이 절대 Ω(nlogn)을 밑돌 수 없음.<br>
이것을 결정 트리 모델을 사용해서 증명할 수 있음.<br>
<br>
편의상 정렬하고자 하는 모든 원소가 다르다고 가정함.<br>
이것으로 일반성을 잃지는 않음.<br>
임의의 비교 정렬은 결정 트리에서 탐색으로 볼 수 있음.<br>
삽입 정렬을 결정 트리 모델로 그려봄.<br>

<pre>
if (a1 < a2)
  if (a2 < a3) return a1 < a2 < a3
  else
    if (a1 < a3) return a1 < a3 < a2
    else return a3 < a1 < a2
else
  if (a1 < a3) return a2 < a1 <a3
  else
    if (a2 < a3)  return a2 < a3 < a1
    else return a3 < a2 <a1
</pre>

리프노드를 만날 때까지 비교를 계속 함.<br>
결정 트리의 루트에서 시작해 리프에 이르면 정렬은 끝남.<br>
정렬 알고리즘은 입력 수열의 모든 가능한 경우에 대해 다 제대로 정렬을 해줘야 하므로 결정 트리의 리프 노드는 n!개가 되어야 함.<br>
n!에 관해서는 스털링의 근사식을 얻을 수 있음.<br>

<pre>
n! = √(2πn)(n/e)^n * (1 + Θ(1/n))

logn! = log( √(2πn)(n/e)^n * (1 + Θ(1/n)) )
      = log√(2πn)(n/e)^n + nlogn - nloge
      = Θ(nlogn)
</pre>

정렬의 수행에서 최악의 경우는 결정 트리에서 가장 깊은 리프 노드까지 내려가는 것임.<br>
n!개의 리프 노드를 가진 트리의 높이는 적어도 ⌈log2(n!)⌉임.<br>
따라서 최악의 깊이는 적어도 ⌈log2(n!)⌉은 되어야 하므로 최악의 경우 수행 시간은 이 전개식의 결론을 이용하면 적어도 Θ(nlogn)임.<br>
즉 Ω(nlogn)임.<br>

**:pushpin: 특수 정렬 알고리즘: 기수 정렬**

지금까지 배운 정렬 알고리즘들은 모두 원소 두 개를 비교함으로써 정렬을 하는 비교 정렬이었음.<br>
즉, 원소의 상대적인 대소 관계만 판단할 뿐이지 원소의 분포나 자릿수 등은 고려하지 않았음.<br>
입력 원소들이 특수한 성질을 만족하는 경우에는 Θ(nlogn)이란 한계를 극복할 수 있음<br>
<br>
기수 정렬은 입력이 모두 k 자릿수 이하의 자연수인 특수한 경우에 사용할 수 있는 방법으로 Θ(n) 시간이 소요되는 정렬 알고리즘임.<br>
우선 가장 낮은 자릿수만 가지고 모든 수를 재배열함.<br>
그런 다음 가장 낮은 자릿수는 잊어버림.<br>
그리고 앞과 같은 방법으로 더 이상 자릿수가 남지 않을 때까지 계속함.<br>
이렇게 하면 마지막에는 정렬된 배열을 갖게 됨.<br>

<pre>
radixSort(A[], n, k)
{
  for <- i to k
    i번째 자릿수에 대해 A[1, ..., n]을 안정성을 유지하면서 정렬함.
}
</pre>

"안정성을 유지하면서 정렬한다"는 것은 값이 같은 원소끼리는 정렬 후에 원래의 순서가 바뀌지 않는 성질을 뜻함.<br>
2150과 2154는 4번째 자릿수가 2로 똑같음.<br>
2150이 2154보다 앞에 있으면 4번째 자리에 대해 정렬했을 때 이 순서는 유지되어야 함.<br>
안정성을 유지하기 위해 다른 정렬 알고리즘을 쓰면 이미 Θ(n)을 초과해버리므로 다른 방법을 사용해야 함.<br>
예를 들어, 0부터 9까지 표시된 10개의 공간을 준비해놓고 각각의 수를 가진 입력은 해당 공간에 차례대로 넣어주는 등 이 부분을 O(n)에 끝내야 함.<br>
알고리즘은 이런 일을 k번 반복하는데 k가 상수이므로 전체 시간은 여전히 O(n)임.<br>

**:pushpin: 특수 정렬 알고리즘: 계수 정렬**

계수 정렬은 정렬하고자 하는 원소들의 값이 O(n)을 넘지 않는 경우에 사용할 수 있음.<br>
예를 들어, 배열 A[1, ..., n]의 원소들이 k를 넘지 않는 자연수인 경우를 들 수 있음.<br>
계수 정렬은 먼저 배열의 원소를 훑어보고 1부터 k까지의 자연수가 각각 몇 번 나타나는지를 셈.<br>
이 정보가 있으면 A[1, ..., n]의 각 원소가 몇 번째에 놓이면 되는지를 계산해낼 수 있음.<br>

<pre>
countingSort(A[], B[], n)
{
  for i <- 1 to k
    C[i] <- 0
  for j <- 1 to n
    C{A[j]]++;
  for i <- 2 to k
    C[i] <- C[i] + C[i-1]
  for j <- n downto 1 {
    B[C[A[j]]] <- A[j];
    C[A[j]]--;
  }
}
</pre>

알고리즘에서 A[1, ..., n]을 정렬한 결과가 배열 B[1, ..., n]에 저장됨.<br>
계수 정렬의 수행 시간은 Θ(n)임.<br>
첫 번째 for 루프는 Θ(k), 두 번째 for 루프는 Θ(n), 세 번째 for 루프는 Θ(k), 마지막 for 루프는 Θ(n)의 시간이 소요됨.<br>
k가 O(n)을 초과하면 시간은 Θ(k)가 됨.<br>
k가 O(nlogn)을 초과하면 계수 정렬은 병합 정렬, 퀵 정렬, 힙 정렬보다 매력이 없어짐.<br>
그래서 일반적으로 계수 정렬은 k가 O(n)을 초과하지 않는 경우에 선형 시간에 정렬하기 위해 사용함.<br>
정렬할 원소가 꼭 양수일 필요도 없음.<br>
원소들이 모두 -k와 k 사이의 정수이고 k가 O(n)일 경우에도 여전히 계수 정렬을 사용하여 선형 시간에 정렬할 수 있음.<br>

<a href="#4">:pencil2: Chapter4. 선택 알고리즘</a>
  
**:pushpin: 평균 선형 시간 선택 알고리즘**

n개의 원소가 규칙 없이 저장된 배열에서 i번째 작은 원소를 찾으려 함.<br>
먼저 퀵 정렬에서 사용한 분할 알고리즘을 상기해봄.<br>
분할 알고리즘은 기준원소보다 작거나 같은 원소는 기준원소의 왼쪽 그룹으로, 기준원소보다 큰 원소는 기준원소의 오른쪽 그룹으로 재배치함.<br>
분할 알고리즘이 리턴하는 값으로 기준 원소가 전체에서 몇 번째 작은 원소인지 알 수 있음.<br>
이것으로 기준원소가 전체에서 k번째 작은 원소란 사실을 알았다고 하자.<br>
이제 i와 k의 값을 비교함.<br>
i가 k보다 작으면, i번째 작은 수는 왼쪽 그룹에 있는 원소 중 하나임.<br>
i가 k와 같으면, 기준원소가 바로 i번째 작은 수임.<br>
i가 k보다 크면, i번째 작은 수는 오른쪽 그룹에 있는 원소 중 하나임.<br>

<pre>
select(A, p, r, i)
{
  if (p=r) then return A[p];
  q <- partition(A, p, r);
  k <- q - p + 1;
  if (i < k) then return select(A, p, q-1, i);
  else if (i = k) then return A[q];
  else return select(A, q+1, r, i-k);
}
</pre>

기준 원소가 전체 집합에서 k번째 작은 원소이면 두 그룹은 각각 k-1개와 n-k개 나뉘고 알고리즘의 수행 시간은 다음과 같음.<br>
T(n) <= max[T(k-1), T(n-k)] + Θ(n)<br>
입력 배열은 가능한 모든 경우가 고루 일어난다고 가정하면, 전체 배열에서 기준원소의 순위 k는 1부터 n까지 동일한 확률을 갖음.<br>
이들의 평균을 위의 관계식에 반영하면 다음과 같음.<br>

<pre>
T(n) 
<= max[T(k-1), T(n-k)] + Θ(n)
<= (1/n)*∑(k=1~n)max[T(k-1), T(n-k)] + Θ(n)<br>
<= (2/n)*∑(⌊n/2⌋~(n-1))T(k) + Θ(n)<br>
⌊n/2⌋<=k<n인 모든 k에 대해 T(k) <= ck라 가정하면<br>
<= (2/n)*∑(⌊n/2⌋~(n-1))ck + Θ(n)<br>
=  (2/n) * (∑(1~(n-1))ck - ∑(⌊n/2⌋~(n-1))ck + Θ(n)<br>
=  (n/2) * [c * (n-1) * n / 2 - c * (⌊n/2⌋ -1) * ⌊n/2⌋ / 2] + Θ(n)
<= 2c / n * ((n-1) * n / 2 - (n / 2 -2) * (n / 2 - 1)) + Θ(n)
=  c(n-1) - (c/n) * (n^2/4 - 3*n/2 + 2) + Θ(n)
=  cn + (-cn /  4  + c / 2 - 2c / n + Θ(n))
<= cn
상수 c를 충분히 크게 잡으면 -cn/4이 Θ(n)을 압도해서 -cn /  4  + c / 2 - 2c / n + Θ(n)이 음수가 되도록 할 수 있음.
</pre>

그러므로 T(n)=O(n)임. T(n)=Ω(n)임은 명백하므로 T(n)=Θ(n)임.<br>
이 알고리즘의 경우 평균적인 경우 Θ(n)의 시간이 소요되지만, 최악의 경우에는 Θ(n^2)의 시간이 소요됨.<br>
최악의 예는 분할 결과 0:n-1로 계속 분할이 되고 찾고자 하는 원소가 운 나쁘게도 큰 그룹에 속하는 일이 반복되는 경우임.<br>
이 때 수행 시간의 점화식은 다음과 같음.<br>
T(n) = T(n-1) + Θ(n)<br>
이것을 전개하면 T(n)=Θ(n^2)이 됨.<br>
항상 이렇게 되지 않아도 이에 준할 정도로 자주 분할의 균형이 깨지면 역시 T(n)=Θ(n^2)이 됨.

**:pushpin: 최악의 경우에도 선형 시간을 보장하는 선택 알고리즘**
  
계속 1:9로 분할이 되고 이 중 나쁜 경우로 큰 그룹(9에 해당하는 부분)에서 탐색을 하게 된다고 하자.<br>
이 경우에는 다음 점화식으로 표현할 수 있음.<br>
T(n) = T(9n/10) + Θ(n)<br>
입력의 크기가 n인 문제를 풀기 위해 Θ(n)의 오버헤드를 사용한 다음 입력의 크기가 9n/10인 문제를 재귀적으로 호출한다.<br>
이것은 계산하면 T(n) = Θ(n)이 됨.<br>
1:99로 분할되어도 여전히 점근적 시간은 T(n) = Θ(n)이 됨.<br>
분할의 균형이 아주 나빠보여도 일정한 상수비만 넘지 않으면 점근적 복잡도는 항상 Θ(n)이 됨.<br>
이 절의 알고리즘은 분할의 균형을 어느 정도까지 보장함으로써 최악의 경우 Θ(n)을 보장함.<br>
그렇지만 분할의 균형만 적당히 맞춘다고 Θ(n)이 무조건 보장되는 것은 아님.<br>
균형을 맞추는 오버헤드가 너무 커져버리면 목표를 이룰 수 없음.<br>

<pre>
linearSelect(A, p, r, i)
{
  1. 원소의 총 수가 5개 이하인 i번째 원소를 찾고 알고리즘을 끝낸다.
  2. 전체 원소를 5개씩의 원소를 가진 ⌈n/5⌉개의 그룹으로 나눈다.(원소의 총수가 5의 배수가 아니면 이 중 한 그룹은 5개 미만이 된다.)
  3. 각 그룹에서 중앙값(원소가 5개이면 3번째 원소)를 찾는다. 이렇게 찾은 중앙값들을 m1, m2, ..., ,m⌈n/5⌉이라 하자.
  4. m1, m2, ..., m⌈n/5⌉들의 중앙값 M을 재귀적으로 구한다. 원소의 총수가 홀수이면 중앙값이 하나이므로 문제가 없고, 원소의 총수가 짝수이면 두 중앙값 중 임의로 선택한다.
  5. M을 기준원소로 삼아 전체 원소를 분할한다.
  6. 분할된 두 그룹 중 적합한 쪽을 선택해 단계 1~6을 재귀적으로 반복한다.
}
</pre>

x를 M보다 작은 원소들, o를 M보다 큰 원소들이라 하고 a를 M보다 크거나 작을 수 있는 원소들이라 가정하자.<br>
가장 바람직한 것은 a가 M의 대소 관계에 따라 왼쪽 그룹과 오른쪽 그룹으로 흩어지는 것이다.<br>
최악의 경우에는 모두 한쪽으로 몰릴 수도 있음.<br>
그럼 이런 최악의 경우에 분할의 균형은 어느 정도까지 나빠질 수 있을까?<br>
o그룹에는 적어도 3n/10-3개의 원소가 포함됨.<br>
M까지 포함하면 3n/10-2개임.<br>
이들을 제외한 나머지 원소는 많아야 n-(3n/10-2) = 7n/10 + 2개임.<br>
최악의 경우에는 이렇게 분할되고, 찾고자 하는 원소가 이 7n/10+2개짜리 그룹에 속함.<br>
이것으로 분할 비율은 최악의 경우에도 7n/10+2 : 3n/10-3이 되어 대략 7:3보다는 나빠지지 않을 수 있게 됨.<br>
select에서 이렇게 분할의 비율이 어느 정도 보장된다면 바로 선형 시간 알고리즘이 됨.<br>
그렇지만 이렇게 분할의 균형을 어느 정도 보장하기 위해서 무시 못할 오버헤드가 듬.<br>
이것은 좋은 기준 원소를 정하는 오버헤드임.<br>
이것으로 얻는 이득이 오버헤드를 극복한다면 좋은 결과를 이끌어낼 수 있음.<br>
<br>
단계 1은 원소의 총 수가 고작 5개 이하인 경우로 상수 시간이 소요될 뿐더러 반복적인 자기호출의 맨 마지막에 단 한 번만 수행되므로 전체 수행 시간에 영향을 주지 않음.
단계 2는 n개의 원소를 5개짜리 그룹으로 나누는 것이므로 각 원소를 한 번씩만 지나가면서 소속 그룹을 정해주면 되어 Θ(n)임.
단계 3은 각 그룹에서 중앙값을 찾는 데 상수 시간이 들고, 이런 작업을 ⌈n/5⌉번 하므로 Θ(n)임.
단계 5는 partition이므로 Θ(n)의 시간이 소요됨.
단계 2, 3, 5를 모두 합쳐서 Θ(n)이 소요됨.
단계 4와 단계 6은 자기호출을 하는 부분임.
즉, 동일한 linearSelect() 알고리즘을 호출하는데 단계 4는 입력의 크기가 ⌈n/5⌉, 단계 6은 입력의 크기가 최대 7n/10 + 2임.
<pre>
따라서 앞 알고리즘의 수행 시간 점화식은 다음과 같음.
T(n) <= T(⌈n/5⌉) + T(7n/10 + 2) + Θ(n)
여기서 기준 원소를 잘 선택하는 오버헤드 부분은  T(⌈n/5⌉) + Θ(n)에 해당함.
이 식을 전개하면 다음과 같음.
T(n) <= T(⌈n/5⌉) + T(7n/10 + 2) + Θ(n)
     <= T(n/5 + 1) T(7n/10 + 2) + Θ(n)
n0 <= k < n인 모든 k에 대해서 T(k) <= ck라고 가정하면 다음과 같음(n0는 경계치)
     <= c(n/5+1) + c(7n/10+2) + Θ(n)
     = c(9n/10 + 3) + Θ(n)
     = cn - cn/10 + 3c + Θ(n)
     <= cn
(-cn/10이 3c + Θ(n)을 압도할 수 있도록 하는 상수 c가 존재하기 때문에 성립함.)
따라서 T(n) <= cn이므로 T(n)=O(n)이다.
T(n) = Ω(n)임은 명백하므로 T(n) = Θ(n)임.
</pre>

n보다 작은 k에 대해서 T(k) <= ck라고 가정해서 위와 같은 전개가 가능했음.<br>
그런데 이것을 만족하려면 n/5+1과 7n/10+2가 각각 n보다 작아야함.<br>
이를 만족하려면 n이 7이상이면 됨.(즉, 경계치 n0가 7이상이 됨.)<br>
알고리즘의 점근적 시간 분석은 충분히 큰 n에 대한 것이므로 n이 7이상이라는 조건은 아주 가벼운 것임.<br>
n/5 <= k < n인 모든 k에 대해 T(k) <= ck라고 가정해도 됨.<br>
